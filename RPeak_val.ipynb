{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31aa22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "R-peak validation pipeline (clean version)\n",
    "-----------------------------------------\n",
    "This script consolidates the essential pieces to:\n",
    "  1) Load ECG from TDMS\n",
    "  2) Parse LabVIEW RR (.lvm) with comma or dot decimals\n",
    "  3) (Optionally) normalize/convert timestamps using Arrow + IANA tz\n",
    "  4) Detect ECG R-peaks (NeuroKit2) and align with LabVIEW R-peaks\n",
    "  5) Create binary peak trains (0/1), zero-pad as needed to overlay\n",
    "  6) Compute metrics with lag & tolerance (TP/FP/FN/Sensitivity/PPV/F1)\n",
    "  7) Plot a quick comparison window\n",
    "\n",
    "Fill in the PATH variables in __main__ and run.\n",
    "Requires: pandas, numpy, neurokit2, nptdms, arrow, scipy, matplotlib\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arrow\n",
    "import nptdms\n",
    "from datetime import datetime, date, time\n",
    "from typing import Tuple, Dict, Iterable, Optional\n",
    "\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import correlate, convolve\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f7015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "#         I/O helpers\n",
    "# -----------------------------\n",
    "\n",
    "def read_labview_rr(path: str, skiprows: int = 22) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Read RR intervals (seconds) from LabVIEW .lvm (or text) file.\n",
    "\n",
    "    - Handles tab-separated with decimal comma (classic LVM) or autodetected separators.\n",
    "    - Picks the most plausible RR column (median ~0.2–3 s).\n",
    "    - Converts from ms/us if needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", engine=\"python\", skiprows=skiprows, header=0, decimal=\",\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=None, engine=\"python\", skiprows=skiprows, header=0)\n",
    "        # force comma -> dot numeric\n",
    "        df = df.apply(lambda s: pd.to_numeric(s.astype(str).str.replace(\",\", \".\"), errors=\"coerce\"))\n",
    "\n",
    "    # choose the RR column\n",
    "    cols = [c for c in df.columns if str(c).lower() not in (\"x_value\", \"xvalue\", \"comment\")]\n",
    "    rr = None\n",
    "    if \"Untitled\" in df.columns:\n",
    "        rr = pd.to_numeric(df[\"Untitled\"], errors=\"coerce\").dropna().to_numpy(float)\n",
    "\n",
    "    if rr is None:\n",
    "        for c in cols:\n",
    "            v = pd.to_numeric(df[c], errors=\"coerce\").dropna().to_numpy(float)\n",
    "            if v.size:\n",
    "                med = float(np.nanmedian(v))\n",
    "                if 0.1 < med < 5.0:  # seconds\n",
    "                    rr = v\n",
    "                    break\n",
    "        if rr is None and cols:\n",
    "            rr = pd.to_numeric(df[cols[0]], errors=\"coerce\").dropna().to_numpy(float)\n",
    "\n",
    "    if rr is None or rr.size == 0:\n",
    "        raise ValueError(\"Could not find an RR column in the provided LabVIEW file.\")\n",
    "\n",
    "    # unit normalization\n",
    "    med = float(np.nanmedian(rr))\n",
    "    if med > 10000:  # microseconds\n",
    "        rr = rr / 1_000_000.0\n",
    "    elif med > 5:    # milliseconds\n",
    "        rr = rr / 1_000.0\n",
    "    return rr\n",
    "\n",
    "\n",
    "def rr_to_peak_samples(rr_seconds: Iterable[float], fs: float, t0_s: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert RR (sec) to absolute peak sample indices (int), assuming first peak at t0_s.\n",
    "\n",
    "    peak_times = [t0, t0 + rr[0], t0 + rr[0]+rr[1], ...]\n",
    "    sample_idx = round(peak_times * fs)\n",
    "    \"\"\"\n",
    "    rr = np.asarray(rr_seconds, dtype=float).ravel()\n",
    "    t_peaks = t0_s + np.cumsum(np.insert(rr, 0, 0.0))\n",
    "    return np.rint(t_peaks * fs).astype(np.int64)\n",
    "\n",
    "\n",
    "def make_binary_series(peak_samples: np.ndarray, n_samples: Optional[int], left_pad: int = 0, right_pad: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a binary series with ones at peak positions, with optional left/right zero-padding.\n",
    "    If n_samples is None: length will be max(peak)+1 plus padding.\n",
    "    Negative peaks are ignored; peaks >= length are dropped.\n",
    "    \"\"\"\n",
    "    peaks = np.asarray(peak_samples, dtype=int)\n",
    "    if n_samples is None:\n",
    "        length = (int(peaks.max()) + 1 if peaks.size else 0) + left_pad + right_pad\n",
    "    else:\n",
    "        length = int(n_samples) + left_pad + right_pad\n",
    "\n",
    "    x = np.zeros(length, dtype=np.uint8)\n",
    "    # shift peaks by left_pad for insertion\n",
    "    shifted = peaks + left_pad\n",
    "    valid = shifted[(shifted >= 0) & (shifted < length)]\n",
    "    x[valid] = 1\n",
    "    return x\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#   Time parsing & alignment\n",
    "# -----------------------------\n",
    "\n",
    "def read_header_datetime_lvm(path: str, default_date_fmt: str = \"%Y/%m/%d\") -> Optional[datetime]:\n",
    "    \"\"\"\n",
    "    Parse 'Date' and 'Time' from a LabVIEW .lvm header before ***End_of_Header***.\n",
    "    Returns a naive datetime (no tz) if found; otherwise None.\n",
    "\n",
    "    'Time' examples: '13:06:19,1816539465369...' or '13:06:19.1816...' or '13:06:19'.\n",
    "    \"\"\"\n",
    "    date_val = None\n",
    "    time_val = None\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            ls = line.strip()\n",
    "            if ls.startswith(\"***End_of_Header***\"):\n",
    "                break\n",
    "            low = ls.lower()\n",
    "            if low.startswith(\"date\\t\") and date_val is None:\n",
    "                date_val = ls.split(\"\\t\", 1)[1].strip()\n",
    "            elif \"time\" in low and \"time_pref\" not in low and low.startswith(\"time\\t\") and time_val is None:\n",
    "                time_val = ls.split(\"\\t\", 1)[1].strip()\n",
    "\n",
    "    if not time_val:\n",
    "        return None\n",
    "\n",
    "    m = re.match(r\"^(\\d{2}:\\d{2}:\\d{2})[,.](\\d+)$\", time_val)\n",
    "    if m:\n",
    "        hhmmss, frac = m.group(1), m.group(2)\n",
    "        # normalize to microseconds with rounding on the 7th digit\n",
    "        if len(frac) > 6 and int(frac[6]) >= 5:\n",
    "            frac6 = str(int(frac[:6]) + 1).zfill(6)\n",
    "        else:\n",
    "            frac6 = frac[:6].ljust(6, \"0\")\n",
    "        time_norm = f\"{hhmmss}.{frac6}\"\n",
    "        t_dt = datetime.strptime(time_norm, \"%H:%M:%S.%f\")\n",
    "    else:\n",
    "        # HH:MM:SS only\n",
    "        m2 = re.match(r\"^\\d{2}:\\d{2}:\\d{2}$\", time_val)\n",
    "        if not m2:\n",
    "            return None\n",
    "        t_dt = datetime.strptime(time_val, \"%H:%M:%S\")\n",
    "\n",
    "    if date_val:\n",
    "        fmts = [default_date_fmt, \"%Y-%m-%d\", \"%d/%m/%Y\", \"%d-%m-%Y\"]\n",
    "        for fmt in fmts:\n",
    "            try:\n",
    "                d = datetime.strptime(date_val, fmt).date()\n",
    "                return datetime.combine(d, t_dt.time())\n",
    "            except ValueError:\n",
    "                continue\n",
    "        # fallback to time only\n",
    "        return t_dt\n",
    "    return t_dt\n",
    "\n",
    "\n",
    "def extract_tdms_channel(tdms_path: str, group_name: Optional[str] = None, channel_name: Optional[str] = None) -> Tuple[pd.Series, float, Optional[arrow.Arrow]]:\n",
    "    \"\"\"\n",
    "    Load a single ECG channel from TDMS. Returns (signal_series, fs, tdms_start_time_arrow_or_None)\n",
    "\n",
    "    - If group/channel are None, picks the first numeric channel found.\n",
    "    - Tries to read sampling rate from channel properties, else falls back to 512 Hz.\n",
    "    - Attempts to fetch a 'time' property and returns it as an Arrow object (tz-aware if possible).\n",
    "    \"\"\"\n",
    "    tdms = nptdms.TdmsFile.read(tdms_path)\n",
    "    chosen = None\n",
    "    fs = None\n",
    "    tdms_time = None\n",
    "\n",
    "    # try to find time property at file or channel level\n",
    "    def find_time(props: dict) -> Optional[arrow.Arrow]:\n",
    "        if not isinstance(props, dict):\n",
    "            return None\n",
    "        for k, v in props.items():\n",
    "            if \"wf_start_time\" in str(k).lower() and v is not None:\n",
    "                try:\n",
    "                    tdms_time = correctTime(v)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        return None\n",
    "\n",
    "    if hasattr(tdms, \"properties\"):\n",
    "        tdms_time = find_time(tdms.properties)\n",
    "\n",
    "    channels = []\n",
    "    for g in tdms.groups():\n",
    "        for ch in g.channels():\n",
    "            try:\n",
    "                arr = ch[:]\n",
    "            except Exception:\n",
    "                arr = ch.data\n",
    "            if arr is None:\n",
    "                continue\n",
    "            if np.asarray(arr).dtype.kind in (\"i\", \"u\", \"f\"):\n",
    "                channels.append((g.name, ch.name, pd.Series(arr, name=f\"{g.name}.{ch.name}\"), ch.properties))\n",
    "\n",
    "    if not channels:\n",
    "        raise RuntimeError(\"No numeric channels found in TDMS file.\")\n",
    "\n",
    "    if group_name and channel_name:\n",
    "        candidates = [t for t in channels if t[0] == group_name and t[1] == channel_name]\n",
    "        if not candidates:\n",
    "            raise RuntimeError(f\"Channel {group_name}.{channel_name} not found in TDMS.\")\n",
    "        chosen = candidates[0]\n",
    "    else:\n",
    "        # pick the first channel named like EKG if present; else first numeric\n",
    "        ekg_like = [t for t in channels if \"ekg\" in t[1].lower() or \"ecg\" in t[1].lower()]\n",
    "        chosen = ekg_like[0] if ekg_like else channels[0]\n",
    "\n",
    "    gname, cname, series, props = chosen\n",
    "\n",
    "    # sampling rate\n",
    "    srate = None\n",
    "    for key in (\"wf_sample_rate\", \"sample_rate\", \"sampling_rate\", \"Rate\", \"rate\"):\n",
    "        if key in props and props[key] not in (None, \"\"):\n",
    "            srate = props[key]\n",
    "            break\n",
    "    try:\n",
    "        if isinstance(srate, (bytes, bytearray)):\n",
    "            srate = float(srate.decode())\n",
    "        elif srate is not None:\n",
    "            srate = float(srate)\n",
    "    except Exception:\n",
    "        srate = None\n",
    "    fs = srate if srate is not None else 512.0\n",
    "\n",
    "    # channel-level time if file-level missing\n",
    "    if tdms_time is None:\n",
    "        tdms_time = find_time(props)\n",
    "\n",
    "    return series.rename(\"ECG\"), float(fs), tdms_time\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#      Peak detection & lag\n",
    "# -----------------------------\n",
    "\n",
    "def detect_ecg_peaks(signal: pd.Series, fs: float) -> np.ndarray:\n",
    "    \"\"\"Clean ECG and return R-peak indices (global sample indices).\"\"\"\n",
    "    cleaned = nk.ecg_clean(signal.to_numpy(), sampling_rate=int(fs))\n",
    "    peaks_dict, _ = nk.ecg_peaks(cleaned, sampling_rate=int(fs))\n",
    "    # NeuroKit returns a binary vector under 'ECG_R_Peaks': 1 at peaks, else 0\n",
    "    binary = peaks_dict.get(\"ECG_R_Peaks\")\n",
    "    if binary is None:\n",
    "        # Older NK versions may return indices elsewhere\n",
    "        # Fallback: take first array-like found\n",
    "        for v in peaks_dict.values():\n",
    "            try:\n",
    "                binary = np.asarray(v)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "    idx = np.flatnonzero(np.asarray(binary, dtype=np.uint8))\n",
    "    return idx\n",
    "\n",
    "\n",
    "def estimate_lag_samples(x: np.ndarray, y: np.ndarray, fs: float, start_s: float = 100.0, dur_s: float = 10.0) -> int:\n",
    "    \"\"\"\n",
    "    Estimate lag between two signals (y relative to x) via cross-correlation on a window.\n",
    "    Returns lag in samples (>0 means y is delayed vs x).\n",
    "    \"\"\"\n",
    "    i0 = int(start_s * fs)\n",
    "    n = int(dur_s * fs)\n",
    "    xa = x[i0:i0+n].astype(float) - float(np.mean(x[i0:i0+n]))\n",
    "    ya = y[i0:i0+n].astype(float) - float(np.mean(y[i0:i0+n]))\n",
    "    c = correlate(ya, xa, mode=\"full\")\n",
    "    lags = np.arange(-len(xa)+1, len(xa))\n",
    "    return int(lags[int(np.argmax(c))])\n",
    "\n",
    "\n",
    "def align_indices(idx: np.ndarray, lag_samples: int) -> np.ndarray:\n",
    "    \"\"\"Shift indices by lag (negative to advance, positive to delay), clip to >= 0.\"\"\"\n",
    "    out = idx - int(lag_samples)\n",
    "    return out[out >= 0]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#   Event metrics (with tol)\n",
    "# -----------------------------\n",
    "\n",
    "def event_metrics_overlap_lag(gold_idx: np.ndarray,\n",
    "                              test_idx: np.ndarray,\n",
    "                              fs: float,\n",
    "                              tol_ms: float = 40.0,\n",
    "                              max_lag_ms: float = 150.0) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute TP/FP/FN/Sensitivity/PPV/F1 after cropping to overlap, estimating a small lag\n",
    "    (±max_lag_ms) and applying a symmetric tolerance window (±tol_ms) around gold peaks.\n",
    "    \"\"\"\n",
    "    gold_idx = np.asarray(gold_idx, dtype=int)\n",
    "    test_idx = np.asarray(test_idx, dtype=int)\n",
    "\n",
    "    lo = max(gold_idx.min(), test_idx.min())\n",
    "    hi = min(gold_idx.max(), test_idx.max())\n",
    "    if hi <= lo:\n",
    "        raise ValueError(\"No temporal overlap between peak sequences.\")\n",
    "\n",
    "    g = gold_idx[(gold_idx >= lo) & (gold_idx < hi)] - lo\n",
    "    t = test_idx[(test_idx >= lo) & (test_idx < hi)] - lo\n",
    "    N = int(hi - lo)\n",
    "    a = np.zeros(N, dtype=np.uint8); a[g] = 1\n",
    "    b = np.zeros(N, dtype=np.uint8); b[t] = 1\n",
    "\n",
    "    # find best small lag\n",
    "    maxlag = int(round(max_lag_ms/1000.0*fs))\n",
    "    bestlag = 0; best = -1\n",
    "    for lag in range(-maxlag, maxlag+1):\n",
    "        if lag < 0:\n",
    "            score = int((a[:lag] & b[-lag:]).sum())\n",
    "        elif lag > 0:\n",
    "            score = int((a[lag:] & b[:-lag]).sum())\n",
    "        else:\n",
    "            score = int((a & b).sum())\n",
    "        if score > best:\n",
    "            best, bestlag = score, lag\n",
    "\n",
    "    # shift after best lag\n",
    "    if bestlag > 0:\n",
    "        b2 = b[bestlag:]; a2 = a[:len(b2)]\n",
    "    elif bestlag < 0:\n",
    "        a2 = a[-bestlag:]; b2 = b[:len(a2)]\n",
    "    else:\n",
    "        a2, b2 = a, b\n",
    "\n",
    "    tol = int(round(tol_ms/1000.0*fs))\n",
    "    win = np.ones(2*tol+1, dtype=int)\n",
    "    TP = int((convolve(a2, win, mode='same') * b2 > 0).sum())\n",
    "    FP = int(int(b2.sum()) - TP)\n",
    "    FN = int(int(a2.sum()) - TP)\n",
    "\n",
    "    sens = TP/(TP+FN) if (TP+FN) > 0 else np.nan\n",
    "    ppv  = TP/(TP+FP) if (TP+FP) > 0 else np.nan\n",
    "    f1   = 2*sens*ppv/(sens+ppv) if (sens>0 and ppv>0) else np.nan\n",
    "\n",
    "    return dict(TP=TP, FP=FP, FN=FN, Sensitivity=sens, PPV=ppv, F1=f1,\n",
    "                lag_samples=bestlag, tol_samples=tol, N_overlap=len(a2), lo=lo, hi=lo+len(a2))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#       Overlay utilities\n",
    "# -----------------------------\n",
    "\n",
    "def overlay_with_padding(len_signal: int,\n",
    "                         lab_samples: np.ndarray,\n",
    "                         nk_samples: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build zero-padded binary trains for LabVIEW and NK so they share a common axis.\n",
    "    - If Lab starts later than ECG, we left-pad Lab with zeros.\n",
    "    - If Lab starts earlier than ECG, we shift Lab to the right by trimming negatives and left-pad NK if needed.\n",
    "    - Length is chosen as the max of (len_signal, last peak + a small buffer).\n",
    "\n",
    "    Returns (lab_bin, nk_bin) as same-length arrays.\n",
    "    \"\"\"\n",
    "    L = int(len_signal)\n",
    "    lab = np.asarray(lab_samples, dtype=int)\n",
    "    nk  = np.asarray(nk_samples, dtype=int)\n",
    "\n",
    "    first_lab = int(lab.min()) if lab.size else 0\n",
    "    first_nk  = int(nk.min()) if nk.size else 0\n",
    "\n",
    "    left_pad_lab = 0\n",
    "    left_pad_nk  = 0\n",
    "\n",
    "    if first_lab > first_nk:\n",
    "        # Lab starts later → left-pad Lab\n",
    "        left_pad_lab = first_lab - first_nk\n",
    "    elif first_nk > first_lab:\n",
    "        # NK starts later → left-pad NK\n",
    "        left_pad_nk = first_nk - first_lab\n",
    "\n",
    "    # Build tentative length\n",
    "    end_lab = int(lab.max()) + left_pad_lab if lab.size else 0\n",
    "    end_nk  = int(nk.max())  + left_pad_nk  if nk.size else 0\n",
    "    length = max(L, end_lab+1, end_nk+1)\n",
    "\n",
    "    lab_bin = make_binary_series(lab, n_samples=length - left_pad_lab, left_pad=left_pad_lab, right_pad=0)\n",
    "    nk_bin  = make_binary_series(nk,  n_samples=length - left_pad_nk,  left_pad=left_pad_nk,  right_pad=0)\n",
    "\n",
    "    # Crop to same length just in case\n",
    "    N = min(len(lab_bin), len(nk_bin))\n",
    "    return lab_bin[:N], nk_bin[:N]\n",
    "\n",
    "# Helper function to correct TDMS time values\n",
    "def correctTime(time_val):\n",
    "    \"\"\"\n",
    "    Normalize a TDMS header time value and return a tz-naive local datetime.\n",
    "\n",
    "    Accepts numpy.datetime64, pandas.Timestamp, datetime, str or Arrow-like values.\n",
    "    - Converts input to a pandas.Timestamp for robust parsing.\n",
    "    - If a global HOURS_TO_ADD (int) is defined, that many hours will be added\n",
    "      (useful for known manual offsets).\n",
    "    - The caller code will convert the timestamp to an Arrow object, set UTC,\n",
    "      convert to Europe/Copenhagen and return a tz-naive local datetime.\n",
    "    \"\"\"\n",
    "    if time_val is None:\n",
    "        raise ValueError(\"time_val must not be None\")\n",
    "\n",
    "    # Parse/normalize to pandas Timestamp for safe arithmetic\n",
    "    try:\n",
    "        t = pd.to_datetime(time_val)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Unable to parse time_val: {e}\")\n",
    "\n",
    "    # Optionally apply a global hours offset if present\n",
    "    add_hours = globals().get(\"HOURS_TO_ADD\", 0)\n",
    "    try:\n",
    "        add_hours = int(add_hours)\n",
    "    except Exception:\n",
    "        add_hours = 0\n",
    "\n",
    "    if add_hours != 0:\n",
    "        t = t + pd.Timedelta(hours=add_hours)\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e0370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "#             Main\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # ========== 1) CONFIG ==========\n",
    "    TDMS_PATH = r\"REPLACE_WITH_TDMS_FILE\"\n",
    "    LABVIEW_LVM_PATH = r\"REPLACE_WITH_LABVIEW_LVM\"\n",
    "    TDMS_PATH = r\"E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 5\\recording 1\\Patient 5_1.tdms\"\n",
    "    LABVIEW_LVM_PATH = r\"E:\\ML algoritme tl anfaldsdetektion vha HRV\\LabView-Results\\Patient5_1-corrected-rr.lvm\"\n",
    "    FS_FALLBACK = 512.0\n",
    "    LOCAL_TZ = \"Europe/Copenhagen\"\n",
    "\n",
    "    # If the LabVIEW stream (or seizure export) starts e.g. 60s after TDMS start, you can\n",
    "    # reflect that here. If None, we try to infer from header times if available.\n",
    "    KNOWN_LAB_INITIAL_SKIP_S: Optional[float] = None  # e.g., 60.0 or None\n",
    "\n",
    "    # Matching / windowing\n",
    "    TOL_MS = 40.0\n",
    "    MAX_LAG_MS = 150.0\n",
    "\n",
    "    # Plot window (global seconds)\n",
    "    PLOT_START_S = 10000.0\n",
    "    PLOT_DUR_S   = 20.0\n",
    "\n",
    "    # ========== 2) LOAD ECG + TIME ==========\n",
    "    ecg, fs, tdms_time = extract_tdms_channel(TDMS_PATH)\n",
    "    if fs is None:\n",
    "        fs = FS_FALLBACK\n",
    "    fs = float(fs)\n",
    "\n",
    "    # Convert TDMS time to local tz for logging\n",
    "    if tdms_time is not None:\n",
    "        tdms_local = correctTime(tdms_time)\n",
    "        print(f\"[TDMS] Start time (local): {tdms_local}\")\n",
    "    else:\n",
    "        print(\"[TDMS] No start time property found.\")\n",
    "\n",
    "    # ========== 3) DETECT R-PEAKS FROM ECG ==========\n",
    "    nk_idx = detect_ecg_peaks(ecg, fs=fs)\n",
    "    print(f\"[NK] Detected {len(nk_idx)} R-peaks from ECG.\")\n",
    "\n",
    "    # (Optional) account for filter lag between raw and cleaned if you're using cleaned for display only.\n",
    "    # Here we work directly on global sample indices, so we skip waveform alignment.\n",
    "\n",
    "    # ========== 4) LABVIEW RR -> PEAK SAMPLES ==========\n",
    "    rr = read_labview_rr(LABVIEW_LVM_PATH)\n",
    "    # Infer offset between TDMS start and LabVIEW first peak, if possible\n",
    "    lab_header_dt = read_header_datetime_lvm(LABVIEW_LVM_PATH)\n",
    "    offset_sec = 0.0\n",
    "\n",
    "    if KNOWN_LAB_INITIAL_SKIP_S is not None:\n",
    "        offset_sec += float(KNOWN_LAB_INITIAL_SKIP_S)\n",
    "\n",
    "    if lab_header_dt is not None and tdms_time is not None:\n",
    "        # Assume Lab header time is wall-clock local time; if naive, localize to LOCAL_TZ\n",
    "        a_lab = arrow.get(pd.to_datetime(lab_header_dt))\n",
    "        if a_lab.tzinfo is None:\n",
    "            a_lab = a_lab.replace(tzinfo=LOCAL_TZ)\n",
    "        # tdms_time is Arrow already (UTC localized earlier)\n",
    "        # Compute difference in seconds: Lab - TDMS\n",
    "        delta_s = (a_lab - tdms_time.to(LOCAL_TZ)).total_seconds()\n",
    "        offset_sec += float(delta_s)\n",
    "        print(f\"[ALIGN] Estimated offset from headers: {delta_s:.3f} s (accumulated offset now {offset_sec:.3f} s)\")\n",
    "    else:\n",
    "        print(\"[ALIGN] Header times missing; using KNOWN_LAB_INITIAL_SKIP_S only.\" if KNOWN_LAB_INITIAL_SKIP_S else\n",
    "              \"[ALIGN] No header times and no known skip; assuming offset_sec=0.\")\n",
    "\n",
    "    lab_idx = rr_to_peak_samples(rr, fs=fs, t0_s=offset_sec)\n",
    "    print(f\"[LAB] Built {len(lab_idx)} LabVIEW peak samples.\")\n",
    "\n",
    "    # ========== 5) METRICS WITH LAG + TOL ==========\n",
    "    metrics = event_metrics_overlap_lag(lab_idx, nk_idx, fs=fs, tol_ms=TOL_MS, max_lag_ms=MAX_LAG_MS)\n",
    "    print(\"[METRICS]\", metrics)\n",
    "\n",
    "    # ========== 6) BUILD ZERO-PADDED BINARY TRAINS FOR OVERLAY ==========\n",
    "    lab_bin, nk_bin = overlay_with_padding(len_signal=len(ecg), lab_samples=lab_idx, nk_samples=nk_idx)\n",
    "    print(f\"[BIN] Built binary trains: lab={lab_bin.shape}, nk={nk_bin.shape}\")\n",
    "\n",
    "    # ========== 7) PLOT A WINDOW ==========\n",
    "    s = int(PLOT_START_S * fs); e = int((PLOT_START_S + PLOT_DUR_S) * fs)\n",
    "    t = np.arange(s, e) / fs\n",
    "\n",
    "    # Windowed peaks (relative to s)\n",
    "    lab_loc = lab_idx[(lab_idx >= s) & (lab_idx < e)] - s\n",
    "    nk_loc  = nk_idx [(nk_idx  >= s) & (nk_idx  < e)] - s\n",
    "\n",
    "    # Quick plot (no specific colors)\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(t, ecg.to_numpy()[s:e], label=\"ECG (raw)\")\n",
    "    plt.scatter(t[lab_loc], ecg.to_numpy()[s:e][lab_loc], s=14, label=\"R (LabVIEW)\")\n",
    "    plt.scatter(t[nk_loc],  ecg.to_numpy()[s:e][nk_loc],  s=14, label=\"R (NeuroKit)\")\n",
    "    plt.xlabel(\"Time (s)\"); plt.ylabel(\"Amplitude\")\n",
    "    plt.title(f\"ECG with R-peaks, window {PLOT_START_S}s–{PLOT_START_S+PLOT_DUR_S}s\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ========== 8) SUMMARY TABLE ==========\n",
    "    # Percent agreement within tolerance (based on metrics)\n",
    "    tp = metrics[\"TP\"]; fp = metrics[\"FP\"]; fn = metrics[\"FN\"]\n",
    "    sens = metrics[\"Sensitivity\"]; ppv = metrics[\"PPV\"]; f1 = metrics[\"F1\"]\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(f\"TP={tp}, FP={fp}, FN={fn}\")\n",
    "    print(f\"Sensitivity={sens:.4f} | PPV={ppv:.4f} | F1={f1:.4f}\")\n",
    "    print(\"================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444e7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit TDMS_PATH and LABVIEW_LVM_PATH inside main() before running this script.\n",
      "[TDMS] No start time property found.\n"
     ]
    }
   ],
   "source": [
    "TDMS_PATH = r\"E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 5\\recording 1\\Patient 5_1.tdms\"\n",
    "LABVIEW_LVM_PATH = r\"E:\\ML algoritme tl anfaldsdetektion vha HRV\\LabView-Results\\Patient5_1-corrected-rr.lvm\"\n",
    "print(\"Edit TDMS_PATH and LABVIEW_LVM_PATH inside main() before running this script.\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
