{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4809122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nptdms import TdmsFile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "path_base_folder = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\"\n",
    "output_base_folder = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\"\n",
    "\n",
    "patient_path_1 = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 1\\recording 2\\Patient 1_2.tdms\"\n",
    "\n",
    "Patients_to_use = [\"Patient 3\", \"Patient 5\", \"Patient 6\", \"Patient 8\", \"Patient 10\", \"Patient 14\", \"Patient 15\", \"Patient 16\", \"Patient 21\", \"Patient 23\", \"Patient 27\", \"Patient 28\", \"Patient 29\", \"Patient 31\", \"Patient 34\", \"Patient 37\", \"Patient 39\", \"Patient 40\", \"Patient 41\", \"Patient 42\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b0484fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_tdms(tdms_path):\n",
    "    tdms_path = Path(tdms_path)\n",
    "    with TdmsFile.read(tdms_path) as tdms:\n",
    "        print(f\"\\n=== FILE: {tdms_path.name} ===\")\n",
    "        print(\"File properties:\")\n",
    "        for k, v in tdms.properties.items():\n",
    "            print(f\"  - {k}: {v}\")\n",
    "        for group in tdms.groups():\n",
    "            print(f\"\\n[Group] {group.name}\")\n",
    "            # (Gruppe-properties kan ikke altid skrives 1:1 ud igen; vi bibeholder primært fil- og kanal-properties)\n",
    "            for ch in group.channels():\n",
    "                print(f\"  [Channel] {ch.name}\")\n",
    "                # Vis udvalgte properties\n",
    "                prop_keys = list(ch.properties.keys())\n",
    "                interesting = [p for p in prop_keys if p.lower() in {\"wf_start_time\", \"wf_increment\", \"unit_string\", \"name\"}]\n",
    "                for p in interesting:\n",
    "                    print(f\"    - {p}: {ch.properties[p]}\")\n",
    "                # Vis antal samples\n",
    "                try:\n",
    "                    n = ch._length  # hurtigt, uden at læse alt\n",
    "                except Exception:\n",
    "                    n = len(ch[:])   # fallback\n",
    "                print(f\"    - samples: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f739a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_tdms_start_time(tdms_file_path, output_path=\"copied_file.tdms\"):\n",
    "    from nptdms import TdmsFile, TdmsWriter, RootObject\n",
    "    import numpy as np\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "\n",
    "    original_file = TdmsFile(tdms_file_path)\n",
    "    original_groups = original_file.groups()\n",
    "    original_channels = [chan for group in original_groups for chan in group.channels()]\n",
    "    original_file.properties[\"Corrected_by: \"] = \"Kasper Viken Jensen\"\n",
    "    starttime_string = original_file.properties[\"name\"]\n",
    "\n",
    "    # Extract start datetime information from the start_time string\n",
    "    datetime_matches = re.search(r'\\d{2}_\\d{2}_\\d{4}*kl_?\\d{6}', starttime_string)\n",
    "    if datetime_matches:\n",
    "        start_dt_str = datetime_matches[0].replace(' kl_', ' ').replace(' kl', ' ')\n",
    "        start_datetime = datetime.strptime(start_dt_str, '%d_%m_%Y %H%M%S')\n",
    "        \n",
    "    else:\n",
    "        start_datetime = None\n",
    "\n",
    "    # Correct the start time in channel properties if found\n",
    "    if start_datetime is not None:\n",
    "        original_channels[0].properties[\"wf_start_time\"] = np.datetime64(start_datetime)\n",
    "\n",
    "    # Write to new TDMS file\n",
    "    with TdmsWriter(output_path) as copied_file:\n",
    "        root_object = RootObject(original_file.properties)\n",
    "        copied_file.write_segment([root_object] + original_groups + original_channels)\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1da9697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final patient folders: ['D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 10\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 14\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 14\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 15\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 15\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 16\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 21\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 21\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 23\\\\enrollment a\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 23\\\\enrollment b\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 23\\\\enrollment b\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 27\\\\enrollment a\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 27\\\\enrollment b\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 28\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 28\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 29\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 3\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 3\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 31\\\\enrollment a\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 31\\\\enrollment b\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 31\\\\enrollment b\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 34\\\\enrollment a\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 34\\\\enrollment b\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 34\\\\enrollment c\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 34\\\\enrollment c\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 37\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 39\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 40\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 40\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 41\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 42\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 5\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 6\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 6\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 8\\\\Enrollment a\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 8\\\\Enrollment a\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 8\\\\Enrollment b']\n"
     ]
    }
   ],
   "source": [
    "final_patient_folders = []\n",
    "folders = [folder for folder in os.listdir(path_base_folder) if folder in Patients_to_use]\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(path_base_folder, folder)\n",
    "    output_path = os.path.join(output_base_folder, folder)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        if not dirs:  # Deepest folder\n",
    "            final_patient_folders.append(root)\n",
    "\n",
    "print(\"Final patient folders:\", final_patient_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47462fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FILE: Patient 10_1.tdms ===\n",
      "File properties:\n",
      "  - name: Sidsel Borg Nielsen 20062017 kl_134443 __23062017_kl_130000.tdms\n",
      "  - author: Jesper\n",
      "  - description: \n",
      "  - registertxt1: Written by National Instruments LabVIEW\n",
      "\n",
      "[Group] Untitled\n",
      "  [Channel] EKG\n",
      "    - wf_start_time: 2017-06-20T11:44:43.000000\n",
      "    - wf_increment: 0.001953125\n",
      "    - unit_string: uV\n",
      "    - samples: 132710144\n",
      "D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 10\\recording 1\n",
      "Patient 10\n",
      "Output path for corrected file: D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\n"
     ]
    }
   ],
   "source": [
    "for folder1 in final_patient_folders[0:1]:  # Test med én mappe først\n",
    "    files_in_folder = os.listdir(folder1)\n",
    "    tdms_files = [f for f in files_in_folder if f.endswith('.tdms')]\n",
    "    if tdms_files:\n",
    "        path_tdms = os.path.join(folder1, tdms_files[0])\n",
    "        inspect_tdms(path_tdms)\n",
    "        tdms_file = TdmsFile.read(path_tdms)\n",
    "\n",
    "        print(folder1)\n",
    "        parent_folder = os.path.basename(os.path.dirname(folder1))\n",
    "        print(parent_folder)\n",
    "        parent_subfolder = os.path.basename(folder1)\n",
    "        output_path_file = os.path.join(output_base_folder, parent_folder, parent_subfolder, tdms_files[0])\n",
    "        print(\"Output path for corrected file:\", output_path_file)\n",
    "\n",
    "        # tdms_file_path = os.path.join(folder1, tdms_files[0])\n",
    "        # output = os.path.join(output_base_folder, os.path.basename(folder1), tdms_files[0])\n",
    "        # x = correct_tdms_start_time(tdms_file_path, output)\n",
    "        # print(\"TDMS file path:\", tdms_file_path)\n",
    "        # print(\"Output path:\", output)\n",
    "        # print(\"Corrected file:\", x)\n",
    "        # print(\"TDMS files:\", tdms_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c869360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file properties:\n",
      "  name: Patient 10_1 20_06_2017 kl_13_44_43 __23_06_2017 kl_13_00_00\n",
      "  author: Jesper\n",
      "  description: \n",
      "  registertxt1: Written by National Instruments LabVIEW\n",
      "  Corrected_by: : Kasper Viken Jensen\n"
     ]
    }
   ],
   "source": [
    "tdms_file.properties[\"name\"] = \"Patient 10_1 20_06_2017 kl_13_44_43 __23_06_2017 kl_13_00_00\"\n",
    "tdms_file.properties[\"Corrected_by: \"] = \"Kasper Viken Jensen\"\n",
    "print(\"Updated file properties:\")\n",
    "for key, value in tdms_file.properties.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250c482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel properties before correction:\n",
      "  wf_start_time: 2017-06-20T11:44:43.000000\n",
      "  wf_start_offset: 0.0\n",
      "  wf_increment: 0.001953125\n",
      "  wf_samples: 132710144\n",
      "  DigitalMaximum: 32767\n",
      "  DigitalMinimum: -32768\n",
      "  NI_ChannelName: EKG\n",
      "  NI_ExpIsRelativeTime: False\n",
      "  wf_time_pref: absolute\n",
      "  NI_ExpStartTimeStamp: 2017-06-20T11:44:43.000000\n",
      "  NI_ExpTimeStamp: 2017-06-20T11:44:43.000000\n",
      "  NI_ExpXDimension: t\n",
      "  wf_xname: Time\n",
      "  wf_xunit_string: s\n",
      "  NI_UnitDescription: uV\n",
      "  unit_string: uV\n",
      "  PhysicalMaximum: 90298.0\n",
      "  PhysicalMinimum: -90298.0\n",
      "  PreFilter: \n",
      "  TransducerType: \n",
      "  ValsPerRec: 128\n"
     ]
    }
   ],
   "source": [
    "timestamp_corrected1 = np.datetime64('2017-06-20T13:44:43.000000')\n",
    "\n",
    "channel1 = tdms_file.groups()[0].channels()[0]\n",
    "print(\"Channel properties before correction:\")\n",
    "for key, value in channel1.properties.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22cd49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel properties after correction:\n",
      "  wf_start_time: 2017-06-20T13:44:43.000000\n",
      "  wf_start_offset: 0.0\n",
      "  wf_increment: 0.001953125\n",
      "  wf_samples: 132710144\n",
      "  DigitalMaximum: 32767\n",
      "  DigitalMinimum: -32768\n",
      "  NI_ChannelName: EKG\n",
      "  NI_ExpIsRelativeTime: False\n",
      "  wf_time_pref: absolute\n",
      "  NI_ExpStartTimeStamp: 2017-06-20T13:44:43.000000\n",
      "  NI_ExpTimeStamp: 2017-06-20T13:44:43.000000\n",
      "  NI_ExpXDimension: t\n",
      "  wf_xname: Time\n",
      "  wf_xunit_string: s\n",
      "  NI_UnitDescription: uV\n",
      "  unit_string: uV\n",
      "  PhysicalMaximum: 90298.0\n",
      "  PhysicalMinimum: -90298.0\n",
      "  PreFilter: \n",
      "  TransducerType: \n",
      "  ValsPerRec: 128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tdms_file.groups()[0].channels()[0].properties[\"wf_start_time\"] = timestamp_corrected1\n",
    "tdms_file.groups()[0].channels()[0].properties[\"NI_ExpStartTimeStamp\"] = timestamp_corrected1\n",
    "tdms_file.groups()[0].channels()[0].properties[\"NI_ExpTimeStamp\"] = timestamp_corrected1\n",
    "\n",
    "print(\"Channel properties after correction:\")\n",
    "for key, value in channel1.properties.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "337a3a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing corrected TDMS file to: D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\n"
     ]
    }
   ],
   "source": [
    "from nptdms import TdmsWriter, RootObject\n",
    "\n",
    "print(\"Writing corrected TDMS file to:\", output_path_file)\n",
    "\n",
    "with TdmsWriter(output_path_file) as writer:\n",
    "    root_object = RootObject(tdms_file.properties)\n",
    "    writer.write_segment([root_object] + tdms_file.groups())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74469205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_patient_folders = []\n",
    "# folders = [folder for folder in os.listdir(path_base_folder) if folder in Patients_to_use]\n",
    "\n",
    "# for folder in folders:\n",
    "#     folder_path = os.path.join(path_base_folder, folder)\n",
    "#     output_path = os.path.join(output_base_folder, folder)\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "#     for root, dirs, files in os.walk(folder_path):\n",
    "#         if not dirs:  # Deepest folder\n",
    "#             final_patient_folders.append(root)\n",
    "\n",
    "# print(\"Final patient folders:\", final_patient_folders)\n",
    "\n",
    "# for folder1 in final_patient_folders:\n",
    "#     files_in_folder = os.listdir(folder1)\n",
    "#     tdms_files = [f for f in files_in_folder if f.endswith('.tdms')]\n",
    "#     if tdms_files:\n",
    "#         tdms_file_path = os.path.join(folder1, tdms_files[0])\n",
    "#         output = os.path.join(output_base_folder, os.path.basename(folder1), tdms_files[0])\n",
    "#         x = correct_tdms_start_time(tdms_file_path, output)\n",
    "#         print(\"TDMS file path:\", tdms_file_path)\n",
    "#         print(\"Output path:\", output)\n",
    "#         print(\"Corrected file:\", x)\n",
    "#         print(\"TDMS files:\", tdms_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e36fed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patient_path = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\"\n",
    "\n",
    "# Remove leading/trailing whitespace and ensure correct joining\n",
    "patient_path_clean = patient_path.strip().lstrip(\"\\\\/\")\n",
    "tdms_file_path = os.path.join(path_base_folder, patient_path_clean)\n",
    "\n",
    "# print(path_base_folder)\n",
    "# print(patient_path)\n",
    "# print(patient_path_1)\n",
    "# print(tdms_file_path)\n",
    "\n",
    "if not os.path.exists(tdms_file_path):\n",
    "    raise FileNotFoundError(f\"Path does not exist: {tdms_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b07aa420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TdmsFile.groups of <nptdms.tdms.TdmsFile object at 0x0000027981C964D0>>\n"
     ]
    }
   ],
   "source": [
    "td = TdmsFile(patient_path_clean)\n",
    "ch = td.groups\n",
    "\n",
    "print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "525dfe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('wf_start_time', numpy.datetime64('2017-06-20T11:44:43.000000')), ('wf_start_offset', 0.0), ('wf_increment', 0.001953125), ('wf_samples', 132710144), ('DigitalMaximum', 32767), ('DigitalMinimum', -32768), ('NI_ChannelName', 'EKG'), ('NI_ExpIsRelativeTime', False), ('wf_time_pref', 'absolute'), ('NI_ExpStartTimeStamp', numpy.datetime64('2017-06-20T11:44:43.000000')), ('NI_ExpTimeStamp', numpy.datetime64('2017-06-20T11:44:43.000000')), ('NI_ExpXDimension', 't'), ('wf_xname', 'Time'), ('wf_xunit_string', 's'), ('NI_UnitDescription', 'uV'), ('unit_string', 'uV'), ('PhysicalMaximum', 90298.0), ('PhysicalMinimum', -90298.0), ('PreFilter', ''), ('TransducerType', ''), ('ValsPerRec', 128)])\n",
      "TDMS File Properties:\n",
      "OrderedDict([('name', 'Sidsel Borg Nielsen 20062017 kl_134443 __23062017_kl_130000.tdms'), ('author', 'Jesper'), ('description', ''), ('registertxt1', 'Written by National Instruments LabVIEW')])\n",
      "\n",
      "TDMS Groups:\n",
      "Group: Untitled\n",
      "Properties: OrderedDict([('wf_xcolumns', 'None')])\n",
      "Channels: ['EKG']\n",
      "  Channel: EKG\n",
      "  Properties: OrderedDict([('wf_start_time', numpy.datetime64('2017-06-20T11:44:43.000000')), ('wf_start_offset', 0.0), ('wf_increment', 0.001953125), ('wf_samples', 132710144), ('DigitalMaximum', 32767), ('DigitalMinimum', -32768), ('NI_ChannelName', 'EKG'), ('NI_ExpIsRelativeTime', False), ('wf_time_pref', 'absolute'), ('NI_ExpStartTimeStamp', numpy.datetime64('2017-06-20T11:44:43.000000')), ('NI_ExpTimeStamp', numpy.datetime64('2017-06-20T11:44:43.000000')), ('NI_ExpXDimension', 't'), ('wf_xname', 'Time'), ('wf_xunit_string', 's'), ('NI_UnitDescription', 'uV'), ('unit_string', 'uV'), ('PhysicalMaximum', 90298.0), ('PhysicalMinimum', -90298.0), ('PreFilter', ''), ('TransducerType', ''), ('ValsPerRec', 128)])\n",
      "  First 5 data points: [516.69718471 541.49864958 574.5672694  591.10157931 591.10157931]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ch = td.groups()[0].channels()[0]\n",
    "\n",
    "# Data\n",
    "x = ch.data.astype(float)\n",
    "\n",
    "print(ch.properties)\n",
    "\n",
    "# Print the first few lines of raw TDMS file metadata\n",
    "print(\"TDMS File Properties:\")\n",
    "print(td.properties)\n",
    "\n",
    "tdms_prop = td.properties\n",
    "\n",
    "group = td.groups()\n",
    "\n",
    "print(\"\\nTDMS Groups:\")\n",
    "for group in td.groups():\n",
    "    print(f\"Group: {group.name}\")\n",
    "    print(\"Properties:\", group.properties)\n",
    "    print(\"Channels:\", [ch.name for ch in group.channels()])\n",
    "    for channel in group.channels():\n",
    "        print(f\"  Channel: {channel.name}\")\n",
    "        print(\"  Properties:\", channel.properties)\n",
    "        print(\"  First 5 data points:\", channel.data[:5])\n",
    "        print()\n",
    "# Samplingfrekvens\n",
    "inc = ch.properties.get(\"wf_increment\", None)\n",
    "\n",
    "# Starttid\n",
    "start_time = ch.properties.get(\"wf_start_time\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b156a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TdmsGroup with path /'Untitled'>\n",
      "<nptdms.tdms.TdmsFile object at 0x0000027981455650>\n"
     ]
    }
   ],
   "source": [
    "# print(group)\n",
    "# print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3281d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-12T09:05:02.000000\n",
      "[ -70.27081712   -1.37785916   37.2021973  ... -125.38518349 -128.14090181\n",
      " -125.38518349]\n"
     ]
    }
   ],
   "source": [
    "# prop = ch.properties\n",
    "\n",
    "# print(prop[\"wf_start_time\"])\n",
    "\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074bea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thue Bundgaard 12_10_2016 kl_110502__14_10_2016_ kl_080000.tdms\n"
     ]
    }
   ],
   "source": [
    "# start_time = td.properties[\"name\"]\n",
    "# print(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15f67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed start datetime: 2016-10-12 11:05:02\n",
      "Parsed end datetime: 2016-10-14 08:00:00\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Extract start and end datetime information from the start_time string\n",
    "# datetime_matches = re.findall(r'\\d{2}_\\d{2}_\\d{4} kl_?\\d{6}', start_time)\n",
    "\n",
    "# datetime_end = re.findall(r'__\\d{2}_\\d{2}_\\d{4}_ kl_?\\d{6}', start_time)\n",
    "\n",
    "# # Parse start datetime\n",
    "# if len(datetime_matches) > 0:\n",
    "#     start_dt_str = datetime_matches[0].replace(' kl_', ' ').replace(' kl', ' ')\n",
    "#     start_datetime = datetime.strptime(start_dt_str, '%d_%m_%Y %H%M%S')\n",
    "#     print(\"Parsed start datetime:\", start_datetime)\n",
    "# else:\n",
    "#     start_datetime = None\n",
    "#     print(\"No valid start datetime found.\")\n",
    "\n",
    "# # Parse end datetime\n",
    "# if len(datetime_end) > 0:\n",
    "#     end_dt_str = datetime_end[0].replace(' kl_', ' ').replace(' kl', ' ')\n",
    "#     end_datetime = datetime.strptime(end_dt_str, '__%d_%m_%Y_ %H%M%S')\n",
    "#     print(\"Parsed end datetime:\", end_datetime)\n",
    "# else:\n",
    "#     end_datetime = None\n",
    "#     print(\"No valid end datetime found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af5f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected start_time: 2016-10-12T11:05:02.000000\n",
      "Before correction: 2016-10-12T11:05:02.000000\n",
      "After correction: 2016-10-12T11:05:02.000000\n",
      "Kasper Viken Jensen\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Update start_time with the correct date, hour, and minute from start_datetime\n",
    "\n",
    "# corrected_start_time = np.datetime64(start_datetime)\n",
    "# print(\"Corrected start_time:\", corrected_start_time)\n",
    "\n",
    "# print(\"Before correction:\", td.groups()[0].channels()[0].properties[\"wf_start_time\"])\n",
    "# td.groups()[0].channels()[0].properties[\"wf_start_time\"] = corrected_start_time\n",
    "# print(\"After correction:\", td.groups()[0].channels()[0].properties[\"wf_start_time\"])\n",
    "\n",
    "# td.properties[\"Corrected_by: \"] = \"Kasper Viken Jensen\"\n",
    "# print(td.properties[\"Corrected_by: \"])\n",
    "\n",
    "# # prop[\"wf_start_time\"] = corrected_start_time\n",
    "# # print(prop[\"wf_start_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e12c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kasper Viken Jensen\n",
      "OrderedDict([('name', 'Thue Bundgaard 12_10_2016 kl_110502__14_10_2016_ kl_080000.tdms'), ('author', 'Jesper'), ('description', ''), ('registertxt1', 'Written by National Instruments LabVIEW'), ('Corrected_by: ', 'Kasper Viken Jensen')])\n"
     ]
    }
   ],
   "source": [
    "# tdms_prop[\"Corrected_by: \"] = \"Kasper Viken Jensen\"\n",
    "\n",
    "# print(tdms_prop[\"Corrected_by: \"])\n",
    "# print(tdms_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896245c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
