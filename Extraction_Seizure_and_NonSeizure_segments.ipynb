{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2e1d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local paths - modify as needed\n",
    "BASE_PATIENTS_DIR   = r\"E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\" # Root directory containing patient data folders\n",
    "BASE_ANNOTATION_DIR = r\"E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\" # Root directory containing annotation files\n",
    "#OUTPUT_ROOT         = r\"E:\\ML algoritme tl anfaldsdetektion vha HRV\\LabView-Results_Excluded_seizures_removed\\NonResponders\" # Root directory for output files\n",
    "OUTPUT_ROOT         = r\"E:\\ML algoritme tl anfaldsdetektion vha HRV\\LabView-Results_Excluded_seizures_removed\\Responders\" # Root directory for output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "818c7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seizure extraction utilities\n",
    "# - Lightweight helpers to map TDMS file times to sample indices, slice windows,\n",
    "#   save CSVs and produce simple ECG plots (absolute or relative time axis).\n",
    "# - Assumptions:\n",
    "#     * TDMS timestamps that are naive are treated as UTC if TDMS_NAIVE_IS_UTC is True.\n",
    "#     * TARGET_TZ is Europe/Copenhagen and used when converting to local naive datetimes.\n",
    "# Notes:\n",
    "#     * Concrete TDMS parsing and annotation parsing live in other cells.\n",
    "#     * This cell purposefully contains only compact utilities and configuration.\n",
    "\n",
    "from nptdms import TdmsFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import arrow\n",
    "from zoneinfo import ZoneInfo  # py>=3.9\n",
    "import gc\n",
    "import psutil\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Processing configuration (adjust in the notebook as needed)\n",
    "TARGET_TZ = ZoneInfo(\"Europe/Copenhagen\")\n",
    "TDMS_NAIVE_IS_UTC = True\n",
    "\n",
    "# Visualization / extraction parameters\n",
    "X_AXIS_MODE = \"absolute\"     # \"absolute\" or \"relative\"\n",
    "TIME_TZ = \"local\"\n",
    "PAD_MIN = 2                  # padding (min) added around seizure windows\n",
    "NONSEIZURE_OFFSET_MIN = 20   # offset (min) for non-seizure window selection\n",
    "MAX_PLOT_POINTS = 200_000    # downsample threshold when plotting\n",
    "\n",
    "# --------------------------\n",
    "# Helper utilities\n",
    "# --------------------------\n",
    "\n",
    "def slice_window(signal_length, fs, i_start, i_end, pad_min):\n",
    "    \"\"\"\n",
    "    Compute inclusive window [i0, i1) around event indices with padding (minutes).\n",
    "    Returns (i0, i1) clipped to [0, signal_length].\n",
    "    - signal_length: int, total samples in the recording\n",
    "    - fs: sampling frequency in Hz\n",
    "    - i_start, i_end: event sample indices (i_end is exclusive)\n",
    "    - pad_min: padding in minutes\n",
    "    \"\"\"\n",
    "    pad = int(round(pad_min * 60 * fs))\n",
    "    i0 = max(0, i_start - pad)\n",
    "    i1 = min(signal_length, i_end + pad)\n",
    "    return i0, i1\n",
    "\n",
    "def save_csv(path, t_rel_s, ecg):\n",
    "    \"\"\"\n",
    "    Save relative time (seconds) and ECG samples to CSV.\n",
    "    - path: output filepath\n",
    "    - t_rel_s: 1D array-like of times in seconds relative to segment start\n",
    "    - ecg: 1D array-like of ECG samples\n",
    "    \"\"\"\n",
    "    pd.DataFrame({\"t_rel_s\": np.asarray(t_rel_s), \"ecg\": np.asarray(ecg)}).to_csv(path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "def _format_dt(py_dt):\n",
    "    \"\"\"Format datetime for titles/filenames.\"\"\"\n",
    "    return py_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def make_time_axis(ax):\n",
    "    \"\"\"Configure matplotlib axis to show time-of-day with sensible ticks.\"\"\"\n",
    "    locator = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "    for lbl in ax.get_xticklabels():\n",
    "        lbl.set_rotation(15)\n",
    "        lbl.set_horizontalalignment('right')\n",
    "\n",
    "def build_segment_datetimes(start_local_dt, start_index, end_index, wf_inc):\n",
    "    \"\"\"\n",
    "    Given the local naive start time of the TDMS file and sample indices,\n",
    "    compute the segment start and end datetimes.\n",
    "    - wf_inc: seconds per sample (1 / fs when fs is constant)\n",
    "    - end_index is treated as exclusive here; the end time corresponds to sample end_index-1.\n",
    "    \"\"\"\n",
    "    seg_start = start_local_dt + dt.timedelta(seconds=start_index * wf_inc)\n",
    "    seg_end = start_local_dt + dt.timedelta(seconds=(end_index - 1) * wf_inc)\n",
    "    return seg_start, seg_end\n",
    "\n",
    "def plot_seizure(out_png, y, fs, wf_inc, tdms_start_local, seg_i0, seg_i1, st_idx, et_idx,\n",
    "                 title_prefix=\"\", x_axis_mode=\"absolute\"):\n",
    "    \"\"\"\n",
    "    Plot a segment containing a seizure and save to PNG.\n",
    "    - y: ECG samples for the plotted segment (length = seg_i1 - seg_i0)\n",
    "    - fs: sampling frequency (Hz)\n",
    "    - wf_inc: seconds per sample (usually 1.0 / fs)\n",
    "    - tdms_start_local: naive local datetime corresponding to sample index 0 of the file\n",
    "    - seg_i0, seg_i1: segment sample range [seg_i0, seg_i1)\n",
    "    - st_idx, et_idx: seizure start/end sample indices (absolute within the file)\n",
    "    - x_axis_mode: \"absolute\" to show clock times, \"relative\" to show seconds from segment start\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    t_rel = np.arange(n) / fs\n",
    "\n",
    "    # Relative positions of seizure markers (seconds from segment start)\n",
    "    start_rel_s = (st_idx - seg_i0) / fs\n",
    "    end_rel_s = (et_idx - seg_i0) / fs\n",
    "\n",
    "    # Absolute datetimes for axis and marker placement\n",
    "    seg_start, seg_end = build_segment_datetimes(tdms_start_local, seg_i0, seg_i1, wf_inc)\n",
    "    event_start = tdms_start_local + dt.timedelta(seconds=st_idx * wf_inc)\n",
    "    event_end = tdms_start_local + dt.timedelta(seconds=et_idx * wf_inc)\n",
    "\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    if x_axis_mode == \"absolute\":\n",
    "        # Build datetimes for each sample (use wf_inc in case fs is non-integer)\n",
    "        datetimes_py = [seg_start + dt.timedelta(seconds=i * wf_inc) for i in range(n)]\n",
    "        ax.plot(datetimes_py, y, label=\"ECG\")\n",
    "        ax.axvline(event_start, linestyle=\"--\", color=\"C1\", label=\"Seizure start\")\n",
    "        ax.axvline(event_end, linestyle=\"--\", color=\"C2\", label=\"Seizure end\")\n",
    "        make_time_axis(ax)\n",
    "        ax.set_xlabel(\"Time (HH:MM:SS)\")\n",
    "    else:\n",
    "        ax.plot(t_rel, y, label=\"ECG\")\n",
    "        ax.axvline(start_rel_s, linestyle=\"--\", color=\"C1\", label=\"Seizure start\")\n",
    "        ax.axvline(end_rel_s, linestyle=\"--\", color=\"C2\", label=\"Seizure end\")\n",
    "        ax.set_xlabel(\"Time (s) relative to segment start\")\n",
    "\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "    ax.set_title(\n",
    "        f\"{title_prefix}  |  Window: {_format_dt(seg_start)} → {_format_dt(seg_end)}  \"\n",
    "        f\"(Event: {_format_dt(event_start)} → {_format_dt(event_end)})\"\n",
    "    )\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def _make_time_axis(ax):\n",
    "    \"\"\"\n",
    "    Configure a matplotlib axis for time-series x-axis:\n",
    "      - automatic locator for nice tick spacing\n",
    "      - HH:MM:SS formatting\n",
    "      - slight rotation for readability\n",
    "    Requires matplotlib.dates as mdates to be available in the notebook.\n",
    "    \"\"\"\n",
    "    locator = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "    for lab in ax.get_xticklabels():\n",
    "        lab.set_rotation(15)\n",
    "        lab.set_horizontalalignment('right')\n",
    "\n",
    "\n",
    "def _seg_times(tdms_start_naive, i0, i1, wf_inc):\n",
    "    \"\"\"\n",
    "    Return start and end datetimes for a waveform segment.\n",
    "\n",
    "    tdms_start_naive: naive (tz-unaware) datetime for the TDMS recording start\n",
    "    i0, i1: sample indices (i1 is exclusive in downstream logic, so end uses i1-1)\n",
    "    wf_inc: seconds per sample\n",
    "    \"\"\"\n",
    "    seg_start = tdms_start_naive + dt.timedelta(seconds=i0 * wf_inc)\n",
    "    seg_end = tdms_start_naive + dt.timedelta(seconds=(i1 - 1) * wf_inc)\n",
    "    return seg_start, seg_end\n",
    "\n",
    "\n",
    "def _thin_for_plot(y, max_points=MAX_PLOT_POINTS):\n",
    "    \"\"\"\n",
    "    Return indices and thinned values for plotting long 1-D arrays.\n",
    "    If length <= max_points the original array is returned (indices are arange).\n",
    "    Otherwise a uniform down-sample with ceil(n / max_points) step is used.\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    if n <= max_points:\n",
    "        return np.arange(n), y\n",
    "    step = int(np.ceil(n / max_points))\n",
    "    idx = np.arange(0, n, step, dtype=int)\n",
    "    return idx, y[idx]\n",
    "\n",
    "\n",
    "def to_naive_local(dt_like):\n",
    "    \"\"\"\n",
    "    Convert various datetime-like inputs to a naive (tz-unaware) local datetime.\n",
    "\n",
    "    Accepts: arrow.Arrow, pandas.Timestamp, datetime.datetime, strings, etc.\n",
    "    Returns:\n",
    "      - datetime.datetime (tzinfo removed, converted to local timezone first when needed)\n",
    "      - None if the input cannot be interpreted\n",
    "    \"\"\"\n",
    "    # Arrow object\n",
    "    if isinstance(dt_like, arrow.Arrow):\n",
    "        return dt_like.to(\"local\").naive\n",
    "\n",
    "    # Python datetime\n",
    "    if isinstance(dt_like, dt.datetime):\n",
    "        if dt_like.tzinfo is not None:\n",
    "            # convert to system local timezone then drop tzinfo\n",
    "            return dt_like.astimezone().replace(tzinfo=None)\n",
    "        return dt_like\n",
    "\n",
    "    # pandas.Timestamp\n",
    "    if isinstance(dt_like, pd.Timestamp):\n",
    "        py = dt_like.to_pydatetime()\n",
    "        if py.tzinfo is not None:\n",
    "            return py.astimezone().replace(tzinfo=None)\n",
    "        return py\n",
    "\n",
    "    # Fallback: try pandas parsing (handles strings, numbers, etc.)\n",
    "    try:\n",
    "        ts = pd.to_datetime(dt_like, errors=\"coerce\")\n",
    "        if pd.isna(ts):\n",
    "            return None\n",
    "        py = ts.to_pydatetime()\n",
    "        if py.tzinfo is not None:\n",
    "            return py.astimezone().replace(tzinfo=None)\n",
    "        return py\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _time_to_timedelta(t):\n",
    "    \"\"\"\n",
    "    Helper: convert a time-like object with hour/minute/second/microsecond\n",
    "    attributes into a pandas Timedelta.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        pd.to_timedelta(t.hour, unit=\"h\")\n",
    "        + pd.to_timedelta(t.minute, unit=\"m\")\n",
    "        + pd.to_timedelta(t.second, unit=\"s\")\n",
    "        + pd.to_timedelta(t.microsecond, unit=\"us\")\n",
    "    )\n",
    "\n",
    "\n",
    "def _parse_time_cell(date_cell, time_cell):\n",
    "    \"\"\"\n",
    "    Combine a date cell and a time cell into a single pandas.Timestamp (local).\n",
    "    Handles:\n",
    "      - Excel float times (fraction of day)\n",
    "      - strings like \"12:30:00\"\n",
    "      - pandas.Timestamp, datetime.datetime, datetime.time\n",
    "      - NaT / NaN\n",
    "    Returns pd.NaT when parsing fails.\n",
    "\n",
    "    Note: date is normalized (time-of-day zeroed) and time is added as a timedelta.\n",
    "    \"\"\"\n",
    "    if pd.isna(time_cell) or pd.isna(date_cell):\n",
    "        return pd.NaT\n",
    "\n",
    "    # Normalize date (drop time component)\n",
    "    date_ts = pd.to_datetime(date_cell, errors=\"coerce\", dayfirst=True)\n",
    "    if pd.isna(date_ts):\n",
    "        return pd.NaT\n",
    "    date_norm = date_ts.normalize()  # pandas.Timestamp at 00:00:00\n",
    "\n",
    "    # Excel float: fraction of a day\n",
    "    if isinstance(time_cell, (int, float, np.integer, np.floating)):\n",
    "        return date_norm + pd.to_timedelta(float(time_cell), unit=\"D\")\n",
    "\n",
    "    # pandas.Timestamp\n",
    "    if isinstance(time_cell, pd.Timestamp):\n",
    "        return date_norm + _time_to_timedelta(time_cell)\n",
    "\n",
    "    # datetime.datetime\n",
    "    if isinstance(time_cell, dt.datetime):\n",
    "        return date_norm + _time_to_timedelta(time_cell)\n",
    "\n",
    "    # datetime.time -> convert to Timedelta relative to date_norm\n",
    "    if isinstance(time_cell, dt.time):\n",
    "        # use attributes of time object\n",
    "        return date_norm + pd.to_timedelta(time_cell.hour, unit=\"h\") + pd.to_timedelta(\n",
    "            time_cell.minute, unit=\"m\"\n",
    "        ) + pd.to_timedelta(time_cell.second, unit=\"s\") + pd.to_timedelta(\n",
    "            time_cell.microsecond, unit=\"us\"\n",
    "        )\n",
    "\n",
    "    # Strings and other types: attempt parsing\n",
    "    s = str(time_cell).strip()\n",
    "    if not s or s.lower() in {\"nan\", \"none\", \"na\"}:\n",
    "        return pd.NaT\n",
    "    parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "    if pd.isna(parsed):\n",
    "        return pd.NaT\n",
    "\n",
    "    return date_norm + _time_to_timedelta(parsed)\n",
    "def _seconds_since_midnight(ts):\n",
    "    \"\"\"\n",
    "    Return seconds since midnight for a (pandas) timestamp-like object.\n",
    "    Returns np.nan for NA inputs.\n",
    "    \"\"\"\n",
    "    if pd.isna(ts):\n",
    "        return np.nan\n",
    "    return ts.hour * 3600 + ts.minute * 60 + ts.second + ts.microsecond / 1e6\n",
    "\n",
    "\n",
    "def load_seizure_annotations_file(path_excel):\n",
    "    \"\"\"\n",
    "    Load a single Excel annotation file where the table header is on row 7 (use header=6).\n",
    "\n",
    "    Expected columns (any reasonable variant/casing), examples:\n",
    "      - Seizure number / Anfald nr\n",
    "      - Date / Dato\n",
    "      - Seizurestart clinic (tt:mm:ss) / Anfaldsstart Klinisk\n",
    "      - Seizurestart EEG (tt:mm:ss) / Anfaldsstart EEG\n",
    "      - Seizureend clinic / Anfaldstop Klinisk\n",
    "      - Seizureend EEG / Anfaldstop EEG\n",
    "      - Seizure type / Anfaldstype\n",
    "      - other / Evt. bemærkninger\n",
    "\n",
    "    Returns a DataFrame with normalized columns:\n",
    "      seizure_number, date (normalized to midnight), start_clinic, start_eeg, end_clinic, end_eeg,\n",
    "      seizure_type, other, plus helper columns with seconds_since_midnight and hour for starts/ends,\n",
    "      and a source_file column with the base filename.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(path_excel, header=6)\n",
    "\n",
    "    # Normalize column names by stripping whitespace\n",
    "    cols = {c: str(c).strip() for c in df.columns}\n",
    "    df.rename(columns=cols, inplace=True)\n",
    "\n",
    "    # Helper to find a column containing a keyword (case-insensitive substring match)\n",
    "    def find_col(key):\n",
    "        key_l = key.lower()\n",
    "        for c in df.columns:\n",
    "            if key_l in c.lower():\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    # Try multiple language/alias variants for important columns\n",
    "    num_col = find_col('Anfald nr') or find_col('seizure')\n",
    "    date_col = find_col('Dato')\n",
    "    s_clin_col = find_col('Anfaldsstart Klinisk') or find_col('Anfaldsstart klinisk')\n",
    "    s_eeg_col = find_col('Anfaldsstart EEG') or find_col('Anfaldsstart eeg')\n",
    "    e_clin_col = find_col('Anfaldstop Klinisk') or find_col('Anfaldstop klinisk')\n",
    "    e_eeg_col = find_col('Anfaldstop EEG') or find_col('Anfaldstop eeg')\n",
    "    type_col = find_col('Anfaldstype') or find_col('anfaldstype')\n",
    "    other_col = find_col('Evt. bemærkninger') or find_col('note') or find_col('other')\n",
    "\n",
    "    res = pd.DataFrame()\n",
    "\n",
    "    # Seizure number: if not present use the row index + 1\n",
    "    res['seizure_number'] = df[num_col] if num_col else (df.index + 1)\n",
    "\n",
    "    # Normalize date column to midnight timestamps if present, else NaT\n",
    "    if date_col:\n",
    "        res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n",
    "    else:\n",
    "        res['date'] = pd.NaT\n",
    "\n",
    "    # Combine date + time for each time column using a parser _parse_time_cell (assumed defined elsewhere).\n",
    "    # Use Series.get on the row to avoid KeyError if date_col is missing.\n",
    "    res['start_clinic'] = df.apply(\n",
    "        lambda r: _parse_time_cell(r.get(date_col), r.get(s_clin_col)) if s_clin_col else pd.NaT,\n",
    "        axis=1\n",
    "    )\n",
    "    res['start_eeg'] = df.apply(\n",
    "        lambda r: _parse_time_cell(r.get(date_col), r.get(s_eeg_col)) if s_eeg_col else pd.NaT,\n",
    "        axis=1\n",
    "    )\n",
    "    res['end_clinic'] = df.apply(\n",
    "        lambda r: _parse_time_cell(r.get(date_col), r.get(e_clin_col)) if e_clin_col else pd.NaT,\n",
    "        axis=1\n",
    "    )\n",
    "    res['end_eeg'] = df.apply(\n",
    "        lambda r: _parse_time_cell(r.get(date_col), r.get(e_eeg_col)) if e_eeg_col else pd.NaT,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Copy type/other columns if present, otherwise fill with None\n",
    "    res['seizure_type'] = df[type_col] if type_col else None\n",
    "    res['other'] = df[other_col] if other_col else None\n",
    "\n",
    "    # Helper columns for statistics: seconds since midnight and hour of day\n",
    "    for prefix in ['start_clinic', 'start_eeg', 'end_clinic', 'end_eeg']:\n",
    "        res[f'{prefix}_seconds'] = res[prefix].apply(_seconds_since_midnight)\n",
    "        # Use .dt.hour safely; if column contains non-datetime values this will raise, but original code assumed datetimes\n",
    "        res[f'{prefix}_hour'] = res[prefix].dt.hour\n",
    "\n",
    "    # Keep original source filename for traceability\n",
    "    res['source_file'] = os.path.basename(path_excel)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def to_naive_local_dt(v):\n",
    "    \"\"\"\n",
    "    Convert a timestamp-like value to a naive (tz-unaware) local datetime in TARGET_TZ.\n",
    "\n",
    "    - Parses with pandas.to_datetime (dayfirst=True).\n",
    "    - If parsed value is tz-aware, converts to TARGET_TZ and drops tzinfo.\n",
    "    - Returns None for unparsable/NA values.\n",
    "    \"\"\"\n",
    "    ts = pd.to_datetime(v, errors=\"coerce\", dayfirst=True, utc=False)\n",
    "    if pd.isna(ts):\n",
    "        return None\n",
    "    py = ts.to_pydatetime()\n",
    "    # If a cell unexpectedly contains a timezone-aware datetime, convert to TARGET_TZ then drop tzinfo\n",
    "    if py.tzinfo:\n",
    "        py = py.astimezone(TARGET_TZ).replace(tzinfo=None)\n",
    "    return py  # naive local datetime\n",
    "\n",
    "\n",
    "def events_from_annotation_df(df, prefer=\"clinic\"):\n",
    "    \"\"\"\n",
    "    Convert DataFrame from load_seizure_annotations_file(...) into a sorted list of (start, end)\n",
    "    tuples as naive local datetime objects.\n",
    "\n",
    "    prefer: \"clinic\" or \"eeg\" to choose which start/end pair to prefer when both exist.\n",
    "    \"\"\"\n",
    "    starts = df.get(f\"start_{prefer}\", df.get(\"start_clinic\"))\n",
    "    ends = df.get(f\"end_{prefer}\", df.get(\"end_clinic\"))\n",
    "\n",
    "    events = []\n",
    "    for st, et in zip(starts, ends):\n",
    "        if pd.isna(st) or pd.isna(et):\n",
    "            continue\n",
    "        st_dt = to_naive_local_dt(st)\n",
    "        et_dt = to_naive_local_dt(et)\n",
    "        # Only keep valid intervals where end is after start\n",
    "        if st_dt and et_dt and et_dt > st_dt:\n",
    "            events.append((st_dt, et_dt))\n",
    "\n",
    "    # Sort by start time and return\n",
    "    events.sort(key=lambda t: t[0])\n",
    "    return events\n",
    "\n",
    "\n",
    "def ensure_dir(p):\n",
    "    \"\"\"Ensure directory p exists (create parents if necessary).\"\"\"\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def find_ecg_channel(td) -> Optional[object]:\n",
    "    \"\"\"\n",
    "    Find the EKG/ECG channel across all groups in a TDMS file.\n",
    "    Matches common names (case-insensitive): 'EKG', 'ECG', 'Lead I', 'Lead1'.\n",
    "    Fallback: return the first channel from the first group if no match is found.\n",
    "    \"\"\"\n",
    "    # Regex matches 'ekg', 'ecg', 'lead i', 'lead1' (allows optional whitespace)\n",
    "    name_re = re.compile(r\"^(ekg|ecg|lead\\s*1|lead\\s*i)$\", re.IGNORECASE)\n",
    "\n",
    "    first_channel = None\n",
    "    for group in td.groups():\n",
    "        channels = group.channels()\n",
    "        for ch in channels:\n",
    "            name = (ch.name or \"\").strip()\n",
    "            if name_re.match(name):\n",
    "                return ch\n",
    "            if first_channel is None:\n",
    "                first_channel = ch  # save as fallback\n",
    "    return first_channel\n",
    "\n",
    "def get_sampling_increment(ch) -> float:\n",
    "    \"\"\"\n",
    "    Get the sample increment (seconds per sample) from the channel properties.\n",
    "    Supported variants:\n",
    "      - 'wf_increment' (sec/sample)\n",
    "      - 'wf_sampling_rate' or 'sampling_rate' (Hz) -> return 1/fs\n",
    "      - 'wf_xscale' as a last resort\n",
    "    Raises RuntimeError if no valid value is found.\n",
    "    \"\"\"\n",
    "    props = getattr(ch, \"properties\", {}) or {}\n",
    "\n",
    "    # Prefer direct increment\n",
    "    inc = props.get(\"wf_increment\", None)\n",
    "    if inc is not None:\n",
    "        try:\n",
    "            inc = float(inc)\n",
    "            if inc > 0:\n",
    "                return inc\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Alternatively sampling rate in Hz\n",
    "    fs = props.get(\"wf_sampling_rate\", props.get(\"sampling_rate\", None))\n",
    "    if fs is not None:\n",
    "        try:\n",
    "            fs = float(fs)\n",
    "            if fs > 0:\n",
    "                return 1.0 / fs\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Last chance: wf_xscale\n",
    "    xscale = props.get(\"wf_xscale\", None)\n",
    "    if xscale is not None:\n",
    "        try:\n",
    "            inc = float(xscale)\n",
    "            if inc > 0:\n",
    "                return inc\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    raise RuntimeError(\"Cannot determine sampling increment (wf_increment/sampling_rate missing or invalid).\")\n",
    "\n",
    "def to_naive_local(dt_like) -> Optional[dt.datetime]:\n",
    "    \"\"\"\n",
    "    Convert various time-like inputs to a naive local datetime (tzinfo=None).\n",
    "    Accepts: arrow.Arrow, datetime, pandas-parsable strings.\n",
    "    Returns None if conversion fails.\n",
    "    \"\"\"\n",
    "    if isinstance(dt_like, arrow.Arrow):\n",
    "        return dt_like.to('local').naive\n",
    "\n",
    "    if isinstance(dt_like, dt.datetime):\n",
    "        # if aware -> convert to local time and drop tzinfo\n",
    "        if dt_like.tzinfo:\n",
    "            return dt_like.astimezone().replace(tzinfo=None)\n",
    "        return dt_like\n",
    "\n",
    "    ts = pd.to_datetime(dt_like, errors='coerce')\n",
    "    if pd.isna(ts):\n",
    "        return None\n",
    "    py = ts.to_pydatetime()\n",
    "    return py.astimezone().replace(tzinfo=None) if py.tzinfo else py\n",
    "\n",
    "def get_tdms_start_local_naive(ch) -> Optional[dt.datetime]:\n",
    "    \"\"\"\n",
    "    Extract the start time from the channel properties and return a naive local datetime.\n",
    "    - If the timestamp is naive it is assumed to be UTC when TDMS_NAIVE_IS_UTC=True.\n",
    "    - Converts to TARGET_TZ (expected ZoneInfo) and removes tzinfo before returning.\n",
    "    Returns None if no known timestamp field is present or parsing fails.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        \"wf_start_time\", \"wf_start\", \"ni_exptimestamp\", \"ni_expisrelativetime\",\n",
    "        \"NI_ExpStartTime\", \"NI_T0\"\n",
    "    ]\n",
    "    props = getattr(ch, \"properties\", {}) or {}\n",
    "\n",
    "    for key in candidates:\n",
    "        if key not in props:\n",
    "            continue\n",
    "        val = props[key]\n",
    "        # Use pandas for tolerant parsing; do not force UTC here\n",
    "        ts = pd.to_datetime(val, errors=\"coerce\", utc=False)\n",
    "        if pd.isna(ts):\n",
    "            continue\n",
    "\n",
    "        py = ts.to_pydatetime()\n",
    "\n",
    "        # If naive: assume UTC or TARGET_TZ depending on global setting\n",
    "        if py.tzinfo is None:\n",
    "            if TDMS_NAIVE_IS_UTC:\n",
    "                py = py.replace(tzinfo=dt.timezone.utc)\n",
    "            else:\n",
    "                py = py.replace(tzinfo=TARGET_TZ)\n",
    "\n",
    "        # Convert to local zone (TARGET_TZ) and drop tzinfo -> naive local time\n",
    "        py_local = py.astimezone(TARGET_TZ)\n",
    "        return py_local.replace(tzinfo=None)\n",
    "\n",
    "    return None\n",
    "\n",
    "def tdms_load_ecg(tdms_path: str) -> Tuple[np.ndarray, float, Optional[dt.datetime], dict]:\n",
    "    \"\"\"\n",
    "    Load ECG/EGK from a TDMS file:\n",
    "      - returns (data_array, fs, start_local_naive, meta)\n",
    "    Meta includes channel name, sampling, number of samples, start time (ISO string) and group.\n",
    "    May raise errors if no channel is found or sampling cannot be determined.\n",
    "    \"\"\"\n",
    "    # Suppress noisy nptdms warnings during reading\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        td = TdmsFile.read(tdms_path)\n",
    "\n",
    "    ecg_ch = find_ecg_channel(td)\n",
    "    if ecg_ch is None:\n",
    "        raise RuntimeError(\"No channels found in the TDMS file.\")\n",
    "\n",
    "    # Data as float numpy array\n",
    "    x = np.asarray(ecg_ch.data, dtype=float)\n",
    "\n",
    "    # Sampling interval and frequency\n",
    "    wf_inc = get_sampling_increment(ecg_ch)\n",
    "    fs = 1.0 / wf_inc\n",
    "\n",
    "    # Start time as naive local datetime (may be None)\n",
    "    start_local_naive = get_tdms_start_local_naive(ecg_ch)\n",
    "\n",
    "    meta = {\n",
    "        \"channel\": getattr(ecg_ch, \"name\", None),\n",
    "        \"group\": getattr(getattr(ecg_ch, \"group\", None), \"name\", None),\n",
    "        \"fs\": fs,\n",
    "        \"wf_increment\": wf_inc,\n",
    "        \"n_samples\": int(x.shape[0]),\n",
    "        \"start_local_iso\": start_local_naive.strftime(\"%Y-%m-%d %H:%M:%S\") if start_local_naive else None,\n",
    "    }\n",
    "    return x, fs, start_local_naive, meta\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def extract_ids_from_relpath(rel_tdms: str) -> Tuple[Optional[int], Optional[int]]:\n",
    "    \"\"\"\n",
    "    Infer numeric patient and recording IDs from a relative TDMS path or filename.\n",
    "\n",
    "    Strategy (in order):\n",
    "      1) Search folder/file path for \"Patient <N>\" and \"Recording <M>\" (case-insensitive).\n",
    "      2) Parse filename patterns like:\n",
    "         - Patient_5_2.tdms  or  Patient-5-2.tdms  (patient=5, recording=2)\n",
    "         - Patient5-2.tdms\n",
    "      3) Fallback: filename \"Patient_5.tdms\" -> patient=5, recording=None\n",
    "\n",
    "    Returns (patient_id, recording_id) where either may be None if not found.\n",
    "    \"\"\"\n",
    "    s = rel_tdms.replace(\"\\\\\", \"/\")\n",
    "    patient_id: Optional[int] = None\n",
    "    rec_id: Optional[int] = None\n",
    "\n",
    "    # 1) Try to find \"patient <N>\" and \"recording <M>\" anywhere in the path\n",
    "    m_p = re.search(r\"(?<!\\d)\\bpatient\\s*(\\d+)\\b\", s, re.IGNORECASE)\n",
    "    m_r = re.search(r\"(?<!\\d)\\brecording\\s*(\\d+)\\b\", s, re.IGNORECASE)\n",
    "    if m_p:\n",
    "        patient_id = int(m_p.group(1))\n",
    "    if m_r:\n",
    "        rec_id = int(m_r.group(1))\n",
    "\n",
    "    # Use filename for additional heuristics\n",
    "    fname = Path(s).name\n",
    "\n",
    "    # 2) Filename with both patient and recording: Patient_<p>_<r>.tdms (robust to -, _, space)\n",
    "    if patient_id is None or rec_id is None:\n",
    "        m = re.search(r\"(?i)^patient[_\\s-]*?(\\d+)[_\\s-]+(\\d+)\\.tdms$\", fname)\n",
    "        if m:\n",
    "            if patient_id is None:\n",
    "                patient_id = int(m.group(1))\n",
    "            if rec_id is None:\n",
    "                rec_id = int(m.group(2))\n",
    "\n",
    "    # 3) Filename with only patient: Patient_<p>.tdms\n",
    "    if patient_id is None:\n",
    "        m = re.search(r\"(?i)^patient[_\\s-]*?(\\d+)\\.tdms$\", fname)\n",
    "        if m:\n",
    "            patient_id = int(m.group(1))\n",
    "\n",
    "    return patient_id, rec_id\n",
    "\n",
    "\n",
    "def discover_tdms_for_patient(\n",
    "    base_dir: str,\n",
    "    patient_selector: Optional[str] = None,\n",
    "    patient_id: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generator yielding (relative_path, absolute_path) for TDMS files for a single patient folder.\n",
    "\n",
    "    Behavior:\n",
    "      - Scans top-level entries in base_dir and inspects the first patient folder that contains TDMS files.\n",
    "      - If patient_selector is provided, only folders whose name contains that substring (case-insensitive)\n",
    "        are considered.\n",
    "      - If patient_id is provided, only folders with a 'Patient <id>' name are considered.\n",
    "      - Within a chosen patient folder TDMS files are collected recursively and yielded sorted by recording id\n",
    "        (if available) then alphabetically.\n",
    "      - Stops after yielding files from the first matching patient folder (intended behavior).\n",
    "    \"\"\"\n",
    "    base_dir = str(base_dir)\n",
    "    entries = sorted(os.listdir(base_dir), key=str.lower)\n",
    "\n",
    "    for entry in entries:\n",
    "        pdir = os.path.join(base_dir, entry)\n",
    "        if not os.path.isdir(pdir):\n",
    "            continue\n",
    "\n",
    "        # Apply selector filters\n",
    "        if patient_selector and patient_selector.lower() not in entry.lower():\n",
    "            continue\n",
    "        if patient_id is not None:\n",
    "            if not re.search(rf\"(?<!\\d)\\bpatient\\s*{patient_id}\\b\", entry, re.IGNORECASE):\n",
    "                continue\n",
    "\n",
    "        # Collect TDMS files under this patient folder\n",
    "        tdms_files = []\n",
    "        for root, _, files in os.walk(pdir):\n",
    "            for f in files:\n",
    "                fl = f.lower()\n",
    "                if not fl.endswith(\".tdms\"):\n",
    "                    continue\n",
    "                # skip index-like files if they appear\n",
    "                if fl.endswith(\".tdms_index\"):\n",
    "                    continue\n",
    "                abs_p = os.path.join(root, f)\n",
    "                rel_p = os.path.relpath(abs_p, base_dir)\n",
    "                tdms_files.append((rel_p, abs_p))\n",
    "\n",
    "        # Sort: prefer numeric recording id when available, otherwise alphabetical\n",
    "        def sort_key(pair):\n",
    "            rel_p, _ = pair\n",
    "            _, rec = extract_ids_from_relpath(rel_p)\n",
    "            return (999999 if rec is None else rec, rel_p.lower())\n",
    "\n",
    "        tdms_files.sort(key=sort_key)\n",
    "\n",
    "        # If we found any TDMS files, yield them and stop (we only process one patient folder)\n",
    "        if tdms_files:\n",
    "            for rel_p, abs_p in tdms_files:\n",
    "                yield rel_p, abs_p\n",
    "            return\n",
    "\n",
    "        # If the caller asked for a specific patient and this folder had no TDMS files, stop scanning.\n",
    "        if patient_selector or patient_id is not None:\n",
    "            return\n",
    "\n",
    "def map_cols(df, starts_list, ends_list):\n",
    "    # Return the first matching start/end column names from the DataFrame, case-insensitive.\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    start_col = next((cols[c.lower()] for c in starts_list if c.lower() in cols), None)\n",
    "    end_col   = next((cols[c.lower()] for c in ends_list   if c.lower() in cols), None)\n",
    "    return start_col, end_col\n",
    "\n",
    "START_ALIASES = [\n",
    "    \"Seizure start\",\"Anfald start\",\"Start\",\"Start time\",\"Starttid\",\"Onset\",\n",
    "    \"Seizure_start\",\"Start (UTC)\"\n",
    "]\n",
    "END_ALIASES   = [\n",
    "    \"Seizure end\",\"Anfald slut\",\"Slut\",\"End time\",\"Sluttid\",\"Offset\",\n",
    "    \"Seizure_end\",\"End (UTC)\"\n",
    "]\n",
    "REC_ALIASES   = [\"Recording\",\"Recording ID\",\"Recording_Id\",\"Rec\",\"Session\"]\n",
    "\n",
    "\n",
    "def parse_annotations_excel_or_csv(path_x, recording_id=None):\n",
    "    \"\"\"\n",
    "    Return a list of (start_dt, end_dt) as naive local datetimes.\n",
    "    Works with .xlsx/.xls (all sheets) and .csv.\n",
    "    Optionally filters rows by a 'recording' column if present.\n",
    "    \"\"\"\n",
    "    events = []\n",
    "    p = Path(path_x)\n",
    "    if p.suffix.lower() in (\".xlsx\", \".xls\"):\n",
    "        # Read all sheets from the Excel file\n",
    "        xls = pd.ExcelFile(path_x)\n",
    "        dfs = [xls.parse(sheet) for sheet in xls.sheet_names]\n",
    "    else:\n",
    "        # Single CSV file -> one DataFrame\n",
    "        dfs = [pd.read_csv(path_x)]\n",
    "\n",
    "    for df in dfs:\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # Optional: filter by recording id if a suitable column exists\n",
    "        # Find columns matching any of REC_ALIASES (exact or case-insensitive)\n",
    "        rec_cols = [\n",
    "            c for c in df.columns\n",
    "            if c.strip() in REC_ALIASES or c.strip().lower() in [a.lower() for a in REC_ALIASES]\n",
    "        ]\n",
    "        if rec_cols and recording_id is not None:\n",
    "            # Build a mask that matches the recording_id in any of the candidate columns.\n",
    "            mask_any = None\n",
    "            for c in rec_cols:\n",
    "                # Extract digits from the column and compare to recording_id as string\n",
    "                m = df[c].astype(str, copy=False).str.extract(r\"(\\d+)\")[0] == str(recording_id)\n",
    "                mask_any = m if mask_any is None else (mask_any | m)\n",
    "            df = df[mask_any.fillna(False)]\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # Column mapping (try original headers, then trimmed headers)\n",
    "        a_col, b_col = map_cols(df, START_ALIASES, END_ALIASES)\n",
    "        if not (a_col and b_col):\n",
    "            df2 = df.copy()\n",
    "            df2.columns = [c.strip() for c in df2.columns]\n",
    "            a_col, b_col = map_cols(df2, START_ALIASES, END_ALIASES)\n",
    "            if a_col and b_col:\n",
    "                df = df2\n",
    "\n",
    "        if a_col and b_col:\n",
    "            # Iterate over rows with non-null start and end values\n",
    "            for _, row in df[[a_col, b_col]].dropna().iterrows():\n",
    "                st = to_naive_local_dt(row[a_col])\n",
    "                et = to_naive_local_dt(row[b_col])\n",
    "                # Keep only valid intervals where end > start\n",
    "                if st and et and et > st:\n",
    "                    events.append((st, et))\n",
    "\n",
    "    # Deduplicate and sort by start time (now plain datetime, not Arrow)\n",
    "    events = sorted(set(events), key=lambda t: t[0])\n",
    "    return events\n",
    "\n",
    "\n",
    "def _read_lvm_header_datetime(path_lvm, header_lines=22):\n",
    "    \"\"\"Read 'Date' and 'Time' from LVM header and return naive local datetime or None.\n",
    "    \n",
    "    Danish: Læs 'Date' og 'Time' fra LVM-header og returnér naiv lokal datetime eller None.\n",
    "    \"\"\"\n",
    "    date_str, time_str = None, None\n",
    "    with open(path_lvm, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= header_lines:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            if line.lower().startswith(\"date\"):\n",
    "                # e.g.: Date    2016/10/12\n",
    "                parts = line.split(None, 1)\n",
    "                if len(parts) > 1:\n",
    "                    date_str = parts[1].strip()\n",
    "            elif line.lower().startswith(\"time\"):\n",
    "                # e.g.: Time    13:06:19,1816539465369109152  (or HH:MM:SS)\n",
    "                parts = line.split(None, 1)\n",
    "                if len(parts) > 1:\n",
    "                    # Replace comma with dot to handle fractional seconds using either separator\n",
    "                    time_str = parts[1].strip().replace(\",\", \".\")\n",
    "    if not date_str or not time_str:\n",
    "        return None\n",
    "    # Try to combine into a single timestamp\n",
    "    ts = pd.to_datetime(f\"{date_str} {time_str}\", errors=\"coerce\", dayfirst=False, utc=False)\n",
    "    if pd.isna(ts):\n",
    "        # Fallback: try parsing date and time separately\n",
    "        d = pd.to_datetime(date_str, errors=\"coerce\")\n",
    "        t = pd.to_datetime(time_str, errors=\"coerce\")\n",
    "        if pd.isna(d) or pd.isna(t):\n",
    "            return None\n",
    "        # Combine date and time components into a single Timestamp\n",
    "        ts = d.normalize() + pd.to_timedelta(t.hour, unit=\"h\") + pd.to_timedelta(t.minute, unit=\"m\") + pd.to_timedelta(t.second, unit=\"s\") + pd.to_timedelta(t.microsecond, unit=\"us\")\n",
    "    py = ts.to_pydatetime()\n",
    "    # If timezone-aware → convert to local timezone and drop tz info; otherwise return as-is\n",
    "    return py.astimezone().replace(tzinfo=None) if py.tzinfo else py\n",
    "\n",
    "def parse_annotations_lvm(path_lvm):\n",
    "    # Read header for possible base time (for relative X_Value)\n",
    "    base_dt = _read_lvm_header_datetime(path_lvm, header_lines=22)\n",
    "\n",
    "    df = pd.read_csv(path_lvm, sep=\"\\t\", decimal=\",\", engine=\"python\", skiprows=22, header=0)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    time_col = next((c for c in [\"X_Value\",\"X Value\",\"Time\",\"Timestamp\",\"DateTime\"] if c in df.columns), None)\n",
    "    comment_col = next((c for c in [\"Comment\",\"Comments\",\"Kommentar\"] if c in df.columns), None)\n",
    "    if not (time_col and comment_col):\n",
    "        return []\n",
    "\n",
    "    # find start/stop rows\n",
    "    starts, ends = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        cmt = str(row[comment_col]).lower()\n",
    "        if \"seizure\" in cmt and \"start\" in cmt:\n",
    "            starts.append(row)\n",
    "        elif \"seizure\" in cmt and (\"end\" in cmt or \"stop\" in cmt):\n",
    "            ends.append(row)\n",
    "\n",
    "    n = min(len(starts), len(ends))\n",
    "    events = []\n",
    "    for i in range(n):\n",
    "        v_start = starts[i][time_col]\n",
    "        v_end   = ends[i][time_col]\n",
    "\n",
    "        st_dt = None\n",
    "        et_dt = None\n",
    "\n",
    "        # Case A: absolute time in the column\n",
    "        st_abs = to_naive_local_dt(v_start)\n",
    "        et_abs = to_naive_local_dt(v_end)\n",
    "        if st_abs and et_abs:\n",
    "            st_dt, et_dt = st_abs, et_abs\n",
    "        else:\n",
    "            # Case B: relative time (seconds) in X_Value + base_dt from header\n",
    "            # (requires base_dt and numeric values)\n",
    "            if base_dt is not None:\n",
    "                try:\n",
    "                    st_sec = float(str(v_start).replace(\",\", \".\"))\n",
    "                    et_sec = float(str(v_end).replace(\",\", \".\"))\n",
    "                    st_dt = base_dt + dt.timedelta(seconds=st_sec)\n",
    "                    et_dt = base_dt + dt.timedelta(seconds=et_sec)\n",
    "                except Exception:\n",
    "                    st_dt = et_dt = None\n",
    "\n",
    "        if st_dt and et_dt and et_dt > st_dt:\n",
    "            events.append((st_dt, et_dt))\n",
    "\n",
    "    # deduplicate + sort\n",
    "    events = sorted(set(events), key=lambda t: t[0])\n",
    "    return events\n",
    "\n",
    "def find_annotations_for_patient(patient_id):\n",
    "    # Find annotation files for a given patient id under BASE_ANNOTATION_DIR.\n",
    "    pid = \"\" if patient_id is None else str(patient_id)\n",
    "    patterns = [\n",
    "        f\"**/*patient*{pid}*.xlsx\", f\"**/*patient*{pid}*.xls\",\n",
    "        f\"**/*patient*{pid}*.csv\",  f\"**/*patient*{pid}*.lvm\",\n",
    "        f\"**/*{pid}*.xlsx\", f\"**/*{pid}*.csv\", f\"**/*{pid}*.lvm\",\n",
    "    ]\n",
    "    found = []\n",
    "    for pat in patterns:\n",
    "        found += glob.glob(os.path.join(BASE_ANNOTATION_DIR, pat), recursive=True)\n",
    "\n",
    "    # unique + priority xlsx/xls > csv > lvm\n",
    "    def rank(p):\n",
    "        pl = p.lower()\n",
    "        if pl.endswith((\".xlsx\", \".xls\")): return 0\n",
    "        if pl.endswith(\".csv\"):            return 1\n",
    "        if pl.endswith(\".lvm\"):            return 2\n",
    "        return 9\n",
    "\n",
    "    return sorted(list(dict.fromkeys(found)), key=rank)\n",
    "\n",
    "def load_events_for_patient_with_excel(patient_id, base_annotation_dir):\n",
    "    # Load events for a patient by searching Excel files in the given base directory.\n",
    "    pid_str = str(patient_id)\n",
    "    pid_0 = pid_str.zfill(2)  # \"6\" -> \"06\"\n",
    "    exts = (\".xls\", \".xlsx\")\n",
    "\n",
    "    patterns = [\n",
    "        f\"**/patient {pid_str}.*\",\n",
    "        f\"**/patient_{pid_str}.*\",\n",
    "        f\"**/patient{pid_str}.*\",\n",
    "        f\"**/patient {pid_0}.*\",\n",
    "        f\"**/patient_{pid_0}.*\",\n",
    "        f\"**/pt {pid_str}.*\",\n",
    "        f\"**/pt_{pid_str}.*\",\n",
    "    ]\n",
    "\n",
    "    # find candidates\n",
    "    cands = []\n",
    "    for pat in patterns:\n",
    "        cands += glob.glob(os.path.join(base_annotation_dir, pat), recursive=True)\n",
    "\n",
    "    # filter to xls/xlsx\n",
    "    cands = [c for c in cands if c.lower().endswith(exts)]  \n",
    "\n",
    "    # try them in order\n",
    "    for path_excel in sorted(dict.fromkeys(cands)):\n",
    "        try:\n",
    "            df = load_seizure_annotations_file(path_excel)\n",
    "            ev = events_from_annotation_df(df, prefer=\"clinic\")\n",
    "            if ev:\n",
    "                return ev, path_excel\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: kunne ikke parse {path_excel}: {e}\")  # Warning: could not parse ...\n",
    "\n",
    "    return [], None\n",
    "\n",
    "def load_events_for_patient_and_enrollment(patient_id, enrollment_name, base_annotation_dir):\n",
    "    \"\"\"\n",
    "    Hvis enrollment_name er fx 'enrollment A', så leder vi efter:\n",
    "      Patient 8a.*, Patient 8_a.*, Patient 8-a.*\n",
    "    Ellers falder vi tilbage til almindelig patient-fil.\n",
    "\n",
    "    If enrollment_name is e.g. 'enrollment A', we look for:\n",
    "      Patient 8a.*, Patient 8_a.*, Patient 8-a.*\n",
    "    Otherwise fall back to the regular patient file.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) if no enrollment -> use the default search\n",
    "    if not enrollment_name:\n",
    "        return load_events_for_patient_with_excel(patient_id, base_annotation_dir)\n",
    "\n",
    "    # 2) find letter/suffix from the enrollment name\n",
    "    # \"enrollment A\" -> \"a\"\n",
    "    m = re.search(r\"enrollment\\s*([A-Za-z])\", enrollment_name, re.IGNORECASE)\n",
    "    if not m:\n",
    "        # if we cannot read a letter -> use the standard search\n",
    "        return load_events_for_patient_with_excel(patient_id, base_annotation_dir)\n",
    "    letter = m.group(1).lower()\n",
    "\n",
    "    pid_str = str(patient_id)\n",
    "    patterns = [\n",
    "        f\"**/patient {pid_str}{letter}.xls\",\n",
    "        f\"**/patient {pid_str}{letter}.xlsx\",\n",
    "        f\"**/patient_{pid_str}{letter}.xls\",\n",
    "        f\"**/patient_{pid_str}{letter}.xlsx\",\n",
    "        f\"**/patient {pid_str}{letter} *.xls\",   # fx 'Patient 8b noget.xls'\n",
    "        f\"**/patient {pid_str}{letter} *.xlsx\",\n",
    "    ]\n",
    "\n",
    "    cands = []\n",
    "    for pat in patterns:\n",
    "        cands += glob.glob(os.path.join(base_annotation_dir, pat), recursive=True)\n",
    "\n",
    "    # if we found something, parse the first usable file\n",
    "    for path_excel in sorted(dict.fromkeys(cands)):\n",
    "        try:\n",
    "            ann_df = load_seizure_annotations_file(path_excel)\n",
    "            ev = events_from_annotation_df(ann_df, prefer=\"clinic\")\n",
    "            if ev:\n",
    "                return ev, path_excel\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: kunne ikke parse {path_excel}: {e}\")  # Warning: could not parse ...\n",
    "\n",
    "    # fallback to standard search if nothing found with the letter suffix\n",
    "    return load_events_for_patient_with_excel(patient_id, base_annotation_dir)\n",
    "\n",
    "def time_to_index(local_dt, tdms_start_local_naive, wf_increment, n_samples):\n",
    "    \"\"\"\n",
    "    Convert absolute local time -> sample index.\n",
    "    local_dt and tdms_start_local_naive can be datetime, pandas.Timestamp or str.\n",
    "    Returns an int index in [0, n_samples-1].\n",
    "    \"\"\"\n",
    "    if wf_increment is None or wf_increment <= 0:\n",
    "        raise ValueError(\"wf_increment must be > 0 (sec/sample).\")\n",
    "\n",
    "    a = to_naive_local_dt(local_dt)\n",
    "    b = to_naive_local_dt(tdms_start_local_naive)\n",
    "    if a is None or b is None:\n",
    "        raise ValueError(f\"Cannot interpret times: local_dt={local_dt!r}, file_start={tdms_start_local_naive!r}\")\n",
    "\n",
    "    dt_s = (a - b).total_seconds()\n",
    "    idx = int(round(dt_s / wf_increment))\n",
    "    return int(np.clip(idx, 0, n_samples - 1))\n",
    "\n",
    "\n",
    "def make_time_axis(ax):\n",
    "    locator = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "    for lab in ax.get_xticklabels():\n",
    "        lab.set_rotation(15)\n",
    "        lab.set_horizontalalignment('right')\n",
    "\n",
    "\n",
    "def build_segment_datetimes(start_local_dt, start_index, end_index, wf_inc):\n",
    "    seg_start = start_local_dt + dt.timedelta(seconds=start_index * wf_inc)\n",
    "    seg_end   = start_local_dt + dt.timedelta(seconds=(end_index - 1) * wf_inc)\n",
    "    return seg_start, seg_end\n",
    "\n",
    "\n",
    "def plot_raw(\n",
    "    out_png, y, fs, wf_inc, tdms_start_local, seg_i0, seg_i1,\n",
    "    title_prefix=\"\", x_axis_mode=\"absolute\"\n",
    "):\n",
    "    n = seg_i1 - seg_i0\n",
    "    t_rel = np.arange(n) / fs\n",
    "    seg_start, seg_end = build_segment_datetimes(tdms_start_local, seg_i0, seg_i1, wf_inc)\n",
    "\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    ax = plt.gca()\n",
    "    if x_axis_mode == \"absolute\":\n",
    "        # Build list of naive datetime objects for each sample\n",
    "        datetimes_py = [seg_start + dt.timedelta(seconds=i * wf_inc) for i in range(n)]\n",
    "        ax.plot(datetimes_py, y)\n",
    "        make_time_axis(ax)\n",
    "        ax.set_xlabel(\"Time (local)\")\n",
    "    else:\n",
    "        ax.plot(t_rel, y)\n",
    "        ax.set_xlabel(\"Time (s) relative to window start\")\n",
    "\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "    ax.set_title(f\"{title_prefix}  |  Window: {_format_dt(seg_start)} → {_format_dt(seg_end)}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def sanity_check_and_log(\n",
    "    out_dir: str,\n",
    "    base_name: str,\n",
    "    tdms_start_naive,   # datetime.datetime (naive local)\n",
    "    wf_inc: float,      # sec/sample\n",
    "    st_idx: int,        # event start sample\n",
    "    et_idx: int,        # event end sample\n",
    "    seg_i0: int,        # segment start sample\n",
    "    seg_i1: int,        # segment end sample (exclusive)\n",
    "    tol_seconds: float | None = None  # default: 1.5 * wf_inc\n",
    "):\n",
    "    \"\"\"\n",
    "    Checks that event markers (st_idx, et_idx) land correctly within the segment [seg_i0, seg_i1),\n",
    "    and that they correspond to the expected relative positions. Saves a JSON report.\n",
    "\n",
    "    Returns (ok: bool, report: dict).\n",
    "    \"\"\"\n",
    "    if tol_seconds is None:\n",
    "        tol_seconds = 1.5 * wf_inc  # ~1–2 samples tolerance\n",
    "\n",
    "    # Absolute times\n",
    "    seg_start = tdms_start_naive + dt.timedelta(seconds=seg_i0 * wf_inc)\n",
    "    seg_end   = tdms_start_naive + dt.timedelta(seconds=(seg_i1 - 1) * wf_inc)  # last sample in the window\n",
    "    event_start = tdms_start_naive + dt.timedelta(seconds=st_idx * wf_inc)\n",
    "    event_end   = tdms_start_naive + dt.timedelta(seconds=et_idx * wf_inc)\n",
    "\n",
    "    # Relative positions in seconds\n",
    "    start_rel_s = (st_idx - seg_i0) * wf_inc\n",
    "    end_rel_s   = (et_idx - seg_i0) * wf_inc\n",
    "\n",
    "    # Expected absolute times from segment start + relative offsets\n",
    "    exp_start = seg_start + dt.timedelta(seconds=start_rel_s)\n",
    "    exp_end   = seg_start + dt.timedelta(seconds=end_rel_s)\n",
    "\n",
    "    # Errors in seconds\n",
    "    err_start_s = (event_start - exp_start).total_seconds()\n",
    "    err_end_s   = (event_end   - exp_end).total_seconds()\n",
    "\n",
    "    # Errors in samples\n",
    "    err_start_samples = err_start_s / wf_inc\n",
    "    err_end_samples   = err_end_s   / wf_inc\n",
    "\n",
    "    # Additional checks\n",
    "    within_segment = (st_idx >= seg_i0) and (et_idx <= seg_i1)\n",
    "    start_on_edge  = (st_idx == seg_i0) or (st_idx == seg_i1)\n",
    "    end_on_edge    = (et_idx == seg_i0) or (et_idx == seg_i1)\n",
    "\n",
    "    # OK criterion\n",
    "    ok = (abs(err_start_s) <= tol_seconds) and (abs(err_end_s) <= tol_seconds) and within_segment\n",
    "\n",
    "    report = {\n",
    "        \"segment_start\": seg_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"segment_end\":   seg_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"event_start\":   event_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"event_end\":     event_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"wf_increment_s\": wf_inc,\n",
    "        \"tol_seconds\": tol_seconds,\n",
    "        \"start_rel_s\": start_rel_s,\n",
    "        \"end_rel_s\":   end_rel_s,\n",
    "        \"error_start_seconds\": err_start_s,\n",
    "        \"error_end_seconds\":   err_end_s,\n",
    "        \"error_start_samples\": err_start_samples,\n",
    "        \"error_end_samples\":   err_end_samples,\n",
    "        \"st_idx\": st_idx,\n",
    "        \"et_idx\": et_idx,\n",
    "        \"seg_i0\": seg_i0,\n",
    "        \"seg_i1\": seg_i1,\n",
    "        \"within_segment\": within_segment,\n",
    "        \"start_on_edge\": start_on_edge,\n",
    "        \"end_on_edge\": end_on_edge,\n",
    "        \"ok\": ok,\n",
    "    }\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with open(os.path.join(out_dir, f\"{base_name}_sanity.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return ok, report\n",
    "\n",
    "def _seg_times(tdms_start_naive, i0, i1, wf_inc):\n",
    "    \"\"\"Return (seg_start, seg_end) as naive local datetimes.\"\"\"\n",
    "    seg_start = tdms_start_naive + dt.timedelta(seconds=i0 * wf_inc)\n",
    "    seg_end = tdms_start_naive + dt.timedelta(seconds=(i1 - 1) * wf_inc)\n",
    "    return seg_start, seg_end\n",
    "\n",
    "\n",
    "def _make_time_axis(ax):\n",
    "    \"\"\"Configure x-axis to show time labels (HH:MM:SS).\"\"\"\n",
    "    locator = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "    for lab in ax.get_xticklabels():\n",
    "        lab.set_rotation(15)\n",
    "        lab.set_horizontalalignment('right')\n",
    "\n",
    "\n",
    "# MAX_PLOT_POINTS is defined in the notebook globals; keep using it as default\n",
    "def build_time_index(seg_start_dt, n, wf_inc):\n",
    "    \"\"\"Build a pandas DatetimeIndex starting at seg_start_dt with n samples spaced by wf_inc seconds.\"\"\"\n",
    "    return pd.date_range(start=seg_start_dt, periods=n, freq=pd.to_timedelta(wf_inc, unit=\"s\"))\n",
    "\n",
    "\n",
    "def thin_for_plot(y, max_points=MAX_PLOT_POINTS):\n",
    "    \"\"\"Return (indices, y_thinned) suitable for plotting when y is large.\"\"\"\n",
    "    n = len(y)\n",
    "    if n <= max_points:\n",
    "        return np.arange(n), y\n",
    "    step = int(np.ceil(n / max_points))\n",
    "    idx = np.arange(0, n, step, dtype=int)\n",
    "    return idx, y[idx]\n",
    "\n",
    "\n",
    "def plot_seizure_abs(\n",
    "    out_png, y, fs, wf_inc, tdms_start_naive,\n",
    "    seg_i0, seg_i1, st_idx, et_idx, title_prefix\n",
    "):\n",
    "    y = np.asarray(y)\n",
    "    n = seg_i1 - seg_i0\n",
    "    if n <= 0:\n",
    "        return\n",
    "\n",
    "    seg_start = tdms_start_naive + dt.timedelta(seconds=seg_i0 * wf_inc)\n",
    "    seg_end = tdms_start_naive + dt.timedelta(seconds=(seg_i1 - 1) * wf_inc)\n",
    "    event_start = tdms_start_naive + dt.timedelta(seconds=st_idx * wf_inc)\n",
    "    event_end = tdms_start_naive + dt.timedelta(seconds=et_idx * wf_inc)\n",
    "\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    full_times = pd.date_range(\n",
    "        start=seg_start,\n",
    "        periods=n,\n",
    "        freq=pd.to_timedelta(wf_inc, unit=\"s\"),\n",
    "    )\n",
    "\n",
    "    idx, y_plot = thin_for_plot(y, max_points=MAX_PLOT_POINTS)\n",
    "    times_plot = full_times[idx]\n",
    "\n",
    "    ax.plot(times_plot, y_plot, label=\"ECG\")\n",
    "    ax.axvline(event_start, linestyle=\"--\", label=\"seizure start\")\n",
    "    ax.axvline(event_end, linestyle=\"--\", label=\"seizure end\")\n",
    "\n",
    "    _make_time_axis(ax)\n",
    "    ax.set_xlabel(\"Time (HH:MM:SS)\")\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "    ax.set_title(\n",
    "        f\"{title_prefix}  |  Window: {seg_start:%Y-%m-%d %H:%M:%S} → {seg_end:%Y-%m-%d %H:%M:%S}  \"\n",
    "        f\"(Event: {event_start:%Y-%m-%d %H:%M:%S} → {event_end:%Y-%m-%d %H:%M:%S})\"\n",
    "    )\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_raw_abs(\n",
    "    out_png, y, fs, wf_inc, tdms_start_naive,\n",
    "    seg_i0, seg_i1, title_prefix\n",
    "):\n",
    "    n = seg_i1 - seg_i0\n",
    "    if n <= 0:\n",
    "        return\n",
    "\n",
    "    seg_start, seg_end = _seg_times(tdms_start_naive, seg_i0, seg_i1, wf_inc)\n",
    "\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    full_times = pd.date_range(start=seg_start, periods=n, freq=pd.to_timedelta(wf_inc, unit=\"s\"))\n",
    "    idx, y_plot = thin_for_plot(np.asarray(y), max_points=MAX_PLOT_POINTS)\n",
    "    times_plot = full_times[idx]\n",
    "\n",
    "    ax.plot(times_plot, y_plot)\n",
    "    _make_time_axis(ax)\n",
    "    ax.set_xlabel(\"Time (HH:MM:SS)\")\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "    ax.set_title(\n",
    "        f\"{title_prefix}  |  Window: {seg_start:%Y-%m-%d %H:%M:%S} → {seg_end:%Y-%m-%d %H:%M:%S}\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def discover_all_tdms(base_dir: str):\n",
    "    \"\"\"Yield (relative_path, absolute_path) for all .tdms files in Patients ePatch data (recursive).\"\"\"\n",
    "    entries = sorted(os.listdir(base_dir), key=str.lower)\n",
    "    for entry in entries:\n",
    "        pdir = os.path.join(base_dir, entry)\n",
    "        if not os.path.isdir(pdir):\n",
    "            continue\n",
    "        for root, _, files in os.walk(pdir):\n",
    "            for f in files:\n",
    "                fl = f.lower()\n",
    "                if not fl.endswith(\".tdms\"):\n",
    "                    continue\n",
    "                if fl.endswith(\".tdms_index\"):  # safety: skip index files\n",
    "                    continue\n",
    "                abs_p = os.path.join(root, f)\n",
    "                rel_p = os.path.relpath(abs_p, base_dir)\n",
    "                yield rel_p, abs_p\n",
    "\n",
    "def build_time_index(seg_start_dt, n, wf_inc):\n",
    "    # Effective time axis: DatetimeIndex in 64-bit\n",
    "    return pd.date_range(start=seg_start_dt, periods=n, freq=pd.to_timedelta(wf_inc, unit=\"s\"))\n",
    "\n",
    "def thin_for_plot(y, max_points=200_000):\n",
    "    \"\"\"Return (idx, y_thin) where idx is a slice/array to pick y down to <= max_points.\"\"\"\n",
    "    n = len(y)\n",
    "    if n <= max_points:\n",
    "        return slice(None), y\n",
    "    step = int(np.ceil(n / max_points))\n",
    "    idx = np.arange(0, n, step, dtype=int)\n",
    "    return idx, y[idx]\n",
    "\n",
    "def print_mem(tag=\"\"):\n",
    "    \"\"\"Print current RAM usage (GB) for the Python process.\"\"\"\n",
    "    mem_gb = psutil.Process().memory_info().rss / (1024 ** 3)\n",
    "    print(f\"[DEBUG] {tag} RAM-forbrug: {mem_gb:.2f} GB\")\n",
    "\n",
    "def split_rel_tdms_path(rel_tdms: str):\n",
    "    \"\"\"\n",
    "    Examples:\n",
    "      \"Patient 8/recording 1/Patient 8_1.tdms\"\n",
    "         -> patient=\"Patient 8\", enrollment=None, recording=\"recording 1\"\n",
    "\n",
    "      \"Patient 8/enrollment A/recording 2/Patient 8_2.tdms\"\n",
    "         -> patient=\"Patient 8\", enrollment=\"enrollment A\", recording=\"recording 2\"\n",
    "    \"\"\"\n",
    "    rel_dir = os.path.dirname(rel_tdms)         # drop the .tdms filename itself\n",
    "    parts = rel_dir.split(os.sep)\n",
    "\n",
    "    patient = None\n",
    "    enrollment = None\n",
    "    recording = None\n",
    "\n",
    "    if len(parts) == 1:\n",
    "        # \"Patient 8\"\n",
    "        patient = parts[0]\n",
    "    elif len(parts) == 2:\n",
    "        # \"Patient 8/recording 1\" OR \"Patient 8/enrollment A\"\n",
    "        patient = parts[0]\n",
    "        # guess based on name\n",
    "        if parts[1].lower().startswith(\"enrollment\"):\n",
    "            enrollment = parts[1]\n",
    "        else:\n",
    "            recording = parts[1]\n",
    "    elif len(parts) >= 3:\n",
    "        # \"Patient 8/enrollment A/recording 1\"\n",
    "        patient = parts[0]\n",
    "        # the next two can be enrollment + recording\n",
    "        if parts[1].lower().startswith(\"enrollment\"):\n",
    "            enrollment = parts[1]\n",
    "            recording = parts[2]\n",
    "        else:\n",
    "            # fallback: patient / <something> / recording\n",
    "            recording = parts[2]\n",
    "            enrollment = parts[1]\n",
    "    return patient, enrollment, recording\n",
    "\n",
    "def process_tdms_recording(rel_tdms, abs_tdms, patient_id, enrollment_name, events_all):\n",
    "    \"\"\"\n",
    "    Process a single TDMS recording for a patient/enrollment.\n",
    "    \"\"\"\n",
    "    patient_folder, enrollment_folder, recording_folder = split_rel_tdms_path(rel_tdms)\n",
    "\n",
    "    print(f\"\\n== TDMS == {rel_tdms}  (patient={patient_id})\")\n",
    "    try:\n",
    "        x, fs, start_local, meta = tdms_load_ecg(abs_tdms)\n",
    "    except Exception as e:\n",
    "        print(f\"[FEJL] TDMS indlæsning: {e}\")\n",
    "        return\n",
    "\n",
    "    if start_local is None:\n",
    "        print(\"[FEJL] TDMS starttid kunne ikke fortolkes – springer denne recording.\")\n",
    "        return\n",
    "\n",
    "    wf_inc = 1.0 / fs\n",
    "    events_in_span = filter_events_for_tdms_span(events_all, start_local, len(x), wf_inc, pad_hours=12, debug_label=rel_tdms)\n",
    "    if not events_in_span:\n",
    "        print(f\"[INFO] {len(events_all)} annotering(er) fundet, men ingen overlapper denne recording.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[OK] {len(events_in_span)}/{len(events_all)} annotering(er) overlapper {recording_folder or 'optagelsen'}.\")\n",
    "\n",
    "    # build output dirs\n",
    "    patient_out_dir = os.path.join(OUTPUT_ROOT, patient_folder)\n",
    "    ensure_dir(patient_out_dir)\n",
    "\n",
    "    if enrollment_name:\n",
    "        enrollment_out_dir = os.path.join(patient_out_dir, enrollment_name)\n",
    "        ensure_dir(enrollment_out_dir)\n",
    "        base_out_dir = enrollment_out_dir\n",
    "    else:\n",
    "        base_out_dir = patient_out_dir\n",
    "\n",
    "    if recording_folder:\n",
    "        out_dir = os.path.join(base_out_dir, recording_folder)\n",
    "    else:\n",
    "        out_dir = base_out_dir\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # 🔹 loop events (you can reuse your existing event-loop here unchanged)\n",
    "    for k, (st_local, et_local) in enumerate(events_in_span, start=1):\n",
    "        base = f\"event{k:02d}\"\n",
    "\n",
    "        # ---- indices for the event itself ----\n",
    "        st_idx = time_to_index(st_local, start_local, wf_inc, len(x))\n",
    "        et_idx = time_to_index(et_local, start_local, wf_inc, len(x))\n",
    "        if et_idx < st_idx:\n",
    "            st_idx, et_idx = et_idx, st_idx\n",
    "\n",
    "        # =======================================================\n",
    "        # A) CONTEXT window (± PAD_MIN) – your current window\n",
    "        # =======================================================\n",
    "        sz_i0, sz_i1 = slice_window(len(x), fs, st_idx, et_idx, PAD_MIN)\n",
    "\n",
    "        if sz_i1 <= sz_i0:\n",
    "            print(f\"[INFO] Tomt seizure-vindue for {base} – springer.\")\n",
    "            continue\n",
    "\n",
    "        y_ctx = x[sz_i0:sz_i1]\n",
    "\n",
    "        ok, rep = sanity_check_and_log(\n",
    "            out_dir,\n",
    "            base,\n",
    "            start_local,\n",
    "            wf_inc,\n",
    "            st_idx,\n",
    "            et_idx,\n",
    "            sz_i0,\n",
    "            sz_i1,\n",
    "        )\n",
    "        print(\n",
    "            f\"[SANITY] {base}: ok={ok} within={rep['within_segment']} \"\n",
    "            f\"err_start={rep['error_start_samples']:.2f} samp, \"\n",
    "            f\"err_end={rep['error_end_samples']:.2f} samp\"\n",
    "        )\n",
    "\n",
    "        # plot + csv context\n",
    "        plot_seizure_abs(\n",
    "            os.path.join(out_dir, f\"{base}_seizure_ctx.png\"),\n",
    "            y=y_ctx,\n",
    "            fs=fs,\n",
    "            wf_inc=wf_inc,\n",
    "            tdms_start_naive=start_local,\n",
    "            seg_i0=sz_i0,\n",
    "            seg_i1=sz_i1,\n",
    "            st_idx=st_idx,\n",
    "            et_idx=et_idx,\n",
    "            title_prefix=f\"Seizure kontekst (±{PAD_MIN} min)\",\n",
    "        )\n",
    "        t_rel_ctx = (np.arange(sz_i0, sz_i1) - sz_i0) / fs\n",
    "        save_csv(os.path.join(out_dir, f\"{base}_seizure_ctx.csv\"), t_rel_ctx, y_ctx)\n",
    "\n",
    "        # =======================================================\n",
    "        # B) SEIZURE-ONLY (the seizure itself)\n",
    "        # =======================================================\n",
    "        so_i0 = max(0, st_idx)\n",
    "        so_i1 = min(len(x), et_idx)\n",
    "        if so_i1 > so_i0:\n",
    "            y_so = x[so_i0:so_i1]\n",
    "            plot_seizure_abs(\n",
    "                os.path.join(out_dir, f\"{base}_seizure_only.png\"),\n",
    "                y=y_so,\n",
    "                fs=fs,\n",
    "                wf_inc=wf_inc,\n",
    "                tdms_start_naive=start_local,\n",
    "                seg_i0=so_i0,\n",
    "                seg_i1=so_i1,\n",
    "                st_idx=st_idx,\n",
    "                et_idx=et_idx,\n",
    "                title_prefix=\"Seizure (kun anfald)\",\n",
    "            )\n",
    "            t_rel_so = (np.arange(so_i0, so_i1) - so_i0) / fs\n",
    "            save_csv(os.path.join(out_dir, f\"{base}_seizure_only.csv\"), t_rel_so, y_so)\n",
    "\n",
    "        # =======================================================\n",
    "        # C) NON-SEIZURE (same length, 20 min before)\n",
    "        # =======================================================\n",
    "        ns_anchor_time = st_local - dt.timedelta(minutes=(NONSEIZURE_OFFSET_MIN + PAD_MIN))\n",
    "        ns_i0 = time_to_index(ns_anchor_time, start_local, wf_inc, len(x))\n",
    "        ns_i1 = ns_i0 + (sz_i1 - sz_i0)\n",
    "        ns_i0 = max(0, ns_i0)\n",
    "        ns_i1 = min(len(x), ns_i1)\n",
    "\n",
    "        if ns_i1 > ns_i0:\n",
    "            y_ns = x[ns_i0:ns_i1]\n",
    "            plot_raw_abs(\n",
    "                os.path.join(out_dir, f\"{base}_nonseizure.png\"),\n",
    "                y=y_ns,\n",
    "                fs=fs,\n",
    "                wf_inc=wf_inc,\n",
    "                tdms_start_naive=start_local,\n",
    "                seg_i0=ns_i0,\n",
    "                seg_i1=ns_i1,\n",
    "                title_prefix=f\"Non-seizure (20 min før; længde {(ns_i1-ns_i0)/fs:.1f}s)\",\n",
    "            )\n",
    "            t_rel_ns = (np.arange(ns_i0, ns_i1) - ns_i0) / fs\n",
    "            save_csv(os.path.join(out_dir, f\"{base}_nonseizure.csv\"), t_rel_ns, y_ns)\n",
    "        else:\n",
    "            print(f\"[INFO] Non-seizure udenfor filgrænser for {base} – springer.\")\n",
    "\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        try:\n",
    "            del y_sz, y_so, y_ns\n",
    "        except NameError:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        print_mem(f\"Efter event {k}\")\n",
    "    del x\n",
    "    gc.collect()\n",
    "\n",
    "def debug_list_annotation_candidates(patient_id, base_annotation_dir):\n",
    "    \"\"\"Print which files in the annotation folder could belong to the patient.\"\"\"\n",
    "    pid_str = str(patient_id)\n",
    "    patterns = [\n",
    "        f\"**/patient {pid_str}.*\",\n",
    "        f\"**/patient_{pid_str}.*\",\n",
    "        f\"**/patient{pid_str}.*\",\n",
    "        f\"**/pt {pid_str}.*\",\n",
    "        f\"**/*{pid_str}*.xls\",\n",
    "        f\"**/*{pid_str}*.xlsx\",\n",
    "    ]\n",
    "    found = []\n",
    "    for pat in patterns:\n",
    "        found += glob.glob(os.path.join(base_annotation_dir, pat), recursive=True)\n",
    "\n",
    "    if not found:\n",
    "        print(f\"[DEBUG] No potential annotation files found for patient {patient_id}\")\n",
    "    else:\n",
    "        print(f\"[DEBUG] Potential annotation files for patient {patient_id}:\")\n",
    "        for fpath in sorted(set(found)):\n",
    "            print(\"   \", fpath)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Run for one patient\n",
    "# --------------------------\n",
    "def filter_events_for_tdms_span(events, tdms_start_naive, n_samples, wf_inc, pad_hours=12, debug_label=None):\n",
    "    tdms_end = tdms_start_naive + dt.timedelta(seconds=n_samples * wf_inc)\n",
    "    lo = tdms_start_naive - dt.timedelta(hours=pad_hours)\n",
    "    hi = tdms_end + dt.timedelta(hours=pad_hours)\n",
    "\n",
    "    kept = []\n",
    "    for (st, et) in events:\n",
    "        # st and et are already naive local (we created to_naive_local)\n",
    "        if st <= hi and et >= lo:\n",
    "            kept.append((st, et))\n",
    "\n",
    "    if debug_label:\n",
    "        print(f\"[DEBUG] TDMS span {debug_label}: {tdms_start_naive} → {tdms_end} (lo={lo}, hi={hi})\")\n",
    "        print(f\"[DEBUG] Events (raw):\")\n",
    "        for i, (st, et) in enumerate(events, start=1):\n",
    "            print(f\"    evt{i}: {st} → {et}\")\n",
    "\n",
    "        print(f\"[DEBUG] -> {len(kept)}/{len(events)} events overlap the TDMS\")\n",
    "\n",
    "    return kept\n",
    "    # return [(st, et) for st, et in events if (st <= hi and et >= lo)]\n",
    "\n",
    "\n",
    "def run_for_patient(patient_selector=None, patient_id=None):\n",
    "    \"\"\"\n",
    "    Process ALL TDMS files for one patient.\n",
    "    - read annotations once from BASE_ANNOTATION_DIR (patient-level)\n",
    "    - apply them to all the patient's recordings\n",
    "    - always save under OUTPUT_ROOT/<Patient X>/[recording y]\n",
    "    \"\"\"\n",
    "    # 1) find all TDMS for the selected patient\n",
    "    tdms_list = list(\n",
    "        discover_tdms_for_patient(\n",
    "            BASE_PATIENTS_DIR,\n",
    "            patient_selector=patient_selector,\n",
    "            patient_id=patient_id,\n",
    "        )\n",
    "    )\n",
    "    if not tdms_list:\n",
    "        print(\"[INFO] No TDMS files found for the selected patient.\")\n",
    "        return\n",
    "\n",
    "    # 2) try to infer patient-id from the first path\n",
    "    p_id, rec_id = extract_ids_from_relpath(tdms_list[0][0])\n",
    "    if p_id is None and patient_id is not None:\n",
    "        p_id = patient_id\n",
    "\n",
    "    # find all enrollments for this patient\n",
    "    enrollments = sorted({\n",
    "        split_rel_tdms_path(rel_tdms)[1]  # enrollment_name\n",
    "        for rel_tdms, _ in tdms_list\n",
    "        if split_rel_tdms_path(rel_tdms)[1] is not None\n",
    "    })\n",
    "\n",
    "    # If no enrollments, create a list with a single None so we still run once\n",
    "    if not enrollments:\n",
    "        enrollments = [None]\n",
    "\n",
    "    # Run ALL enrollments for this patient\n",
    "    for enrollment_name in enrollments:\n",
    "        print(f\"\\n[INFO] Processing {patient_selector or f'Patient {p_id}'} | Enrollment: {enrollment_name or '(none)'}\")\n",
    "\n",
    "        # find TDMS that belong to this enrollment\n",
    "        tdms_for_enrollment = [\n",
    "            (rel, abs_)\n",
    "            for rel, abs_ in tdms_list\n",
    "            if split_rel_tdms_path(rel)[1] == enrollment_name\n",
    "        ]\n",
    "\n",
    "        # Load annotations for this enrollment (a, b, c etc.)\n",
    "        events_all, ann_src = load_events_for_patient_and_enrollment(\n",
    "            p_id, enrollment_name, BASE_ANNOTATION_DIR\n",
    "        )\n",
    "\n",
    "        debug_list_annotation_candidates(p_id, BASE_ANNOTATION_DIR)\n",
    "\n",
    "        if not events_all:\n",
    "            print(f\"[INFO] No annotations found for {patient_selector} / {enrollment_name} — skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[OK] Found {len(events_all)} annotation(s) in: {ann_src}\")\n",
    "\n",
    "        print_mem(\"Before patient loop\")\n",
    "        # Run the TDMS files matching this enrollment\n",
    "        for rel_tdms, abs_tdms in tdms_for_enrollment:\n",
    "            process_tdms_recording(\n",
    "                rel_tdms=rel_tdms,\n",
    "                abs_tdms=abs_tdms,\n",
    "                patient_id=p_id,\n",
    "                enrollment_name=enrollment_name,\n",
    "                events_all=events_all\n",
    "            )\n",
    "\n",
    "    print_mem(f\"After cleanup recording {rec_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2681ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing Patient 38 | Enrollment: enrollment a\n",
      "Warning: kunne ikke parse E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\patient 38a.xls: Can only use .dt accessor with datetimelike values\n",
      "[DEBUG] Potential annotation files for patient 38:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 38a.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 38b.xls\n",
      "[INFO] No annotations found for Patient 38 / enrollment a — skipping.\n",
      "\n",
      "[INFO] Processing Patient 38 | Enrollment: enrollment b\n",
      "[DEBUG] Potential annotation files for patient 38:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 38a.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 38b.xls\n",
      "[OK] Found 2 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\patient 38b.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 0.08 GB\n",
      "\n",
      "== TDMS == Patient 38\\enrollment b\\recording 1\\Patient 38b_1.tdms  (patient=38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 38\\enrollment b\\recording 1\\Patient 38b_1.tdms: 2018-06-18 14:30:46 → 2018-06-19 11:18:36.500000 (lo=2018-06-18 02:30:46, hi=2018-06-19 23:18:36.500000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2018-06-18 17:11:26 → 2018-06-18 17:11:49\n",
      "    evt2: 2018-06-19 11:12:10 → 2018-06-19 11:12:37\n",
      "[DEBUG] -> 2/2 events overlap the TDMS\n",
      "[OK] 2/2 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 0.48 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 0.53 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 0.24 GB\n"
     ]
    }
   ],
   "source": [
    "# For individual run and testing\n",
    "if __name__ == \"__main__\":\n",
    "    run_for_patient(patient_selector=\"Patient 38\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01f57cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responders: ['Patient 3', 'Patient 5', 'Patient 6', 'Patient 8', 'Patient 10', 'Patient 14', 'Patient 15', 'Patient 16', 'Patient 21', 'Patient 23', 'Patient 27', 'Patient 28', 'Patient 29', 'Patient 31', 'Patient 34', 'Patient 37', 'Patient 39', 'Patient 40', 'Patient 41', 'Patient 42']\n",
      "Non-Responders: ['Patient 1', 'Patient 2', 'Patient 4', 'Patient 7', 'Patient 9', 'Patient 11', 'Patient 12', 'Patient 13', 'Patient 17', 'Patient 18', 'Patient 19', 'Patient 20', 'Patient 22', 'Patient 24', 'Patient 25', 'Patient 26', 'Patient 30', 'Patient 32', 'Patient 33', 'Patient 35', 'Patient 36', 'Patient 38', 'Patient 43']\n"
     ]
    }
   ],
   "source": [
    "# Separating responders and non-responders as Jeppesen et al. describes.\n",
    "patients_responders_numbers = [3,5,6,8,10,14,15,16,21,23,27,28,29,31,34,37,39,40,41,42]\n",
    "patients_non_responders_numbers = [1,2,4,7,9,11,12,13,17,18,19,20,22,24,25,26,30,32,33,35,36,38,43]\n",
    "patients_responders = [f\"Patient {num}\" for num in patients_responders_numbers]\n",
    "patients_non_responders = [f\"Patient {num}\" for num in patients_non_responders_numbers]\n",
    "print(\"Responders:\", patients_responders)\n",
    "print(\"Non-Responders:\", patients_non_responders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7fc4ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing Patient 1 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 1:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 1.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 10.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 11.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 12.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 13.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 14.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 15.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 16.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 17.xlsx\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 18.xlsx\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 19.xlsx\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 21.xlsx\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 31a.xlsx\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 31b.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 41.xls\n",
      "[OK] Found 3 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 1.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 1.52 GB\n",
      "\n",
      "== TDMS == Patient 1\\recording 1\\Patient 1_1.tdms  (patient=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 1\\recording 1\\Patient 1_1.tdms: 2016-02-22 11:04:14 → 2016-02-24 16:09:49.750000 (lo=2016-02-21 23:04:14, hi=2016-02-25 04:09:49.750000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2016-02-23 23:14:01 → 2016-02-23 23:14:42\n",
      "    evt2: 2016-02-25 05:20:48 → 2016-02-25 05:22:20\n",
      "    evt3: 2016-02-26 05:30:53 → 2016-02-26 05:31:34\n",
      "[DEBUG] -> 1/3 events overlap the TDMS\n",
      "[OK] 1/3 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nptdms.reader WARNING] Unrecognised version number: 538972776\n",
      "[nptdms.reader WARNING] Last segment metadata is incomplete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Efter event 1 RAM-forbrug: 0.97 GB\n",
      "\n",
      "== TDMS == Patient 1\\recording 2\\Patient 1_2.tdms  (patient=1)\n",
      "[FEJL] TDMS indlæsning: No channels found in the TDMS file.\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 0.24 GB\n",
      "\n",
      "[INFO] Processing Patient 2 | Enrollment: (none)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Potential annotation files for patient 2:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 12.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 2.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 20.xlsx\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 21.xlsx\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 22.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 23a.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 23b.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 24.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 25.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 26.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 27a.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 27b.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 28.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 29.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 32.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 42.xls\n",
      "[OK] Found 9 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 2.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 0.24 GB\n",
      "\n",
      "== TDMS == Patient 2\\recording 1\\Patient 2_1.tdms  (patient=2)\n",
      "[DEBUG] TDMS span Patient 2\\recording 1\\Patient 2_1.tdms: 2016-05-03 13:19:05 → 2016-05-04 07:03:49.750000 (lo=2016-05-03 01:19:05, hi=2016-05-04 19:03:49.750000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2016-05-03 14:12:27 → 2016-05-03 14:14:12\n",
      "    evt2: 2016-05-03 14:56:10 → 2016-05-03 14:57:06\n",
      "    evt3: 2016-05-03 16:14:49 → 2016-05-03 16:15:58\n",
      "    evt4: 2016-05-03 22:48:44 → 2016-05-03 22:49:13\n",
      "    evt5: 2016-05-04 01:23:37 → 2016-05-04 01:24:22\n",
      "    evt6: 2016-05-04 03:14:40 → 2016-05-04 03:15:27\n",
      "    evt7: 2016-05-04 03:58:53 → 2016-05-04 03:59:33\n",
      "    evt8: 2016-05-04 05:26:31 → 2016-05-04 05:26:53\n",
      "    evt9: 2016-05-04 06:21:30 → 2016-05-04 06:21:57\n",
      "[DEBUG] -> 9/9 events overlap the TDMS\n",
      "[OK] 9/9 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 0.54 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 0.58 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 0.62 GB\n",
      "[SANITY] event04: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 4 RAM-forbrug: 0.66 GB\n",
      "[SANITY] event05: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 5 RAM-forbrug: 0.70 GB\n",
      "[SANITY] event06: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 6 RAM-forbrug: 0.74 GB\n",
      "[SANITY] event07: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 7 RAM-forbrug: 0.78 GB\n",
      "[SANITY] event08: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 8 RAM-forbrug: 0.81 GB\n",
      "[SANITY] event09: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 9 RAM-forbrug: 0.84 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 0.60 GB\n",
      "\n",
      "[INFO] Processing Patient 4 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 4:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 14.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 24.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 34a.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 34b.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 34c.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 4.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 40.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 41.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 42.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 43.xls\n",
      "[OK] Found 1 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 4.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 0.60 GB\n",
      "\n",
      "== TDMS == Patient 4\\recording 1\\Patient 4_1.tdms  (patient=4)\n",
      "[DEBUG] TDMS span Patient 4\\recording 1\\Patient 4_1.tdms: 2016-09-12 12:50:00 → 2016-09-13 04:38:45.001953 (lo=2016-09-12 00:50:00, hi=2016-09-13 16:38:45.001953)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2016-09-12 15:45:57 → 2016-09-12 15:47:34\n",
      "[DEBUG] -> 1/1 events overlap the TDMS\n",
      "[OK] 1/1 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 0.86 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 0.64 GB\n",
      "\n",
      "[INFO] Processing Patient 7 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 7:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 17.xlsx\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 27a.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 27b.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 37.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 7.xls\n",
      "[OK] Found 4 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 7.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 0.64 GB\n",
      "\n",
      "== TDMS == Patient 7\\recording 1\\Patient 7_1.tdms  (patient=7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 7\\recording 1\\Patient 7_1.tdms: 2017-02-06 12:26:21 → 2017-02-08 14:00:58.500000 (lo=2017-02-06 00:26:21, hi=2017-02-09 02:00:58.500000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-02-07 09:42:22 → 2017-02-07 09:43:11\n",
      "    evt2: 2017-02-07 11:20:41 → 2017-02-07 11:21:34\n",
      "    evt3: 2017-02-07 12:50:27 → 2017-02-07 12:51:15\n",
      "    evt4: 2017-02-09 15:02:12 → 2017-02-09 15:03:11\n",
      "[DEBUG] -> 3/4 events overlap the TDMS\n",
      "[OK] 3/4 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 1.36 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 1.41 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 1.45 GB\n",
      "\n",
      "== TDMS == Patient 7\\recording 2\\Patient 7_2.tdms  (patient=7)\n",
      "[DEBUG] TDMS span Patient 7\\recording 2\\Patient 7_2.tdms: 2017-02-08 14:02:18 → 2017-02-09 16:36:10 (lo=2017-02-08 02:02:18, hi=2017-02-10 04:36:10)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-02-07 09:42:22 → 2017-02-07 09:43:11\n",
      "    evt2: 2017-02-07 11:20:41 → 2017-02-07 11:21:34\n",
      "    evt3: 2017-02-07 12:50:27 → 2017-02-07 12:51:15\n",
      "    evt4: 2017-02-09 15:02:12 → 2017-02-09 15:03:11\n",
      "[DEBUG] -> 1/4 events overlap the TDMS\n",
      "[OK] 1/4 annotering(er) overlapper recording 2.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 1.18 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 0.81 GB\n",
      "\n",
      "[INFO] Processing Patient 9 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 9:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 19.xlsx\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 29.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 39.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 9.xls\n",
      "[OK] Found 3 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 9.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 0.81 GB\n",
      "\n",
      "== TDMS == Patient 9\\recording 1\\Patient 9_1.tdms  (patient=9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 9\\recording 1\\Patient 9_1.tdms: 2017-06-13 11:03:27 → 2017-06-15 14:17:29 (lo=2017-06-12 23:03:27, hi=2017-06-16 02:17:29)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-06-13 17:42:04 → 2017-06-13 17:46:01\n",
      "    evt2: 2017-06-13 18:01:33 → 2017-06-13 18:04:53\n",
      "    evt3: 2017-06-13 22:33:49 → 2017-06-13 22:38:41\n",
      "[DEBUG] -> 3/3 events overlap the TDMS\n",
      "[OK] 3/3 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 1.57 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 1.61 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 1.66 GB\n",
      "\n",
      "== TDMS == Patient 9\\recording 2\\Patient 9_2.tdms  (patient=9)\n",
      "[DEBUG] TDMS span Patient 9\\recording 2\\Patient 9_2.tdms: 2017-06-15 14:19:43 → 2017-06-16 07:20:24.750000 (lo=2017-06-15 02:19:43, hi=2017-06-16 19:20:24.750000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-06-13 17:42:04 → 2017-06-13 17:46:01\n",
      "    evt2: 2017-06-13 18:01:33 → 2017-06-13 18:04:53\n",
      "    evt3: 2017-06-13 22:33:49 → 2017-06-13 22:38:41\n",
      "[DEBUG] -> 0/3 events overlap the TDMS\n",
      "[INFO] 3 annotering(er) fundet, men ingen overlapper denne recording.\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 0.95 GB\n",
      "\n",
      "[INFO] Processing Patient 11 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 11:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 11.xls\n",
      "[OK] Found 6 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 11.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 0.95 GB\n",
      "\n",
      "== TDMS == Patient 11\\recording 1\\Patient 11_1.tdms  (patient=11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 11\\recording 1\\Patient 11_1.tdms: 2017-08-14 14:11:21 → 2017-08-17 13:56:05.500000 (lo=2017-08-14 02:11:21, hi=2017-08-18 01:56:05.500000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-08-17 11:46:25 → 2017-08-17 11:47:29\n",
      "    evt2: 2017-08-17 13:42:18 → 2017-08-17 13:42:39\n",
      "    evt3: 2017-08-17 17:58:24 → 2017-08-17 17:59:03\n",
      "    evt4: 2017-08-17 18:33:42 → 2017-08-17 18:34:17\n",
      "    evt5: 2017-08-17 19:19:31 → 2017-08-17 19:20:06\n",
      "    evt6: 2017-08-17 20:48:17 → 2017-08-17 20:48:40\n",
      "[DEBUG] -> 6/6 events overlap the TDMS\n",
      "[OK] 6/6 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 1.97 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 2.01 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 2.02 GB\n",
      "[SANITY] event04: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 4 RAM-forbrug: 2.03 GB\n",
      "[SANITY] event05: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 5 RAM-forbrug: 2.04 GB\n",
      "[SANITY] event06: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 6 RAM-forbrug: 2.05 GB\n",
      "\n",
      "== TDMS == Patient 11\\recording 2\\Patient 11_2.tdms  (patient=11)\n",
      "[DEBUG] TDMS span Patient 11\\recording 2\\Patient 11_2.tdms: 2017-08-17 13:57:14 → 2017-08-18 08:42:17.250000 (lo=2017-08-17 01:57:14, hi=2017-08-18 20:42:17.250000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-08-17 11:46:25 → 2017-08-17 11:47:29\n",
      "    evt2: 2017-08-17 13:42:18 → 2017-08-17 13:42:39\n",
      "    evt3: 2017-08-17 17:58:24 → 2017-08-17 17:59:03\n",
      "    evt4: 2017-08-17 18:33:42 → 2017-08-17 18:34:17\n",
      "    evt5: 2017-08-17 19:19:31 → 2017-08-17 19:20:06\n",
      "    evt6: 2017-08-17 20:48:17 → 2017-08-17 20:48:40\n",
      "[DEBUG] -> 6/6 events overlap the TDMS\n",
      "[OK] 6/6 annotering(er) overlapper recording 2.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 1.34 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 1.36 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 1.39 GB\n",
      "[SANITY] event04: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 4 RAM-forbrug: 1.43 GB\n",
      "[SANITY] event05: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 5 RAM-forbrug: 1.46 GB\n",
      "[SANITY] event06: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 6 RAM-forbrug: 1.49 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 1.24 GB\n",
      "\n",
      "[INFO] Processing Patient 12 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 12:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 12.xls\n",
      "[OK] Found 2 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 12.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 1.24 GB\n",
      "\n",
      "== TDMS == Patient 12\\recording 1\\Patient 12_1.tdms  (patient=12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 12\\recording 1\\Patient 12_1.tdms: 2017-10-02 13:36:06 → 2017-10-04 13:34:50.500000 (lo=2017-10-02 01:36:06, hi=2017-10-05 01:34:50.500000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2007-10-04 11:47:56 → 2007-10-04 11:50:16\n",
      "    evt2: 2007-10-04 14:57:59 → 2007-10-04 15:00:08\n",
      "[DEBUG] -> 0/2 events overlap the TDMS\n",
      "[INFO] 2 annotering(er) fundet, men ingen overlapper denne recording.\n",
      "\n",
      "== TDMS == Patient 12\\recording 2\\Patient 12_2.tdms  (patient=12)\n",
      "[DEBUG] TDMS span Patient 12\\recording 2\\Patient 12_2.tdms: 2017-10-04 13:35:50 → 2017-10-06 07:59:30.001953 (lo=2017-10-04 01:35:50, hi=2017-10-06 19:59:30.001953)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2007-10-04 11:47:56 → 2007-10-04 11:50:16\n",
      "    evt2: 2007-10-04 14:57:59 → 2007-10-04 15:00:08\n",
      "[DEBUG] -> 0/2 events overlap the TDMS\n",
      "[INFO] 2 annotering(er) fundet, men ingen overlapper denne recording.\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 1.24 GB\n",
      "\n",
      "[INFO] Processing Patient 13 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 13:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 13.xls\n",
      "[OK] Found 8 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 13.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 1.24 GB\n",
      "\n",
      "== TDMS == Patient 13\\recording 1\\Patient 13_1.tdms  (patient=13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 13\\recording 1\\Patient 13_1.tdms: 2017-10-30 14:01:40 → 2017-11-01 15:42:18.250000 (lo=2017-10-30 02:01:40, hi=2017-11-02 03:42:18.250000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-10-30 23:36:34 → 2017-10-30 23:37:51\n",
      "    evt2: 2017-11-02 01:28:37 → 2017-11-02 01:29:04\n",
      "    evt3: 2017-11-02 03:20:37 → 2017-11-02 03:22:00\n",
      "    evt4: 2017-11-02 04:45:29 → 2017-11-02 04:46:28\n",
      "    evt5: 2017-11-02 07:18:49 → 2017-11-02 07:19:24\n",
      "    evt6: 2017-11-02 09:02:11 → 2017-11-02 09:03:04\n",
      "    evt7: 2017-11-02 10:19:12 → 2017-11-02 10:19:51\n",
      "    evt8: 2017-11-02 13:12:29 → 2017-11-02 13:12:50\n",
      "[DEBUG] -> 3/8 events overlap the TDMS\n",
      "[OK] 3/8 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 1.98 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 1.98 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 1.99 GB\n",
      "\n",
      "== TDMS == Patient 13\\recording 2\\Patient 13_2.tdms  (patient=13)\n",
      "[DEBUG] TDMS span Patient 13\\recording 2\\Patient 13_2.tdms: 2017-11-01 15:41:37 → 2017-11-02 15:11:03.750000 (lo=2017-11-01 03:41:37, hi=2017-11-03 03:11:03.750000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-10-30 23:36:34 → 2017-10-30 23:37:51\n",
      "    evt2: 2017-11-02 01:28:37 → 2017-11-02 01:29:04\n",
      "    evt3: 2017-11-02 03:20:37 → 2017-11-02 03:22:00\n",
      "    evt4: 2017-11-02 04:45:29 → 2017-11-02 04:46:28\n",
      "    evt5: 2017-11-02 07:18:49 → 2017-11-02 07:19:24\n",
      "    evt6: 2017-11-02 09:02:11 → 2017-11-02 09:03:04\n",
      "    evt7: 2017-11-02 10:19:12 → 2017-11-02 10:19:51\n",
      "    evt8: 2017-11-02 13:12:29 → 2017-11-02 13:12:50\n",
      "[DEBUG] -> 7/8 events overlap the TDMS\n",
      "[OK] 7/8 annotering(er) overlapper recording 2.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 1.67 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 1.71 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 1.76 GB\n",
      "[SANITY] event04: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 4 RAM-forbrug: 1.79 GB\n",
      "[SANITY] event05: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 5 RAM-forbrug: 1.82 GB\n",
      "[SANITY] event06: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 6 RAM-forbrug: 1.86 GB\n",
      "[SANITY] event07: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 7 RAM-forbrug: 1.89 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 1.57 GB\n",
      "\n",
      "[INFO] Processing Patient 17 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 17:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 17.xlsx\n",
      "[OK] Found 1 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 17.xlsx\n",
      "[DEBUG] Before patient loop RAM-forbrug: 1.57 GB\n",
      "\n",
      "== TDMS == Patient 17\\recording 1\\Patient 17_1.tdms  (patient=17)\n",
      "[DEBUG] TDMS span Patient 17\\recording 1\\Patient 17_1.tdms: 2016-04-05 14:22:34 → 2016-04-06 09:35:12 (lo=2016-04-05 02:22:34, hi=2016-04-06 21:35:12)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2016-04-06 03:00:06 → 2016-04-06 03:00:57\n",
      "[DEBUG] -> 1/1 events overlap the TDMS\n",
      "[OK] 1/1 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 1.88 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 1.61 GB\n",
      "\n",
      "[INFO] Processing Patient 18 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 18:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 18.xlsx\n",
      "[OK] Found 2 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 18.xlsx\n",
      "[DEBUG] Before patient loop RAM-forbrug: 1.61 GB\n",
      "\n",
      "== TDMS == Patient 18\\recording 1\\Patient 18_1.tdms  (patient=18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 18\\recording 1\\Patient 18_1.tdms: 2016-10-03 11:45:47 → 2016-10-05 09:24:37.750000 (lo=2016-10-02 23:45:47, hi=2016-10-05 21:24:37.750000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2016-10-04 14:44:34 → 2016-10-04 14:46:06\n",
      "    evt2: 2016-10-04 19:35:47 → 2016-10-04 19:45:18\n",
      "[DEBUG] -> 2/2 events overlap the TDMS\n",
      "[OK] 2/2 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 2.28 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 2.34 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 1.71 GB\n",
      "\n",
      "[INFO] Processing Patient 19 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 19:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 19.xlsx\n",
      "[OK] Found 3 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 19.xlsx\n",
      "[DEBUG] Before patient loop RAM-forbrug: 1.71 GB\n",
      "\n",
      "== TDMS == Patient 19\\recording 1\\Patient 19_1.tdms  (patient=19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 19\\recording 1\\Patient 19_1.tdms: 2016-11-15 09:50:01 → 2016-11-17 09:51:09.250000 (lo=2016-11-14 21:50:01, hi=2016-11-17 21:51:09.250000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2016-11-15 14:49:42 → 2016-11-15 14:49:54\n",
      "    evt2: 2016-11-16 20:55:37 → 2016-11-16 20:56:01\n",
      "    evt3: 2016-11-17 03:14:11 → 2016-11-17 03:14:20\n",
      "[DEBUG] -> 3/3 events overlap the TDMS\n",
      "[OK] 3/3 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 2.40 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 2.43 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 2.48 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 1.82 GB\n",
      "\n",
      "[INFO] Processing Patient 20 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 20:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 20.xlsx\n",
      "[OK] Found 5 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 20.xlsx\n",
      "[DEBUG] Before patient loop RAM-forbrug: 1.82 GB\n",
      "\n",
      "== TDMS == Patient 20\\recording 1\\Patient 20_1.tdms  (patient=20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 20\\recording 1\\Patient 20_1.tdms: 2017-03-20 13:12:03 → 2017-03-22 15:19:39 (lo=2017-03-20 01:12:03, hi=2017-03-23 03:19:39)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-03-21 06:18:12 → 2017-03-21 06:19:10\n",
      "    evt2: 2017-03-22 03:18:53 → 2017-03-22 03:19:25\n",
      "    evt3: 2017-03-22 06:39:15 → 2017-03-22 06:40:55\n",
      "    evt4: 2017-03-23 01:45:54 → 2017-03-23 01:47:08\n",
      "    evt5: 2017-03-23 04:21:54 → 2017-03-23 04:22:51\n",
      "[DEBUG] -> 4/5 events overlap the TDMS\n",
      "[OK] 4/5 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 2.54 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 2.58 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 2.62 GB\n",
      "[SANITY] event04: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 4 RAM-forbrug: 2.66 GB\n",
      "\n",
      "== TDMS == Patient 20\\recording 2\\Patient 20_2.tdms  (patient=20)\n",
      "[DEBUG] TDMS span Patient 20\\recording 2\\Patient 20_2.tdms: 2017-03-22 15:15:02 → 2017-03-23 09:39:57.500000 (lo=2017-03-22 03:15:02, hi=2017-03-23 21:39:57.500000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-03-21 06:18:12 → 2017-03-21 06:19:10\n",
      "    evt2: 2017-03-22 03:18:53 → 2017-03-22 03:19:25\n",
      "    evt3: 2017-03-22 06:39:15 → 2017-03-22 06:40:55\n",
      "    evt4: 2017-03-23 01:45:54 → 2017-03-23 01:47:08\n",
      "    evt5: 2017-03-23 04:21:54 → 2017-03-23 04:22:51\n",
      "[DEBUG] -> 4/5 events overlap the TDMS\n",
      "[OK] 4/5 annotering(er) overlapper recording 2.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 2.23 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 2.25 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 2.31 GB\n",
      "[SANITY] event04: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 4 RAM-forbrug: 2.35 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.10 GB\n",
      "\n",
      "[INFO] Processing Patient 22 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 22:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 22.xls\n",
      "[OK] Found 4 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 22.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 2.10 GB\n",
      "\n",
      "== TDMS == Patient 22\\recording 1\\Patient 22_1.tdms  (patient=22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 22\\recording 1\\Patient 22_1.tdms: 2017-10-09 13:23:52 → 2017-10-11 13:10:21.750000 (lo=2017-10-09 01:23:52, hi=2017-10-12 01:10:21.750000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-10-11 13:18:21 → 2017-10-11 13:20:13\n",
      "    evt2: 2017-10-11 19:02:18 → 2017-10-11 19:09:54\n",
      "    evt3: 2017-10-12 04:05:42 → 2017-10-12 04:10:05\n",
      "    evt4: 2017-10-12 14:51:12 → 2017-10-12 14:53:06\n",
      "[DEBUG] -> 2/4 events overlap the TDMS\n",
      "[OK] 2/4 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 2.53 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 2.54 GB\n",
      "\n",
      "== TDMS == Patient 22\\recording 2\\Patient 22_2.tdms  (patient=22)\n",
      "[DEBUG] TDMS span Patient 22\\recording 2\\Patient 22_2.tdms: 2017-10-11 13:11:18 → 2017-10-13 12:25:55.250000 (lo=2017-10-11 01:11:18, hi=2017-10-14 00:25:55.250000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-10-11 13:18:21 → 2017-10-11 13:20:13\n",
      "    evt2: 2017-10-11 19:02:18 → 2017-10-11 19:09:54\n",
      "    evt3: 2017-10-12 04:05:42 → 2017-10-12 04:10:05\n",
      "    evt4: 2017-10-12 14:51:12 → 2017-10-12 14:53:06\n",
      "[DEBUG] -> 4/4 events overlap the TDMS\n",
      "[OK] 4/4 annotering(er) overlapper recording 2.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 2.58 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 2.64 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 2.68 GB\n",
      "[SANITY] event04: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 4 RAM-forbrug: 2.73 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.08 GB\n",
      "\n",
      "[INFO] Processing Patient 24 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 24:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 24.xls\n",
      "[OK] Found 3 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 24.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 2.08 GB\n",
      "\n",
      "== TDMS == Patient 24\\recording 1\\Patient 24_1.tdms  (patient=24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 24\\recording 1\\Patient 24_1.tdms: 2018-01-29 11:30:46 → 2018-01-31 14:22:12.500000 (lo=2018-01-28 23:30:46, hi=2018-02-01 02:22:12.500000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2018-01-31 04:06:40 → 2018-01-31 04:08:47\n",
      "    evt2: 2018-01-31 12:50:09 → 2018-01-31 12:53:22\n",
      "    evt3: 2018-02-01 17:38:00 → 2018-02-01 17:39:34\n",
      "[DEBUG] -> 2/3 events overlap the TDMS\n",
      "[OK] 2/3 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 2.83 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 2.88 GB\n",
      "\n",
      "== TDMS == Patient 24\\recording 2\\Patient 24_2.tdms  (patient=24)\n",
      "[DEBUG] TDMS span Patient 24\\recording 2\\Patient 24_2.tdms: 2018-01-31 14:22:57 → 2018-02-02 09:10:00.001953 (lo=2018-01-31 02:22:57, hi=2018-02-02 21:10:00.001953)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2018-01-31 04:06:40 → 2018-01-31 04:08:47\n",
      "    evt2: 2018-01-31 12:50:09 → 2018-01-31 12:53:22\n",
      "    evt3: 2018-02-01 17:38:00 → 2018-02-01 17:39:34\n",
      "[DEBUG] -> 3/3 events overlap the TDMS\n",
      "[OK] 3/3 annotering(er) overlapper recording 2.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 2.78 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 2.81 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 2.86 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.27 GB\n",
      "\n",
      "[INFO] Processing Patient 25 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 25:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 25.xls\n",
      "[OK] Found 1 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 25.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 2.27 GB\n",
      "\n",
      "== TDMS == Patient 25\\recording 1\\Patient 25_1.tdms  (patient=25)\n",
      "[DEBUG] TDMS span Patient 25\\recording 1\\Patient 25_1.tdms: 2018-02-05 13:20:34 → 2018-02-08 13:20:33.500000 (lo=2018-02-05 01:20:34, hi=2018-02-09 01:20:33.500000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2018-02-07 05:04:39 → 2018-02-07 05:05:29\n",
      "[DEBUG] -> 1/1 events overlap the TDMS\n",
      "[OK] 1/1 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 3.29 GB\n",
      "\n",
      "== TDMS == Patient 25\\recording 2\\Patient 25_2.tdms  (patient=25)\n",
      "[DEBUG] TDMS span Patient 25\\recording 2\\Patient 25_2.tdms: 2018-02-08 13:40:00 → 2018-02-09 06:37:42.250000 (lo=2018-02-08 01:40:00, hi=2018-02-09 18:37:42.250000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2018-02-07 05:04:39 → 2018-02-07 05:05:29\n",
      "[DEBUG] -> 0/1 events overlap the TDMS\n",
      "[INFO] 1 annotering(er) fundet, men ingen overlapper denne recording.\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.30 GB\n",
      "\n",
      "[INFO] Processing Patient 26 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 26:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 26.xls\n",
      "[OK] Found 1 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 26.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 2.30 GB\n",
      "\n",
      "== TDMS == Patient 26\\recording 1\\Patient 26_1.tdms  (patient=26)\n",
      "[DEBUG] TDMS span Patient 26\\recording 1\\Patient 26_1.tdms: 2018-03-21 15:51:03 → 2018-03-23 06:34:26.750000 (lo=2018-03-21 03:51:03, hi=2018-03-23 18:34:26.750000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2018-03-21 18:28:52 → 2018-03-21 18:29:59\n",
      "[DEBUG] -> 1/1 events overlap the TDMS\n",
      "[OK] 1/1 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 2.87 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.34 GB\n",
      "\n",
      "[INFO] Processing Patient 30 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 30:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 30.xls\n",
      "[OK] Found 3 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 30.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 2.34 GB\n",
      "\n",
      "== TDMS == Patient 30\\recording 1\\Patient 30_1.tdms  (patient=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 30\\recording 1\\Patient 30_1.tdms: 2018-09-04 13:09:13 → 2018-09-06 07:40:44.500000 (lo=2018-09-04 01:09:13, hi=2018-09-06 19:40:44.500000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2018-09-05 03:37:59 → 2018-09-05 03:38:27\n",
      "    evt2: 2018-09-06 01:44:12 → 2018-09-06 01:44:32\n",
      "    evt3: 2018-09-06 04:07:42 → 2018-09-06 04:08:02\n",
      "[DEBUG] -> 3/3 events overlap the TDMS\n",
      "[OK] 3/3 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 2.97 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 3.02 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 3.06 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.47 GB\n",
      "\n",
      "[INFO] Processing Patient 32 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 32:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 32.xls\n",
      "[OK] Found 3 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 32.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 2.47 GB\n",
      "\n",
      "== TDMS == Patient 32\\recording 1\\Patient 32_1.tdms  (patient=32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 32\\recording 1\\Patient 32_1.tdms: 2017-05-29 13:58:02 → 2017-05-31 09:47:57 (lo=2017-05-29 01:58:02, hi=2017-05-31 21:47:57)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2017-05-30 07:12:36 → 2017-05-30 07:13:38\n",
      "    evt2: 2017-05-30 15:21:10 → 2017-05-30 15:22:00\n",
      "    evt3: 2017-05-31 06:45:21 → 2017-05-31 06:45:50\n",
      "[DEBUG] -> 3/3 events overlap the TDMS\n",
      "[OK] 3/3 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 3.12 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 3.16 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 3.20 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.60 GB\n",
      "\n",
      "[INFO] Processing Patient 33 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 33:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 33.xls\n",
      "[INFO] No annotations found for Patient 33 / None — skipping.\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.60 GB\n",
      "\n",
      "[INFO] Processing Patient 35 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 35:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 35.xls\n",
      "[OK] Found 5 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 35.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 2.60 GB\n",
      "\n",
      "== TDMS == Patient 35\\recording 1\\Patient 35_1.tdms  (patient=35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 35\\recording 1\\Patient 35_1.tdms: 2018-02-12 16:35:14 → 2018-02-13 13:01:09.750000 (lo=2018-02-12 04:35:14, hi=2018-02-14 01:01:09.750000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2018-02-13 02:11:09 → 2018-02-13 02:11:58\n",
      "    evt2: 2018-02-13 03:11:43 → 2018-02-13 03:12:07\n",
      "    evt3: 2018-02-13 04:35:32 → 2018-02-13 04:36:07\n",
      "    evt4: 2018-02-13 06:24:05 → 2018-02-13 06:24:43\n",
      "    evt5: 2018-02-13 07:31:41 → 2018-02-13 07:32:26\n",
      "[DEBUG] -> 5/5 events overlap the TDMS\n",
      "[OK] 5/5 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 2.92 GB\n",
      "[SANITY] event02: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 2 RAM-forbrug: 2.95 GB\n",
      "[SANITY] event03: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 3 RAM-forbrug: 3.00 GB\n",
      "[SANITY] event04: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 4 RAM-forbrug: 3.04 GB\n",
      "[SANITY] event05: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 5 RAM-forbrug: 3.08 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.80 GB\n",
      "\n",
      "[INFO] Processing Patient 36 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 36:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 36.xls\n",
      "[OK] Found 1 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 36.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 2.80 GB\n",
      "\n",
      "== TDMS == Patient 36\\recording 1\\Patient 36_1.tdms  (patient=36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvjkv\\AppData\\Local\\Temp\\ipykernel_15320\\3480391652.py:349: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  res['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TDMS span Patient 36\\recording 1\\Patient 36_1.tdms: 2018-02-19 14:25:18 → 2018-02-22 08:33:58.750000 (lo=2018-02-19 02:25:18, hi=2018-02-22 20:33:58.750000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2018-02-19 20:38:44 → 2018-02-19 20:40:30\n",
      "[DEBUG] -> 1/1 events overlap the TDMS\n",
      "[OK] 1/1 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 3.75 GB\n",
      "\n",
      "== TDMS == Patient 36\\recording 2\\Patient 36_2.tdms  (patient=36)\n",
      "[DEBUG] TDMS span Patient 36\\recording 2\\Patient 36_2.tdms: 2018-02-22 08:31:43 → 2018-02-23 08:18:01.500000 (lo=2018-02-21 20:31:43, hi=2018-02-23 20:18:01.500000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2018-02-19 20:38:44 → 2018-02-19 20:40:30\n",
      "[DEBUG] -> 0/1 events overlap the TDMS\n",
      "[INFO] 1 annotering(er) fundet, men ingen overlapper denne recording.\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.84 GB\n",
      "\n",
      "[INFO] Processing Patient 38 | Enrollment: enrollment a\n",
      "Warning: kunne ikke parse E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\patient 38a.xls: Can only use .dt accessor with datetimelike values\n",
      "[DEBUG] Potential annotation files for patient 38:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 38a.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 38b.xls\n",
      "[INFO] No annotations found for Patient 38 / enrollment a — skipping.\n",
      "\n",
      "[INFO] Processing Patient 38 | Enrollment: enrolment b\n",
      "[DEBUG] Potential annotation files for patient 38:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 38a.xls\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 38b.xls\n",
      "[INFO] No annotations found for Patient 38 / enrolment b — skipping.\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.84 GB\n",
      "\n",
      "[INFO] Processing Patient 43 | Enrollment: (none)\n",
      "[DEBUG] Potential annotation files for patient 43:\n",
      "    E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 43.xls\n",
      "[OK] Found 1 annotation(s) in: E:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Seizure log ePatch patients with seizures - excluded seizures removed\\Patient 43.xls\n",
      "[DEBUG] Before patient loop RAM-forbrug: 2.84 GB\n",
      "\n",
      "== TDMS == Patient 43\\recording 1\\Patient 43_1.tdms  (patient=43)\n",
      "[DEBUG] TDMS span Patient 43\\recording 1\\Patient 43_1.tdms: 2018-10-24 11:17:05 → 2018-10-26 08:30:32.500000 (lo=2018-10-23 23:17:05, hi=2018-10-26 20:30:32.500000)\n",
      "[DEBUG] Events (raw):\n",
      "    evt1: 2018-10-24 12:54:35 → 2018-10-24 12:55:55\n",
      "[DEBUG] -> 1/1 events overlap the TDMS\n",
      "[OK] 1/1 annotering(er) overlapper recording 1.\n",
      "[SANITY] event01: ok=True within=True err_start=0.00 samp, err_end=0.00 samp\n",
      "[DEBUG] Efter event 1 RAM-forbrug: 3.50 GB\n",
      "[DEBUG] After cleanup recording 1 RAM-forbrug: 2.88 GB\n"
     ]
    }
   ],
   "source": [
    "# for i in patients_responders:\n",
    "#     run_for_patient(i)\n",
    "\n",
    "# print(len(patients_responders))\n",
    "\n",
    "for i in patients_non_responders:\n",
    "    run_for_patient(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553986da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing patients due to excluded seizures from annotations.\n",
    "missing_patients = [\"Patient 6\", \"Patient 10\", \"Patient 29\"]\n",
    "\n",
    "for i in missing_patients:\n",
    "    run_for_patient(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
