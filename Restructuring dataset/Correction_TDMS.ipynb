{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f58149f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nptdms import TdmsFile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "path_base_folder = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\"\n",
    "output_base_folder = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\"\n",
    "\n",
    "patient_path_1 = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 1\\recording 2\\Patient 1_2.tdms\"\n",
    "\n",
    "Patients_to_use = [\"Patient 3\", \"Patient 5\", \"Patient 6\", \"Patient 8\", \"Patient 10\", \"Patient 14\", \"Patient 15\", \"Patient 16\", \"Patient 21\", \"Patient 23\", \"Patient 27\", \"Patient 28\", \"Patient 29\", \"Patient 31\", \"Patient 34\", \"Patient 37\", \"Patient 39\", \"Patient 40\", \"Patient 41\", \"Patient 42\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "518610c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_tdms(tdms_path):\n",
    "    tdms_path = Path(tdms_path)\n",
    "    with TdmsFile.read(tdms_path) as tdms:\n",
    "        print(f\"\\n=== FILE: {tdms_path.name} ===\")\n",
    "        print(\"File properties:\")\n",
    "        for k, v in tdms.properties.items():\n",
    "            print(f\"  - {k}: {v}\")\n",
    "        for group in tdms.groups():\n",
    "            print(f\"\\n[Group] {group.name}\")\n",
    "            # (Gruppe-properties kan ikke altid skrives 1:1 ud igen; vi bibeholder primært fil- og kanal-properties)\n",
    "            for ch in group.channels():\n",
    "                print(f\"  [Channel] {ch.name}\")\n",
    "                # Vis udvalgte properties\n",
    "                prop_keys = list(ch.properties.keys())\n",
    "                interesting = [p for p in prop_keys if p.lower() in {\"wf_start_time\", \"wf_increment\", \"unit_string\", \"name\"}]\n",
    "                for p in interesting:\n",
    "                    print(f\"    - {p}: {ch.properties[p]}\")\n",
    "                # Vis antal samples\n",
    "                try:\n",
    "                    n = ch._length  # hurtigt, uden at læse alt\n",
    "                except Exception:\n",
    "                    n = len(ch[:])   # fallback\n",
    "                print(f\"    - samples: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fad6fa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final patient folders: ['D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 10\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 14\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 14\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 15\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 15\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 16\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 21\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 21\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 23\\\\enrollment a\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 23\\\\enrollment b\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 23\\\\enrollment b\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 27\\\\enrollment a\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 27\\\\enrollment b\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 28\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 28\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 29\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 3\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 3\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 31\\\\enrollment a\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 31\\\\enrollment b\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 31\\\\enrollment b\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 34\\\\enrollment a\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 34\\\\enrollment b\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 34\\\\enrollment c\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 34\\\\enrollment c\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 37\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 39\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 40\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 40\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 41\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 42\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 5\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 6\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 6\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 8\\\\Enrollment a\\\\recording 1', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 8\\\\Enrollment a\\\\recording 2', 'D:\\\\ML algoritme tl anfaldsdetektion vha HRV\\\\ePatch data from Aarhus to Lausanne\\\\Patients ePatch data\\\\Patient 8\\\\Enrollment b']\n"
     ]
    }
   ],
   "source": [
    "final_patient_folders = []\n",
    "folders = [folder for folder in os.listdir(path_base_folder) if folder in Patients_to_use]\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(path_base_folder, folder)\n",
    "    output_path = os.path.join(output_base_folder, folder)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        if not dirs:  # Deepest folder\n",
    "            final_patient_folders.append(root)\n",
    "\n",
    "print(\"Final patient folders:\", final_patient_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7e99b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FILE: Patient 10_1.tdms ===\n",
      "File properties:\n",
      "  - name: Sidsel Borg Nielsen 20062017 kl_134443 __23062017_kl_130000.tdms\n",
      "  - author: Jesper\n",
      "  - description: \n",
      "  - registertxt1: Written by National Instruments LabVIEW\n",
      "\n",
      "[Group] Untitled\n",
      "  [Channel] EKG\n",
      "    - wf_start_time: 2017-06-20T11:44:43.000000\n",
      "    - wf_increment: 0.001953125\n",
      "    - unit_string: uV\n",
      "    - samples: 132710144\n",
      "D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 10\\recording 1\n",
      "Patient 10\n",
      "Output path for corrected file: D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\n"
     ]
    }
   ],
   "source": [
    "for folder1 in final_patient_folders[0:1]:  # Test med én mappe først\n",
    "    files_in_folder = os.listdir(folder1)\n",
    "    tdms_files = [f for f in files_in_folder if f.endswith('.tdms')]\n",
    "    if tdms_files:\n",
    "        path_tdms = os.path.join(folder1, tdms_files[0])\n",
    "        inspect_tdms(path_tdms)\n",
    "        tdms_file = TdmsFile.read(path_tdms)\n",
    "\n",
    "        print(folder1)\n",
    "        parent_folder = os.path.basename(os.path.dirname(folder1))\n",
    "        print(parent_folder)\n",
    "        parent_subfolder = os.path.basename(folder1)\n",
    "        output_path_file = os.path.join(output_base_folder, parent_folder, parent_subfolder, tdms_files[0])\n",
    "        print(\"Output path for corrected file:\", output_path_file)\n",
    "\n",
    "        # tdms_file_path = os.path.join(folder1, tdms_files[0])\n",
    "        # output = os.path.join(output_base_folder, os.path.basename(folder1), tdms_files[0])\n",
    "        # x = correct_tdms_start_time(tdms_file_path, output)\n",
    "        # print(\"TDMS file path:\", tdms_file_path)\n",
    "        # print(\"Output path:\", output)\n",
    "        # print(\"Corrected file:\", x)\n",
    "        # print(\"TDMS files:\", tdms_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef589837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file properties:\n",
      "  name: Patient 10_1 20_06_2017 kl_13_44_43 __23_06_2017 kl_13_00_00\n",
      "  author: Jesper\n",
      "  description: \n",
      "  registertxt1: Written by National Instruments LabVIEW\n",
      "  Corrected_by: : Kasper Viken Jensen\n"
     ]
    }
   ],
   "source": [
    "tdms_file.properties[\"name\"] = \"Patient 10_1 20_06_2017 kl_13_44_43 __23_06_2017 kl_13_00_00\"\n",
    "tdms_file.properties[\"Corrected_by: \"] = \"Kasper Viken Jensen\"\n",
    "print(\"Updated file properties:\")\n",
    "for key, value in tdms_file.properties.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5b23981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel properties before correction:\n",
      "  wf_start_time: 2017-06-20T11:44:43.000000\n",
      "  wf_start_offset: 0.0\n",
      "  wf_increment: 0.001953125\n",
      "  wf_samples: 132710144\n",
      "  DigitalMaximum: 32767\n",
      "  DigitalMinimum: -32768\n",
      "  NI_ChannelName: EKG\n",
      "  NI_ExpIsRelativeTime: False\n",
      "  wf_time_pref: absolute\n",
      "  NI_ExpStartTimeStamp: 2017-06-20T11:44:43.000000\n",
      "  NI_ExpTimeStamp: 2017-06-20T11:44:43.000000\n",
      "  NI_ExpXDimension: t\n",
      "  wf_xname: Time\n",
      "  wf_xunit_string: s\n",
      "  NI_UnitDescription: uV\n",
      "  unit_string: uV\n",
      "  PhysicalMaximum: 90298.0\n",
      "  PhysicalMinimum: -90298.0\n",
      "  PreFilter: \n",
      "  TransducerType: \n",
      "  ValsPerRec: 128\n"
     ]
    }
   ],
   "source": [
    "timestamp_corrected1 = np.datetime64('2017-06-20T13:44:43.000000')\n",
    "\n",
    "channel1 = tdms_file.groups()[0].channels()[0]\n",
    "print(\"Channel properties before correction:\")\n",
    "for key, value in channel1.properties.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2914346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel properties after correction:\n",
      "  wf_start_time: 2017-06-20T13:44:43.000000\n",
      "  wf_start_offset: 0.0\n",
      "  wf_increment: 0.001953125\n",
      "  wf_samples: 132710144\n",
      "  DigitalMaximum: 32767\n",
      "  DigitalMinimum: -32768\n",
      "  NI_ChannelName: EKG\n",
      "  NI_ExpIsRelativeTime: False\n",
      "  wf_time_pref: absolute\n",
      "  NI_ExpStartTimeStamp: 2017-06-20T13:44:43.000000\n",
      "  NI_ExpTimeStamp: 2017-06-20T13:44:43.000000\n",
      "  NI_ExpXDimension: t\n",
      "  wf_xname: Time\n",
      "  wf_xunit_string: s\n",
      "  NI_UnitDescription: uV\n",
      "  unit_string: uV\n",
      "  PhysicalMaximum: 90298.0\n",
      "  PhysicalMinimum: -90298.0\n",
      "  PreFilter: \n",
      "  TransducerType: \n",
      "  ValsPerRec: 128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tdms_file.groups()[0].channels()[0].properties[\"wf_start_time\"] = timestamp_corrected1\n",
    "tdms_file.groups()[0].channels()[0].properties[\"NI_ExpStartTimeStamp\"] = timestamp_corrected1\n",
    "tdms_file.groups()[0].channels()[0].properties[\"NI_ExpTimeStamp\"] = timestamp_corrected1\n",
    "\n",
    "print(\"Channel properties after correction:\")\n",
    "for key, value in channel1.properties.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb7790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: Untitled\n",
      "  Channel: EKG\n",
      "    wf_start_time: 2017-06-20T13:44:43.000000\n",
      "    wf_start_offset: 0.0\n",
      "    wf_increment: 0.001953125\n",
      "    wf_samples: 132710144\n",
      "    DigitalMaximum: 32767\n",
      "    DigitalMinimum: -32768\n",
      "    NI_ChannelName: EKG\n",
      "    NI_ExpIsRelativeTime: False\n",
      "    wf_time_pref: absolute\n",
      "    NI_ExpStartTimeStamp: 2017-06-20T13:44:43.000000\n",
      "    NI_ExpTimeStamp: 2017-06-20T13:44:43.000000\n",
      "    NI_ExpXDimension: t\n",
      "    wf_xname: Time\n",
      "    wf_xunit_string: s\n",
      "    NI_UnitDescription: uV\n",
      "    unit_string: uV\n",
      "    PhysicalMaximum: 90298.0\n",
      "    PhysicalMinimum: -90298.0\n",
      "    PreFilter: \n",
      "    TransducerType: \n",
      "    ValsPerRec: 128\n"
     ]
    }
   ],
   "source": [
    "# for group in tdms_file.groups():\n",
    "#     print(f\"Group: {group.name}\")\n",
    "#     for channel in group.channels():\n",
    "#         print(f\"  Channel: {channel.name}\")\n",
    "#         for key, value in channel.properties.items():\n",
    "#             print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95347d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing corrected TDMS file to: D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m rawdata_csv_path \u001b[38;5;241m=\u001b[39m output_path_file\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tdms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_rawdata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m channel \u001b[38;5;241m=\u001b[39m channels_to_copy[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 35\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavetxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrawdata_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw data written to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrawdata_csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Write group, channel data and properties to another CSV\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kvjkv\\anaconda3\\envs\\kvj\\Lib\\site-packages\\numpy\\lib\\npyio.py:1628\u001b[0m, in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[0;32m   1624\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1625\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between array dtype (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1626\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat specifier (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1627\u001b[0m                             \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mstr\u001b[39m(X\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;28mformat\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m-> 1628\u001b[0m         \u001b[43mfh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(footer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1631\u001b[0m     footer \u001b[38;5;241m=\u001b[39m footer\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m comments)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from nptdms import TdmsWriter, RootObject\n",
    "# import os\n",
    "\n",
    "# print(\"Writing corrected TDMS file to:\", output_path_file)\n",
    "\n",
    "# # Ensure the parent directory exists\n",
    "# os.makedirs(os.path.dirname(output_path_file), exist_ok=True)\n",
    "\n",
    "# from nptdms import RootObject\n",
    "# import csv\n",
    "\n",
    "# original_groups = tdms_file.groups()\n",
    "# original_channels = [chan for chan in original_groups[0].channels()]\n",
    "\n",
    "# # for chan in original_channels:\n",
    "# #     print(f\"Channel: {chan.name}\")\n",
    "# #     for key, value in chan.properties.items():\n",
    "# #         print(f\"  {key}: {value}\")\n",
    "\n",
    "# # with TdmsWriter(output_path_file) as writer:\n",
    "# #     # Create RootObject with updated file properties\n",
    "# #     root_obj = RootObject(tdms_file.properties)\n",
    "# #     # Get all groups and channels from the original file\n",
    "# channels_to_copy = [chan for chan in original_channels]\n",
    "# #     writer.write_segment([root_obj] + original_groups + channels_to_copy)\n",
    "\n",
    "# # for chan in channels_to_copy:\n",
    "# #     print(f\"Channel: {chan.name}\")\n",
    "# #     for key, value in chan.properties.items():\n",
    "# #         print(f\"  {key}: {value}\")\n",
    "\n",
    "# # Write raw data to CSV\n",
    "# rawdata_csv_path = output_path_file.replace('.tdms', '_rawdata.csv')\n",
    "# channel = channels_to_copy[0]\n",
    "# np.savetxt(rawdata_csv_path, channel[:], delimiter=\",\")\n",
    "# print(f\"Raw data written to: {rawdata_csv_path}\")\n",
    "\n",
    "# # Write group, channel data and properties to another CSV\n",
    "# meta_csv_path = output_path_file.replace('.tdms', '_meta.csv')\n",
    "# with open(meta_csv_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     writer.writerow(['Group', 'Channel', 'Property', 'Value'])\n",
    "#     for group in tdms_file.groups():\n",
    "#         for chan in group.channels():\n",
    "#             for key, value in chan.properties.items():\n",
    "#                 writer.writerow([group.name, chan.name, key, value])\n",
    "# print(f\"Group/channel properties written to: {meta_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be317a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels to copy: ['EKG']\n",
      "Channel: EKG\n",
      "  wf_start_time: 2017-06-20T13:44:43.000000\n",
      "  wf_start_offset: 0.0\n",
      "  wf_increment: 0.001953125\n",
      "  wf_samples: 132710144\n",
      "  DigitalMaximum: 32767\n",
      "  DigitalMinimum: -32768\n",
      "  NI_ChannelName: EKG\n",
      "  NI_ExpIsRelativeTime: False\n",
      "  wf_time_pref: absolute\n",
      "  NI_ExpStartTimeStamp: 2017-06-20T13:44:43.000000\n",
      "  NI_ExpTimeStamp: 2017-06-20T13:44:43.000000\n",
      "  NI_ExpXDimension: t\n",
      "  wf_xname: Time\n",
      "  wf_xunit_string: s\n",
      "  NI_UnitDescription: uV\n",
      "  unit_string: uV\n",
      "  PhysicalMaximum: 90298.0\n",
      "  PhysicalMinimum: -90298.0\n",
      "  PreFilter: \n",
      "  TransducerType: \n",
      "  ValsPerRec: 128\n"
     ]
    }
   ],
   "source": [
    "# channels_to_copy = [chan for chan in tdms_file.groups()[0].channels()]\n",
    "# print(\"Channels to copy:\", [chan.name for chan in channels_to_copy])\n",
    "# for chan in channels_to_copy:\n",
    "#     print(f\"Channel: {chan.name}\")\n",
    "#     for key, value in chan.properties.items():\n",
    "#         print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91450e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting corrected TDMS file:\n",
      "Group: Untitled\n"
     ]
    }
   ],
   "source": [
    "# # Inspect the corrected file\n",
    "# # inspect_tdms(output_path_file)\n",
    "\n",
    "# tdms_corrected_1 = TdmsFile(output_path_file)\n",
    "\n",
    "# print(\"Inspecting corrected TDMS file:\")\n",
    "# for group in tdms_corrected_1.groups()[0:1]:\n",
    "#     print(f\"Group: {group.name}\")\n",
    "#     for channel in group.channels():\n",
    "#         print(f\"  Channel: {channel.name}\")\n",
    "#         for key, value in channel.properties.items():\n",
    "#             print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e48382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nptdms import TdmsFile, TdmsWriter, RootObject\n",
    "\n",
    "# original_file = TdmsFile(r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\")\n",
    "# original_groups = original_file.groups()\n",
    "# original_channels = [chan for group in original_groups for chan in group.channels()]\n",
    "\n",
    "# with TdmsWriter(r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\") as copied_file:\n",
    "#     root_object = RootObject(original_file.properties)\n",
    "#     channels_to_copy = [chan for chan in original_channels]\n",
    "#     copied_file.write_segment([root_object] + original_groups + channels_to_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b32b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: Untitled\n"
     ]
    }
   ],
   "source": [
    "# corrected_file21 = TdmsFile(r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\")\n",
    "\n",
    "# groups = corrected_file21.groups()\n",
    "# channels = [chan for group in groups for chan in group.channels()]\n",
    "\n",
    "# for group in groups:\n",
    "#     print(f\"Group: {group.name}\")\n",
    "#     for channel in group.channels():\n",
    "#         print(f\"  Channel: {channel.name}\")\n",
    "#         for key, value in channel.properties.items():\n",
    "#             print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Skrev TDMS med 1 kanal(er) → D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\n"
     ]
    }
   ],
   "source": [
    "# from nptdms import TdmsFile, TdmsWriter\n",
    "# from nptdms.writer import RootObject, GroupObject, ChannelObject\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "\n",
    "# src = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\"\n",
    "# dst = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\"\n",
    "\n",
    "# Path(dst).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# with TdmsFile.read(src) as tf:\n",
    "#     # Fil-properties (du kan tilføje/rette her)\n",
    "#     file_props = dict(tf.properties) if tf.properties else {}\n",
    "#     file_props[\"Corrected_by\"] = \"Kasper Viken Jensen\"\n",
    "\n",
    "#     writer_objs = [RootObject(file_props)]\n",
    "\n",
    "#     # Kopiér alle grupper og kanaler\n",
    "#     for g in tf.groups():\n",
    "#         # group-properties hvis du vil bevare dem:\n",
    "#         g_props = dict(getattr(g, \"properties\", {}) or {})\n",
    "#         writer_objs.append(GroupObject(g.name, properties=g_props))\n",
    "\n",
    "#         for ch in g.channels():\n",
    "#             data = ch[:]                      # <-- HENTER SAMPLES\n",
    "#             props = dict(ch.properties) or {} # kopiér kanal-properties\n",
    "\n",
    "#             # Gør tidsfelter skrivbare (numpy.datetime64 anbefales)\n",
    "#             for k in (\"wf_start_time\", \"NI_ExpStartTimeStamp\", \"NI_ExpTimeStamp\"):\n",
    "#                 if k in props and isinstance(props[k], str):\n",
    "#                     try:\n",
    "#                         props[k] = np.datetime64(props[k])\n",
    "#                     except Exception:\n",
    "#                         pass\n",
    "\n",
    "#             # wf_samples udledes af len(data) – fjern hvis til stede\n",
    "#             props.pop(\"wf_samples\", None)\n",
    "\n",
    "#             writer_objs.append(ChannelObject(g.name, ch.name, data, properties=props))\n",
    "\n",
    "# # Skriv hele segmentet på én gang\n",
    "# with TdmsWriter(dst) as w:\n",
    "#     w.write_segment(writer_objs)\n",
    "\n",
    "# print(f\"[OK] Skrev TDMS med {sum(isinstance(o, ChannelObject) for o in writer_objs)} kanal(er) → {dst}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94655086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Planlægger at skrive 1 kanal(er):\n",
      "  - Untitled/EKG: samples=132710144, dtype=float64\n",
      "[OK] Skrev 1 kanal(er) til: D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\n"
     ]
    }
   ],
   "source": [
    "# from nptdms import TdmsFile, TdmsWriter\n",
    "# from nptdms.writer import RootObject, GroupObject, ChannelObject\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "\n",
    "# def copy_tdms(src_path, dst_path, corrected_by=None):\n",
    "#     src = Path(src_path); dst = Path(dst_path)\n",
    "#     dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     with TdmsFile.read(src) as tf, TdmsWriter(dst) as writer:\n",
    "#         file_props = dict(tf.properties) if tf.properties else {}\n",
    "#         if corrected_by:\n",
    "#             file_props[\"Corrected_by\"] = corrected_by\n",
    "\n",
    "#         writer_objs = [RootObject(file_props)]\n",
    "#         channel_names = []  # til debug-print\n",
    "\n",
    "#         for g in tf.groups():\n",
    "#             g_props = dict(getattr(g, \"properties\", {}) or {})\n",
    "#             writer_objs.append(GroupObject(g.name, properties=g_props))\n",
    "\n",
    "#             for ch in g.channels():\n",
    "#                 # DETACH data fra memmap\n",
    "#                 data = np.array(ch[:], copy=True)\n",
    "\n",
    "#                 # props = dict(ch.properties) or {}\n",
    "#                 props = {}\n",
    "#                 # normalisér tidsfelter\n",
    "#                 for k in (\"wf_start_time\", \"NI_ExpStartTimeStamp\", \"NI_ExpTimeStamp\"):\n",
    "#                     if k in props and isinstance(props[k], str):\n",
    "#                         try:\n",
    "#                             props[k] = np.datetime64(props[k])\n",
    "#                         except Exception:\n",
    "#                             pass\n",
    "#                 props.pop(\"wf_samples\", None)\n",
    "\n",
    "#                 writer_objs.append(ChannelObject(g.name, ch.name, data, properties=props))\n",
    "#                 channel_names.append((g.name, ch.name, len(data), data.dtype))\n",
    "\n",
    "#         print(f\"[DEBUG] Planlægger at skrive {len(channel_names)} kanal(er):\")\n",
    "#         for gn, cn, n, dt in channel_names:\n",
    "#             print(f\"  - {gn}/{cn}: samples={n}, dtype={dt}\")\n",
    "\n",
    "#         if not channel_names:\n",
    "#             raise RuntimeError(\"Ingen kanaler fundet at skrive. Tjek at kildefilen faktisk indeholder kanaler.\")\n",
    "\n",
    "#         writer.write_segment(writer_objs)\n",
    "\n",
    "#     print(f\"[OK] Skrev {len(channel_names)} kanal(er) til: {dst}\")\n",
    "\n",
    "# # Eksempel-kald:\n",
    "# copy_tdms(\n",
    "#     r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\",\n",
    "#     r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\",\n",
    "#     corrected_by=\"Kasper Viken Jensen\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0efb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Group] Untitled\n"
     ]
    }
   ],
   "source": [
    "# with TdmsFile.read(r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\") as t:\n",
    "#     for g in t.groups():\n",
    "#         print(\"[Group]\", g.name)\n",
    "#         for c in g.channels():\n",
    "#             print(\"  [Channel]\", c.name, \"samples:\", len(c[:]))\n",
    "#             for k in (\"wf_start_time\",\"wf_increment\",\"unit_string\",\"NI_ChannelName\"):\n",
    "#                 if k in c.properties:\n",
    "#                     print(\"    -\", k, \":\", c.properties[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74482215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] minimal write: channels=1\n",
      "[OK] Minimal kopi skrevet → D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1__mincopy.tdms\n",
      "[Group] Untitled\n",
      "  [Channel] EKG samples: 132710144\n"
     ]
    }
   ],
   "source": [
    "from nptdms import TdmsFile, TdmsWriter\n",
    "from nptdms.writer import RootObject, GroupObject, ChannelObject\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def tdms_min_copy(src_path, dst_path, corrected_by=None):\n",
    "    src, dst = Path(src_path), Path(dst_path)\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with TdmsFile.read(src) as tf, TdmsWriter(dst) as writer:\n",
    "        file_props = dict(tf.properties) if tf.properties else {}\n",
    "        if corrected_by:\n",
    "            file_props[\"Corrected_by\"] = corrected_by\n",
    "\n",
    "        objs = [RootObject(file_props)]\n",
    "        ch_count = 0\n",
    "        for g in tf.groups():\n",
    "            objs.append(GroupObject(g.name))\n",
    "            for ch in g.channels():\n",
    "                data = np.array(ch[:], copy=True)  # detach fra memmap\n",
    "                objs.append(ChannelObject(g.name, ch.name, data, properties={}))  # <-- ingen props\n",
    "                ch_count += 1\n",
    "\n",
    "        print(f\"[DEBUG] minimal write: channels={ch_count}\")\n",
    "        writer.write_segment(objs)\n",
    "\n",
    "    print(f\"[OK] Minimal kopi skrevet → {dst}\")\n",
    "\n",
    "\n",
    "# Kør denne med dine stier\n",
    "src = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1.tdms\"\n",
    "dst_min = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1__mincopy.tdms\"\n",
    "tdms_min_copy(src, dst_min, corrected_by=\"Kasper Viken Jensen\")\n",
    "\n",
    "# Verificér: bør nu vise kanaler\n",
    "with TdmsFile.read(dst_min) as t:\n",
    "    for g in t.groups():\n",
    "        print(\"[Group]\", g.name)\n",
    "        for c in g.channels():\n",
    "            print(\"  [Channel]\", c.name, \"samples:\", len(c[:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cd782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Good props: ['wf_start_time', 'wf_start_offset', 'wf_increment', 'wf_samples', 'DigitalMaximum', 'DigitalMinimum', 'NI_ChannelName', 'NI_ExpIsRelativeTime', 'wf_time_pref', 'NI_ExpStartTimeStamp', 'NI_ExpTimeStamp', 'NI_ExpXDimension', 'wf_xname', 'wf_xunit_string', 'NI_UnitDescription', 'unit_string', 'PhysicalMaximum', 'PhysicalMinimum', 'PreFilter', 'TransducerType', 'ValsPerRec']\n",
      "[!!] Bad props: []\n"
     ]
    }
   ],
   "source": [
    "from nptdms import TdmsFile, TdmsWriter\n",
    "from nptdms.writer import RootObject, GroupObject, ChannelObject\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tempfile, os\n",
    "\n",
    "def _safe_cast_prop(k, v):\n",
    "    \"\"\"Gør properties 'writer-venlige'.\"\"\"\n",
    "    import numpy as np\n",
    "    # Tillad primitive typer\n",
    "    if isinstance(v, (str, int, float, bool)):\n",
    "        return v\n",
    "    # Datotider → numpy.datetime64 hvis mulig\n",
    "    for dt_type in (\"datetime\", \"Timestamp\"):\n",
    "        if dt_type in type(v).__name__:\n",
    "            try: return np.datetime64(str(v))\n",
    "            except Exception: pass\n",
    "    # numpy scalars → Python scalars\n",
    "    if hasattr(v, \"item\"):\n",
    "        try: return v.item()\n",
    "        except Exception: pass\n",
    "    # arrays/lister → skip (TDMS props er skalarer)\n",
    "    if isinstance(v, (list, tuple, np.ndarray)):\n",
    "        raise TypeError(\"sequence not allowed\")\n",
    "    # fallback → str\n",
    "    return str(v)\n",
    "\n",
    "def find_bad_props(src_path, sample_len=2000):\n",
    "    src = Path(src_path)\n",
    "    bad = []\n",
    "    good = []\n",
    "    with TdmsFile.read(src) as tf:\n",
    "        g = tf.groups()[0]\n",
    "        ch = g.channels()[0]\n",
    "        data_small = np.array(ch[:sample_len], copy=True)\n",
    "\n",
    "        all_props = dict(ch.properties) or {}\n",
    "        # Gå alle keys igennem enkeltvist\n",
    "        for k, v in all_props.items():\n",
    "            tmpfile = Path(tempfile.gettempdir())/f\"tdms_prop_test_{k}.tdms\"\n",
    "            try:\n",
    "                cast_v = _safe_cast_prop(k, v)\n",
    "                with TdmsWriter(tmpfile) as w:\n",
    "                    objs = [\n",
    "                        RootObject({}),\n",
    "                        GroupObject(g.name),\n",
    "                        ChannelObject(g.name, ch.name, data_small, properties={k: cast_v})\n",
    "                    ]\n",
    "                    w.write_segment(objs)\n",
    "                # læs tilbage og tjek at kanalen findes\n",
    "                with TdmsFile.read(tmpfile) as t2:\n",
    "                    ok = any(True for _g in t2.groups() for _ in _g.channels())\n",
    "                if ok:\n",
    "                    good.append(k)\n",
    "                else:\n",
    "                    bad.append(k)\n",
    "            except Exception as e:\n",
    "                bad.append((k, repr(e)))\n",
    "            finally:\n",
    "                try: os.remove(tmpfile)\n",
    "                except Exception: pass\n",
    "    return good, bad\n",
    "\n",
    "good_props, bad_props = find_bad_props(src)\n",
    "print(\"[OK] Good props:\", good_props)\n",
    "print(\"[!!] Bad props:\", bad_props)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e4ac537c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Skrev 1 kanal(er) til: D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1__final.tdms\n",
      "[Group] Untitled\n",
      "  [Channel] EKG samples: 132710144\n",
      "    - wf_start_time: 2017-06-20T11:44:43.000000\n",
      "    - wf_start_offset: 0.0\n",
      "    - wf_increment: 0.001953125\n",
      "    - DigitalMaximum: 32767\n",
      "    - DigitalMinimum: -32768\n",
      "    - NI_ChannelName: EKG\n",
      "    - NI_ExpIsRelativeTime: False\n",
      "    - wf_time_pref: absolute\n",
      "    - NI_ExpStartTimeStamp: 2017-06-20T11:44:43.000000\n",
      "    - NI_ExpTimeStamp: 2017-06-20T11:44:43.000000\n",
      "    - NI_ExpXDimension: t\n",
      "    - wf_xname: Time\n",
      "    - wf_xunit_string: s\n",
      "    - NI_UnitDescription: uV\n",
      "    - unit_string: uV\n",
      "    - PhysicalMaximum: 90298.0\n",
      "    - PhysicalMinimum: -90298.0\n",
      "    - PreFilter: \n",
      "    - TransducerType: \n",
      "    - ValsPerRec: 128\n"
     ]
    }
   ],
   "source": [
    "from nptdms import TdmsFile, TdmsWriter\n",
    "from nptdms.writer import RootObject, GroupObject, ChannelObject\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def write_final_with_whitelist(src_path, dst_path, good_keys, corrected_by=None, new_start_time=None):\n",
    "    src, dst = Path(src_path), Path(dst_path)\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def safe_cast_all(props: dict):\n",
    "        out = {}\n",
    "        for k, v in props.items():\n",
    "            if k not in good_keys:\n",
    "                continue\n",
    "            # cast som i scanneren\n",
    "            try:\n",
    "                out[k] = _safe_cast_prop(k, v)\n",
    "            except Exception:\n",
    "                continue\n",
    "        # evt. overskriv starttid\n",
    "        if new_start_time is not None:\n",
    "            out[\"wf_start_time_original\"] = props.get(\"wf_start_time\", None)\n",
    "            out[\"wf_start_time\"] = np.datetime64(new_start_time)\n",
    "        # fjern 'wf_samples' – udledes af datas længde\n",
    "        out.pop(\"wf_samples\", None)\n",
    "        return out\n",
    "\n",
    "    with TdmsFile.read(src) as tf, TdmsWriter(dst) as writer:\n",
    "        file_props = dict(tf.properties) if tf.properties else {}\n",
    "        if corrected_by:\n",
    "            file_props[\"Corrected_by\"] = corrected_by\n",
    "\n",
    "        objs = [RootObject(file_props)]\n",
    "        ch_count = 0\n",
    "\n",
    "        for g in tf.groups():\n",
    "            objs.append(GroupObject(g.name))\n",
    "            for ch in g.channels():\n",
    "                data = np.array(ch[:], copy=True)\n",
    "                props = safe_cast_all(dict(ch.properties) or {})\n",
    "                objs.append(ChannelObject(g.name, ch.name, data, properties=props))\n",
    "                ch_count += 1\n",
    "\n",
    "        writer.write_segment(objs)\n",
    "\n",
    "    print(f\"[OK] Skrev {ch_count} kanal(er) til: {dst}\")\n",
    "\n",
    "# Kør den endelige skrivning:\n",
    "dst_final = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1__final.tdms\"\n",
    "write_final_with_whitelist(\n",
    "    src_path=src,\n",
    "    dst_path=dst_final,\n",
    "    good_keys=set(good_props),             # fra Celle 2\n",
    "    corrected_by=\"Kasper Viken Jensen\",\n",
    "    new_start_time=None                    # eller fx \"2017-06-20T13:44:43\"\n",
    ")\n",
    "\n",
    "# Verificér\n",
    "with TdmsFile.read(dst_final) as t:\n",
    "    for g in t.groups():\n",
    "        print(\"[Group]\", g.name)\n",
    "        for c in g.channels():\n",
    "            print(\"  [Channel]\", c.name, \"samples:\", len(c[:]))\n",
    "            for k in c.properties:\n",
    "                print(f\"    - {k}: {c.properties[k]}\")\n",
    "            # for k in (\"wf_start_time\",\"wf_increment\",\"unit_string\",\"NI_ChannelName\"):\n",
    "            #     if k in c.properties:\n",
    "            #         print(\"    -\", k, \":\", c.properties[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "64dd884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Group] Untitled\n",
      "  [Channel] EKG samples: 132710144\n",
      "    - wf_start_time: 2017-06-20T11:44:43.000000\n",
      "    - wf_start_offset: 0.0\n",
      "    - wf_increment: 0.001953125\n",
      "    - DigitalMaximum: 32767\n",
      "    - DigitalMinimum: -32768\n",
      "    - NI_ChannelName: EKG\n",
      "    - NI_ExpIsRelativeTime: False\n",
      "    - wf_time_pref: absolute\n",
      "    - NI_ExpStartTimeStamp: 2017-06-20T11:44:43.000000\n",
      "    - NI_ExpTimeStamp: 2017-06-20T11:44:43.000000\n",
      "    - NI_ExpXDimension: t\n",
      "    - wf_xname: Time\n",
      "    - wf_xunit_string: s\n",
      "    - NI_UnitDescription: uV\n",
      "    - unit_string: uV\n",
      "    - PhysicalMaximum: 90298.0\n",
      "    - PhysicalMinimum: -90298.0\n",
      "    - PreFilter: \n",
      "    - TransducerType: \n",
      "    - ValsPerRec: 128\n"
     ]
    }
   ],
   "source": [
    "with TdmsFile.read(r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 10\\recording 1\\Patient 10_1__final.tdms\") as t:\n",
    "    for g in t.groups():\n",
    "        print(\"[Group]\", g.name)\n",
    "        for c in g.channels():\n",
    "            print(\"  [Channel]\", c.name, \"samples:\", len(c[:]))\n",
    "            for k in c.properties:\n",
    "                print(f\"    - {k}: {c.properties[k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e685ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nptdms import TdmsFile, TdmsWriter, TdmsChannel\n",
    "# from pathlib import Path\n",
    "# from datetime import datetime\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# def parse_time_from_string(s: str):\n",
    "#     \"\"\"\n",
    "#     Forsøger fleksibelt at finde et timestamp i en tekst.\n",
    "#     Understøtter fx:\n",
    "#       - 2021-09-30 14:23:05\n",
    "#       - 2021-09-30T14:23:05\n",
    "#       - 30-09-2021 14:23:05\n",
    "#       - 20210930_142305\n",
    "#       - ...og varianter med -, _, / og mellemrum\n",
    "#     Returnerer naive datetime (lokal tid) hvis fundet, ellers None.\n",
    "#     \"\"\"\n",
    "#     candidates = []\n",
    "\n",
    "#     # ISO-ish\n",
    "#     m = re.search(r'(\\d{4}-\\d{2}-\\d{2})[ T_](\\d{2}:\\d{2}:\\d{2})', s)\n",
    "#     if m:\n",
    "#         candidates.append(f\"{m.group(1)} {m.group(2)}\")\n",
    "\n",
    "#     # Kompakt YYYYMMDD_HHMMSS eller YYYYMMDDHHMMSS\n",
    "#     m = re.search(r'(\\d{8})[_-]?(\\d{6})', s)\n",
    "#     if m:\n",
    "#         d, t = m.group(1), m.group(2)\n",
    "#         candidates.append(f\"{d[:4]}-{d[4:6]}-{d[6:]} {t[:2]}:{t[2:4]}:{t[4:]}\")\n",
    "\n",
    "#     # DMY fx 30-09-2021 14:23:05\n",
    "#     m = re.search(r'(\\d{2})[-_/](\\d{2})[-_/](\\d{4})[ T_](\\d{2}):(\\d{2}):(\\d{2})', s)\n",
    "#     if m:\n",
    "#         day, mon, year, hh, mm, ss = m.groups()\n",
    "#         candidates.append(f\"{year}-{mon}-{day} {hh}:{mm}:{ss}\")\n",
    "\n",
    "#     # Ekstra: håndter formater som \"30_09_2021 kl142305\" eller \"30_09_2021 kl_142305\"\n",
    "#     m = re.search(r'(\\d{2})_(\\d{2})_(\\d{4})\\s*kl_?(\\d{2})(\\d{2})(\\d{2})', s)\n",
    "#     if m:\n",
    "#         day, mon, year, hh, mm, ss = m.groups()\n",
    "#         candidates.append(f\"{year}-{mon}-{day} {hh}:{mm}:{ss}\")\n",
    "\n",
    "#     # Prøv alle kandidater\n",
    "#     for c in candidates:\n",
    "#         try:\n",
    "#             return datetime.strptime(c, \"%Y-%m-%d %H:%M:%S\")\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "\n",
    "#     # Sidste forsøg: ren dato uden tid -> sæt tid=00:00:00\n",
    "#     m = re.search(r'(\\d{4}-\\d{2}-\\d{2})', s)\n",
    "#     if m:\n",
    "#         try:\n",
    "#             return datetime.strptime(m.group(1)+\" 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "\n",
    "#     return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f24c3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inspect_tdms(tdms_path):\n",
    "#     tdms_path = Path(tdms_path)\n",
    "#     with TdmsFile.read(tdms_path) as tdms:\n",
    "#         print(f\"\\n=== FILE: {tdms_path.name} ===\")\n",
    "#         print(\"File properties:\")\n",
    "#         for k, v in tdms.properties.items():\n",
    "#             print(f\"  - {k}: {v}\")\n",
    "#         for group in tdms.groups():\n",
    "#             print(f\"\\n[Group] {group.name}\")\n",
    "#             # (Gruppe-properties kan ikke altid skrives 1:1 ud igen; vi bibeholder primært fil- og kanal-properties)\n",
    "#             for ch in group.channels():\n",
    "#                 print(f\"  [Channel] {ch.name}\")\n",
    "#                 # Vis udvalgte properties\n",
    "#                 prop_keys = list(ch.properties.keys())\n",
    "#                 interesting = [p for p in prop_keys if p.lower() in {\"wf_start_time\", \"wf_increment\", \"unit_string\", \"name\"}]\n",
    "#                 for p in interesting:\n",
    "#                     print(f\"    - {p}: {ch.properties[p]}\")\n",
    "#                 # Vis antal samples\n",
    "#                 try:\n",
    "#                     n = ch._length  # hurtigt, uden at læse alt\n",
    "#                 except Exception:\n",
    "#                     n = len(ch[:])   # fallback\n",
    "#                 print(f\"    - samples: {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98b71250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def correct_tdms(\n",
    "#     src_path,\n",
    "#     dst_path,\n",
    "#     corrected_by: str,\n",
    "#     start_time_source: str = \"file_property_name\",  # \"file_property_name\" | \"filename\" | \"exact_string\"\n",
    "#     start_time_exact_string: str = None,            # brug hvis start_time_source == \"exact_string\"\n",
    "#     file_property_key: str = \"name\",                # brugt hvis start_time_source == \"file_property_name\"\n",
    "#     preserve_original_start_time: bool = True\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Læser src TDMS, sætter 'Corrected_by' på filniveau, opdaterer wf_start_time på alle kanaler der har det,\n",
    "#     og skriver til dst TDMS. Skriver altid en liste af kanaler til TdmsWriter (ikke bare en gruppe),\n",
    "#     så vi undgår TypeError i write_segment.\n",
    "#     \"\"\"\n",
    "#     src_path = Path(src_path)\n",
    "#     dst_path = Path(dst_path)\n",
    "#     dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     with TdmsFile.read(src_path) as tdms:\n",
    "#         # 1) Find korrekt starttid\n",
    "#         if start_time_source == \"file_property_name\":\n",
    "#             base = tdms.properties.get(file_property_key, \"\")\n",
    "#             dt = parse_time_from_string(str(base))\n",
    "#             if dt is None:\n",
    "#                 raise ValueError(f\"Kunne ikke parse tid fra file property '{file_property_key}': {base}\")\n",
    "#         elif start_time_source == \"filename\":\n",
    "#             dt = parse_time_from_string(src_path.stem)\n",
    "#             if dt is None:\n",
    "#                 raise ValueError(f\"Kunne ikke parse tid fra filnavn: {src_path.name}\")\n",
    "#         elif start_time_source == \"exact_string\":\n",
    "#             if not start_time_exact_string:\n",
    "#                 raise ValueError(\"Du valgte 'exact_string', men gav ingen streng i start_time_exact_string.\")\n",
    "#             dt = parse_time_from_string(start_time_exact_string)\n",
    "#             if dt is None:\n",
    "#                 raise ValueError(f\"Kunne ikke parse tid fra given streng: {start_time_exact_string}\")\n",
    "#         else:\n",
    "#             raise ValueError(f\"Ukendt start_time_source: {start_time_source}\")\n",
    "\n",
    "#         # nptdms forventer numpy.datetime64 til wf_start_time\n",
    "#         wf_start_np = np.datetime64(dt)\n",
    "\n",
    "#         # 2) Forbered fil-properties\n",
    "#         new_file_props = dict(tdms.properties) if tdms.properties else {}\n",
    "#         new_file_props[\"Corrected_by\"] = corrected_by\n",
    "#         new_file_props[\"Corrected_on\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#         # 3) Byg nye kanaler (vi kopierer data + kanal-properties, og justerer wf_start_time hvor relevant)\n",
    "#         channels_to_write = []\n",
    "#         for group in tdms.groups():\n",
    "#             for ch in group.channels():\n",
    "#                 data = ch[:]  # læs hele kanalens data\n",
    "#                 ch_props = dict(ch.properties) if ch.properties else {}\n",
    "#                 if \"wf_start_time\" in ch_props:\n",
    "#                     if preserve_original_start_time and \"wf_start_time_original\" not in ch_props:\n",
    "#                         ch_props[\"wf_start_time_original\"] = ch_props[\"wf_start_time\"]\n",
    "#                     ch_props[\"wf_start_time\"] = wf_start_np\n",
    "#                 # Konstruér en ny kanal med samme group/name + data + (opdaterede) props\n",
    "#                 new_ch = ch.copy_with_new_properties(properties=ch_props)\n",
    "#                 channels_to_write.append(new_ch)\n",
    "\n",
    "#         # 4) Skriv ny TDMS — VIGTIGT: skriv KANALERNE, ikke kun en gruppe\n",
    "#         with TdmsWriter(dst_path) as writer:\n",
    "#             writer.write_segment(channels_to_write, file_properties=new_file_props)\n",
    "\n",
    "#     return {\n",
    "#         \"dst\": str(dst_path),\n",
    "#         \"parsed_start_time\": str(dt),\n",
    "#         \"channels_written\": len(channels_to_write)\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3403309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Eksempel: tjek én fil inden du retter\n",
    "\n",
    "# tdms_file = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 5\\recording 1\\Patient 5_1.tdms\"\n",
    "# inspect_tdms(tdms_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "922007bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 5\\recording 1\\Patient 5_1.tdms\"\n",
    "# dst = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 5\\recording 1\\Patient 5_1_corrected.tdms\"\n",
    "\n",
    "# result = correct_tdms(\n",
    "#     src_path=src,\n",
    "#     dst_path=dst,\n",
    "#     corrected_by=\"Kasper Vilken Jensen\",\n",
    "#     start_time_source=\"file_property_name\",   # bruger file.properties[\"name\"]\n",
    "#     file_property_key=\"name\"\n",
    "# )\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f4f3ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Henrik Koch 07092016_kl_105046__09092016_kl_103000.tdms ===\n",
      "File properties:\n",
      "  - name: Henrik Koch 07092016_kl_105046__09092016_kl_103000.tdms\n",
      "  - author: Jesper\n",
      "  - description: \n",
      "  - registertxt1: Written by National Instruments LabVIEW\n",
      "\n",
      "[Group] Untitled\n",
      "  [Channel] EKG\n",
      "    - wf_start_time: 2016-09-07T08:50:46.000000\n",
      "    - wf_start_offset: 0.0\n",
      "    - wf_increment: 0.001953125\n",
      "    - wf_samples: 86426624\n",
      "    - DigitalMaximum: 32767\n",
      "    - DigitalMinimum: -32768\n",
      "    - NI_ChannelName: EKG\n",
      "    - NI_ExpIsRelativeTime: False\n",
      "    - wf_time_pref: absolute\n",
      "    - NI_ExpStartTimeStamp: 2016-09-07T08:50:46.000000\n",
      "    - NI_ExpTimeStamp: 2016-09-07T08:50:46.000000\n",
      "    - NI_ExpXDimension: t\n",
      "    - wf_xname: Time\n",
      "    - wf_xunit_string: s\n",
      "    - NI_UnitDescription: uV\n",
      "    - unit_string: uV\n",
      "    - PhysicalMaximum: 90298.0\n",
      "    - PhysicalMinimum: -90298.0\n",
      "    - PreFilter: \n",
      "    - TransducerType: \n",
      "    - ValsPerRec: 128\n",
      "    - samples: 86426624\n",
      "\n",
      "[OK] WRITE_PLAN initialiseret (redigér i næste celle).\n"
     ]
    }
   ],
   "source": [
    "from nptdms import TdmsFile\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "src = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch data from Aarhus to Lausanne\\Patients ePatch data\\Patient 3\\recording 2\\Henrik Koch 07092016_kl_105046__09092016_kl_103000.tdms\"\n",
    "dst_min = r\"D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 3\\recording 2\\Patient 3_2.tdms\"\n",
    "\n",
    "# >>> ANGIV KILDEFILEN HER <<<\n",
    "SRC_PATH = Path(src)\n",
    "\n",
    "def _parse_time_from_text(text: str):\n",
    "    m = re.search(r'(\\d{4}-\\d{2}-\\d{2})[ T_](\\d{2}:\\d{2}:\\d{2})', text)\n",
    "    if m: return f\"{m.group(1)}T{m.group(2)}\"\n",
    "    m = re.search(r'(\\d{8})[_-]?(\\d{6})', text)  # fx 20170620_134443\n",
    "    if m:\n",
    "        d,t = m.groups()\n",
    "        return f\"{d[:4]}-{d[4:6]}-{d[6:]}T{t[:2]}:{t[2:4]}:{t[4:]}\"\n",
    "    return None\n",
    "\n",
    "with TdmsFile.read(SRC_PATH) as tf:\n",
    "    print(f\"=== {SRC_PATH.name} ===\")\n",
    "    print(\"File properties:\")\n",
    "    for k,v in tf.properties.items():\n",
    "        print(f\"  - {k}: {v}\")\n",
    "    # evt. forslag til starttid ud fra file.properties[\"name\"]\n",
    "    suggested = _parse_time_from_text(str(tf.properties.get(\"name\",\"\")))\n",
    "    if suggested:\n",
    "        print(f\"\\n[Hint] Parsed starttid fra 'name': {suggested}\")\n",
    "\n",
    "    for g in tf.groups():\n",
    "        print(f\"\\n[Group] {g.name}\")\n",
    "        for ch in g.channels():\n",
    "            print(f\"  [Channel] {ch.name}\")\n",
    "            # vis centrale props\n",
    "            for k in ch.properties:\n",
    "                print(f\"    - {k}: {ch.properties[k]}\")\n",
    "            # antal samples (hurtigt estimat)\n",
    "            try:\n",
    "                n = ch._length\n",
    "            except Exception:\n",
    "                n = len(ch[:])\n",
    "            print(f\"    - samples: {n}\")\n",
    "\n",
    "# Initier en 'skrivningsplan' du kan redigere i næste celle\n",
    "WRITE_PLAN = {\n",
    "    \"corrected_by\": None,              # fx \"Kasper Viken Jensen\"\n",
    "    \"new_start_time\": None,            # fx \"2017-06-20 13:44:43\" (bruges hvis sat)\n",
    "    \"use_name_property\": True,         # brug parsed tid fra file.properties[\"name\"] hvis new_start_time ikke er sat\n",
    "    \"file_prop_overrides\": {},         # ekstra fil-properties du vil tilføje/rette\n",
    "    \"channel_prop_overrides\": {},      # per-kanal overrides: {\"GroupName/ChannelName\": {\"key\": value, ...}, ...}\n",
    "}\n",
    "print(\"\\n[OK] WRITE_PLAN initialiseret (redigér i næste celle).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "23624e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected time: 2016-09-07T10:50:46.000000\n"
     ]
    }
   ],
   "source": [
    "from numpy import datetime64\n",
    "\n",
    "\n",
    "corrected_time = datetime64('2016-09-07T10:50:46.000000')\n",
    "print(\"Corrected time:\", corrected_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "13b6b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] WRITE_PLAN opdateret:\n",
      " - corrected_by : Kasper Viken Jensen\n",
      " - new_start_time : None\n",
      " - use_name_property : True\n",
      " - file_prop_overrides : {'description': 'Corrected start time & metadata', 'name': 'Patient 3_2 07_09_2016 kl_10_50_46__09_09_2016 kl_10_30_00'}\n",
      " - channel_prop_overrides : {'Untitled/EKG': {}}\n",
      " - wf_start_time : 2016-09-07T10:50:46.000000\n",
      " - NI_ExpStartTimeStamp : 2016-09-07T10:50:46.000000\n",
      " - NI_ExpTimeStamp : 2016-09-07T10:50:46.000000\n"
     ]
    }
   ],
   "source": [
    "# Eksempler — RET som du vil:\n",
    "\n",
    "# Hvem har rettet\n",
    "WRITE_PLAN[\"corrected_by\"] = \"Kasper Viken Jensen\"\n",
    "\n",
    "# Sæt ny starttid eksplicit (foretrækkes for at undgå tz/DST-skred)\n",
    "# (Hvis du lader den være None, og use_name_property=True, bruges parsed værdi fra file.properties[\"name\"])\n",
    "WRITE_PLAN[\"wf_start_time\"] = corrected_time  # eller None\n",
    "WRITE_PLAN[\"NI_ExpStartTimeStamp\"] = corrected_time  # eller None\n",
    "WRITE_PLAN[\"NI_ExpTimeStamp\"] = corrected_time  # eller None\n",
    "\n",
    "# Evt. ekstra fil-properties\n",
    "WRITE_PLAN[\"file_prop_overrides\"].update({\n",
    "    \"description\": \"Corrected start time & metadata\",\n",
    "    \"name\": \"Patient 3_2 07_09_2016 kl_10_50_46__09_09_2016 kl_10_30_00\",\n",
    "})\n",
    "\n",
    "# Evt. kanal-specifikke overrides (eksempel)\n",
    "# key = \"Group/Channel\", fx \"Untitled/EKG\"\n",
    "WRITE_PLAN[\"channel_prop_overrides\"][\"Untitled/EKG\"] = {\n",
    "    # \"unit_string\": \"uV\",\n",
    "    # \"wf_xunit_string\": \"s\",\n",
    "}\n",
    "\n",
    "print(\"[OK] WRITE_PLAN opdateret:\")\n",
    "for k,v in WRITE_PLAN.items():\n",
    "    print(\" -\", k, \":\", v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b9e87be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Skrev ny fil → D:\\ML algoritme tl anfaldsdetektion vha HRV\\ePatch - Corrected\\Patients ePatch data\\Patient 3\\recording 2\\Patient 3_2.tdms\n"
     ]
    }
   ],
   "source": [
    "# === CELLE 3 — Skriv ny TDMS med rettelser fra WRITE_PLAN ===\n",
    "from nptdms import TdmsFile, TdmsWriter\n",
    "from nptdms.writer import RootObject, GroupObject, ChannelObject\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# >>> ANGIV DESTINATIONSFIL HER <<<\n",
    "DST_PATH = Path(dst_min)\n",
    "DST_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _to_npdt64(ts):\n",
    "    \"\"\"Accepterer str / numpy.datetime64 / pandas.Timestamp / datetime -> np.datetime64 (naiv).\"\"\"\n",
    "    if ts is None:\n",
    "        return None\n",
    "    if isinstance(ts, np.datetime64):\n",
    "        return ts\n",
    "    # str: tillad både \"YYYY-MM-DD HH:MM:SS\" og \"YYYY-MM-DDTHH:MM:SS\"\n",
    "    if isinstance(ts, str):\n",
    "        return np.datetime64(ts.replace(\" \", \"T\"))\n",
    "    # generisk fallback\n",
    "    try:\n",
    "        return np.datetime64(str(ts).replace(\" \", \"T\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "with TdmsFile.read(SRC_PATH) as tf, TdmsWriter(DST_PATH) as writer:\n",
    "    # Fil-properties\n",
    "    file_props = dict(tf.properties) if tf.properties else {}\n",
    "    if WRITE_PLAN.get(\"corrected_by\"):\n",
    "        file_props[\"Corrected_by\"] = WRITE_PLAN[\"corrected_by\"]\n",
    "    file_props.update(WRITE_PLAN.get(\"file_prop_overrides\", {}))\n",
    "\n",
    "    # Forbered tidsfelter fra WRITE_PLAN (kan være None)\n",
    "    plan_wf_start   = _to_npdt64(WRITE_PLAN.get(\"wf_start_time\"))\n",
    "    plan_exp_start  = _to_npdt64(WRITE_PLAN.get(\"NI_ExpStartTimeStamp\"))\n",
    "    plan_exp_time   = _to_npdt64(WRITE_PLAN.get(\"NI_ExpTimeStamp\"))\n",
    "\n",
    "    objs = [RootObject(file_props)]\n",
    "\n",
    "    for g in tf.groups():\n",
    "        g_props = dict(getattr(g, \"properties\", {}) or {})\n",
    "        objs.append(GroupObject(g.name, properties=g_props))\n",
    "\n",
    "        for ch in g.channels():\n",
    "            data = np.array(ch[:], copy=True)            # detach fra memmap\n",
    "            props = dict(ch.properties) or {}\n",
    "            props.pop(\"wf_samples\", None)                # afledes af datalængde\n",
    "\n",
    "            # Bevar originaler hvis vi kommer til at overskrive\n",
    "            if plan_wf_start is not None:\n",
    "                if \"wf_start_time_original\" not in props:\n",
    "                    props[\"wf_start_time_original\"] = props.get(\"wf_start_time\", None)\n",
    "                props[\"wf_start_time\"] = plan_wf_start\n",
    "\n",
    "            if plan_exp_start is not None:\n",
    "                if \"NI_ExpStartTimeStamp_original\" not in props:\n",
    "                    props[\"NI_ExpStartTimeStamp_original\"] = props.get(\"NI_ExpStartTimeStamp\", None)\n",
    "                props[\"NI_ExpStartTimeStamp\"] = plan_exp_start\n",
    "\n",
    "            if plan_exp_time is not None:\n",
    "                if \"NI_ExpTimeStamp_original\" not in props:\n",
    "                    props[\"NI_ExpTimeStamp_original\"] = props.get(\"NI_ExpTimeStamp\", None)\n",
    "                props[\"NI_ExpTimeStamp\"] = plan_exp_time\n",
    "\n",
    "            # Cast evt. resterende NI-tidsfelter, hvis de er strenge\n",
    "            for k in (\"wf_start_time\", \"NI_ExpStartTimeStamp\", \"NI_ExpTimeStamp\"):\n",
    "                if k in props and isinstance(props[k], str):\n",
    "                    try:\n",
    "                        props[k] = np.datetime64(props[k].replace(\" \", \"T\"))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            # Kanal-specifikke overrides fra WRITE_PLAN (fx \"Untitled/EKG\")\n",
    "            key = f\"{g.name}/{ch.name}\"\n",
    "            for kk, vv in WRITE_PLAN.get(\"channel_prop_overrides\", {}).get(key, {}).items():\n",
    "                props[kk] = vv\n",
    "\n",
    "            objs.append(ChannelObject(g.name, ch.name, data, properties=props))\n",
    "\n",
    "    writer.write_segment(objs)\n",
    "\n",
    "print(f\"[OK] Skrev ny fil → {DST_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "80492703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Patient 3_2.tdms ===\n",
      "File properties: {'name': 'Patient 3_2 07_09_2016 kl_10_50_46__09_09_2016 kl_10_30_00', 'author': 'Jesper', 'description': 'Corrected start time & metadata', 'registertxt1': 'Written by National Instruments LabVIEW', 'Corrected_by': 'Kasper Viken Jensen'}\n",
      "[Group] Untitled\n",
      "  [Channel] EKG samples: 86426624\n",
      "    - wf_start_time: 2016-09-07T10:50:46.000000\n",
      "    - wf_start_offset: 0.0\n",
      "    - wf_increment: 0.001953125\n",
      "    - DigitalMaximum: 32767\n",
      "    - DigitalMinimum: -32768\n",
      "    - NI_ChannelName: EKG\n",
      "    - NI_ExpIsRelativeTime: False\n",
      "    - wf_time_pref: absolute\n",
      "    - NI_ExpStartTimeStamp: 2016-09-07T10:50:46.000000\n",
      "    - NI_ExpTimeStamp: 2016-09-07T10:50:46.000000\n",
      "    - NI_ExpXDimension: t\n",
      "    - wf_xname: Time\n",
      "    - wf_xunit_string: s\n",
      "    - NI_UnitDescription: uV\n",
      "    - unit_string: uV\n",
      "    - PhysicalMaximum: 90298.0\n",
      "    - PhysicalMinimum: -90298.0\n",
      "    - PreFilter: \n",
      "    - TransducerType: \n",
      "    - ValsPerRec: 128\n",
      "    - wf_start_time_original: 2016-09-07T08:50:46.000000\n",
      "    - NI_ExpStartTimeStamp_original: 2016-09-07T08:50:46.000000\n",
      "    - NI_ExpTimeStamp_original: 2016-09-07T08:50:46.000000\n"
     ]
    }
   ],
   "source": [
    "from nptdms import TdmsFile\n",
    "\n",
    "def quick_check(path):\n",
    "    from pathlib import Path\n",
    "    with TdmsFile.read(path) as t:\n",
    "        print(f\"=== {Path(path).name} ===\")\n",
    "        print(\"File properties:\", dict(t.properties))\n",
    "        for g in t.groups():\n",
    "            print(\"[Group]\", g.name)\n",
    "            for c in g.channels():\n",
    "                print(\"  [Channel]\", c.name, \"samples:\", len(c[:]))\n",
    "                for k in c.properties:\n",
    "                    print(f\"    - {k}: {c.properties[k]}\")\n",
    "\n",
    "quick_check(DST_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b61a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
